{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training Demo: Fine tuning a pre-trained Hugging Face model with SageMaker for question answering\n",
    "\n",
    "### Distributed Data Parallel Training  with using `SageMakerTrainer` combined with Hugging Face pre-trained models on `squad` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions) \n",
    "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "    2. [Estimator Parameters](#Estimator-Parameters)   \n",
    "    3. [Download fine-tuned model from s3](#Download-fine-tuned-model-from-s3)\n",
    "    3. [Attach to old training job to an estimator ](#Attach-to-old-training-job-to-an-estimator)  \n",
    "5. [_Coming soon_:Push model to the Hugging Face hub](#Push-model-to-the-Hugging-Face-hub)"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end `distributed` Question-Answering example. In this demo, we will use the Hugging Face `transformers` and `datasets` library together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer for question-answering on multiple-gpus. In particular, the pre-trained model will be fine-tuned using the `squad` dataset. The demo will use the new `smdistributed` library to run training on multiple gpus as training scripting we are going to use one of the `transformers` [example scripts from the repository](https://github.com/huggingface/transformers/blob/master/examples/question-answering/run_qa.py).\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" accelerate --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**upgrade ipywidgets for `datasets` library and restart kernel, only needed when prerpocessing is done in the notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "#!conda install -c conda-forge ipywidgets -y\n",
    "#IPython.Application.instance().kernel.do_shutdown(True) # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using SageMaker version: 2.96.0\n",
      "Using Transfomer version: 4.19.2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker.huggingface\n",
    "import logging\n",
    "import transformers\n",
    "\n",
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.info(f'Using SageMaker version: {sagemaker.__version__}')\n",
    "logger.info(f'Using Transfomer version: {transformers.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::976939723775:role/service-role/AmazonSageMaker-ExecutionRole-20210317T133000\n",
      "sagemaker bucket: sagemaker-us-west-2-976939723775\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "#role = sagemaker.get_execution_role()\n",
    "role = 'arn:aws:iam::976939723775:role/service-role/AmazonSageMaker-ExecutionRole-20210317T133000'\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            sagemaker_session=sess,\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6.0',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating an Estimator and start a training job\n",
    "\n",
    "In this example we are going to use the capability to download/use a fine-tuning script from a `git`- repository. We are using the `run_qa.py` from the `transformers` example scripts. You can find the code [here](https://github.com/huggingface/transformers/tree/master/examples/question-answering).\n",
    "\n",
    "\n",
    "### RANK in MPI\n",
    "\n",
    "As to your question: processes are the actual instances of the program that are running. MPI allows you to create logical groups of processes, and in each group, a process is identified by its rank. This is an integer in the range [0, N-1] where N is the size of the group. Communicators are objects that handle communication between processes. An intra-communicator handles processes within a single group, while an inter-communicator handles communication between two distinct groups.\n",
    "\n",
    "By default, you have a single group that contains all your processes, and the intra-communicator MPI_COMM_WORLD that handles communication between them. This is sufficient for most applications, and does blur the distinction between process and rank a bit. The main thing to remember is that the rank of a process is always relative to a group. If you were to split your processes into two groups (e.g. one group to read input and another group to process data), then each process would now have two ranks: the one it originally had in MPI_COMM_WORLD, and one in its new group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'bert-large-uncased-whole-word-masking'\n",
    "#tokenizer_name = 'roberta-large'\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_name_or_path': tokenizer_name,\n",
    "    'dataset_name':'squad',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'fp16': True,\n",
    "    'per_device_train_batch_size': 4,\n",
    "    'per_device_eval_batch_size': 4,\n",
    "    'num_train_epochs': 2,\n",
    "    'max_seq_length': 384,\n",
    "    'max_steps': 100,\n",
    "    'pad_to_max_length': True,\n",
    "    'doc_stride': 128,\n",
    "    'output_dir': '/opt/ml/model',\n",
    "}\n",
    "\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "dp_options = {\n",
    "    \"enabled\":True,\n",
    "    \"custom_mpi_options\": \"-verbose -x NCCL_DEBUG=VERSION -x RANK=0 -x WORLD_SIZE=16\",\n",
    "    \"parameters\": {\n",
    "        \"microbatches\": 4,\n",
    "    }\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {\n",
    "    'smdistributed':{'dataparallel':dp_options}, \n",
    "    'parameter_server':{'enabled':False},\n",
    "}\n",
    "\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'}\n",
    "hub = {\n",
    "    'HF_MODEL_ID':tokenizer_name,\n",
    "    'SAGEMAKER_REQUIREMENTS': 'glue_requirements.txt', # path relative to `source_dir` below.\n",
    "    'HF_TASK':'question-answering'\n",
    "}\n",
    "# instance configurations\n",
    "instance_type='ml.p3.16xlarge'\n",
    "instance_count=2 # if set 1 you might get \"Environment variable SAGEMAKER_INSTANCE_TYPE is not set\" error\n",
    "volume_size=200\n",
    "\n",
    "\n",
    "# metric definition to extract the results\n",
    "metric_definitions=[\n",
    "     {\"Name\": \"train_runtime\", \"Regex\": \"train_runtime.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'train_samples_per_second', 'Regex': \"train_samples_per_second.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'epoch', 'Regex': \"epoch.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'f1', 'Regex': \"f1.*=\\D*(.*?)$\"},\n",
    "     {'Name': 'exact_match', 'Regex': \"exact_match.*=\\D*(.*?)$\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator\n",
    "huggingface_estimator = HuggingFace(entry_point='run_qa.py',\n",
    "                                    source_dir='./scripts/',\n",
    "                                    #git_config=git_config,\n",
    "                                    env=hub,\n",
    "                                    metric_definitions=metric_definitions,\n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=instance_count,\n",
    "                                    volume_size=volume_size,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.17',\n",
    "                                    pytorch_version='1.10',\n",
    "                                    py_version='py38',\n",
    "                                    distribution= distribution,\n",
    "                                    hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-01 19:38:45 Starting - Starting the training job."
     ]
    }
   ],
   "source": [
    "# starting the train job\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-pytorch-training-2022-06-03-21-36-30-007'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.m4.2xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.950797975063324, 'start': 33, 'end': 42, 'answer': 'Nuremberg'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "\"inputs\": {\n",
    "\t#\"question\": \"What is used for inference?\",\n",
    "    \"question\": \"What does Philipp live?\",\n",
    "\t\"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n",
    "\t}\n",
    "}\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display HF Estimator parameters and artifacts (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container image used for training job: \n",
      "None\n",
      "\n",
      "s3 uri where the trained model is located: \n",
      "s3://sagemaker-us-west-2-976939723775/huggingface-pytorch-training-2022-06-03-21-36-30-007/output/model.tar.gz\n",
      "\n",
      "latest training job name for this estimator: \n",
      "huggingface-pytorch-training-2022-06-03-21-36-30-007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-03 21:54:18 Starting - Preparing the instances for training\n",
      "2022-06-03 21:54:18 Downloading - Downloading input data\n",
      "2022-06-03 21:54:18 Training - Training image download completed. Training in progress.\n",
      "2022-06-03 21:54:18 Uploading - Uploading generated training model\n",
      "2022-06-03 21:54:18 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:50,796 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:50,868 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:50,875 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:50,875 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:51,573 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:51,573 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:51,577 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:51,579 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:51,579 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:50,562 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:50,637 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:50,644 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:50,644 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:51,419 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:51,419 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:51,421 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:51,421 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.172.180\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:52,434 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:52,627 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:52,628 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:52,628 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:52,628 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:52,632 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:52,581 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:52,581 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:53,594 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:53,792 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:53,792 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:53,792 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:53,792 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:53,792 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:53,792 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:53,797 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2022-06-03 21:43:53,871 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"-verbose -x NCCL_DEBUG=VERSION -x RANK=0 -x WORLD_SIZE=16\",\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"sagemaker_parameter_server_enabled\": false\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_name\": \"squad\",\n",
      "        \"do_eval\": true,\n",
      "        \"do_train\": true,\n",
      "        \"doc_stride\": 128,\n",
      "        \"fp16\": true,\n",
      "        \"max_seq_length\": 384,\n",
      "        \"max_steps\": 100,\n",
      "        \"model_name_or_path\": \"bert-large-uncased-whole-word-masking\",\n",
      "        \"num_train_epochs\": 2,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"pad_to_max_length\": true,\n",
      "        \"per_device_eval_batch_size\": 4,\n",
      "        \"per_device_train_batch_size\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-06-03-21-36-30-007\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-976939723775/huggingface-pytorch-training-2022-06-03-21-36-30-007/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_qa\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_qa.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_name\":\"squad\",\"do_eval\":true,\"do_train\":true,\"doc_stride\":128,\"fp16\":true,\"max_seq_length\":384,\"max_steps\":100,\"model_name_or_path\":\"bert-large-uncased-whole-word-masking\",\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"pad_to_max_length\":true,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_qa.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"-verbose -x NCCL_DEBUG=VERSION -x RANK=0 -x WORLD_SIZE=16\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_parameter_server_enabled\":false}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_qa\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-976939723775/huggingface-pytorch-training-2022-06-03-21-36-30-007/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"-verbose -x NCCL_DEBUG=VERSION -x RANK=0 -x WORLD_SIZE=16\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_parameter_server_enabled\":false},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"dataset_name\":\"squad\",\"do_eval\":true,\"do_train\":true,\"doc_stride\":128,\"fp16\":true,\"max_seq_length\":384,\"max_steps\":100,\"model_name_or_path\":\"bert-large-uncased-whole-word-masking\",\"num_train_epochs\":2,\"output_dir\":\"/opt/ml/model\",\"pad_to_max_length\":true,\"per_device_eval_batch_size\":4,\"per_device_train_batch_size\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-06-03-21-36-30-007\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-976939723775/huggingface-pytorch-training-2022-06-03-21-36-30-007/source/sourcedir.tar.gz\",\"module_name\":\"run_qa\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_qa.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_name\",\"squad\",\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--doc_stride\",\"128\",\"--fp16\",\"True\",\"--max_seq_length\",\"384\",\"--max_steps\",\"100\",\"--model_name_or_path\",\"bert-large-uncased-whole-word-masking\",\"--num_train_epochs\",\"2\",\"--output_dir\",\"/opt/ml/model\",\"--pad_to_max_length\",\"True\",\"--per_device_eval_batch_size\",\"4\",\"--per_device_train_batch_size\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_NAME=squad\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_DOC_STRIDE=128\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=384\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=100\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=bert-large-uncased-whole-word-masking\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_PAD_TO_MAX_LENGTH=true\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -verbose -x NCCL_DEBUG=VERSION -x RANK=0 -x WORLD_SIZE=16 -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /opt/conda/bin/python3.8 -m mpi4py run_qa.py --dataset_name squad --do_eval True --do_train True --doc_stride 128 --fp16 True --max_seq_length 384 --max_steps 100 --model_name_or_path bert-large-uncased-whole-word-masking --num_train_epochs 2 --output_dir /opt/ml/model --pad_to_max_length True --per_device_eval_batch_size 4 --per_device_train_batch_size 4\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.147.85' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:54,641 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=50, name='orted', status='sleeping', started='21:43:54')]\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:54,641 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=50, name='orted', status='sleeping', started='21:43:54')]\u001b[0m\n",
      "\u001b[35m2022-06-03 21:43:54,641 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=50, name='orted', status='sleeping', started='21:43:54')]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Running smdistributed.dataparallel v1.4.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:44:10 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:_n_gpu=1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:adafactor=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:adam_beta1=0.9,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:adam_beta2=0.999,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:adam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:bf16=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:bf16_full_eval=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:dataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:dataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:dataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:ddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:ddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:debug=[],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:deepspeed=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:disable_tqdm=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:do_eval=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:do_predict=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:do_train=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:eval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:eval_steps=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:evaluation_strategy=IntervalStrategy.NO,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fp16=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fp16_backend=auto,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fp16_full_eval=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:fp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:gradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:gradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:greater_is_better=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:group_by_length=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:half_precision_backend=auto,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:hub_model_id=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:hub_strategy=HubStrategy.EVERY_SAVE,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:hub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:ignore_data_skip=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:label_names=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:label_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:learning_rate=5e-05,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:length_column_name=length,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:load_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:local_rank=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:log_level=-1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:log_level_replica=-1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:log_on_each_node=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_dir=/opt/ml/model/runs/Jun03_21-44-00_algo-1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_first_step=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_steps=500,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:logging_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:lr_scheduler_type=SchedulerType.LINEAR,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:max_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:max_steps=100,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:metric_for_best_model=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:mp_parameters=,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:no_cuda=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:num_train_epochs=2.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:optim=OptimizerNames.ADAMW_HF,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:output_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:overwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:past_index=-1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:per_device_eval_batch_size=4,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:per_device_train_batch_size=4,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:prediction_loss_only=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:push_to_hub=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:push_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:push_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:remove_unused_columns=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:report_to=[],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:resume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:run_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:save_on_each_node=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:save_steps=500,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:save_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:save_total_limit=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:seed=42,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:sharded_ddp=[],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:skip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:tf32=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:tpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:tpu_num_cores=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:use_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:warmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:warmup_steps=0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:weight_decay=0.0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:xpu_backend=None,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:)\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:06/03/2022 21:44:10 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading: 5.27kB [00:00, 5.24MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading: 5.27kB [00:00, 4.82MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Downloading: 5.27kB [00:00, 4.67MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading: 5.27kB [00:00, 2.86MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Downloading: 5.27kB [00:00, 4.81MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Downloading: 5.27kB [00:00, 4.73MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Downloading: 5.27kB [00:00, 4.83MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading: 5.27kB [00:00, 4.04MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading: 5.27kB [00:00, 4.70MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading: 5.27kB [00:00, 4.74MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading: 5.27kB [00:00, 5.28MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading: 2.36kB [00:00, 1.89MB/s]                   \u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading: 2.36kB [00:00, 2.22MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Downloading: 2.36kB [00:00, 2.40MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Downloading: 2.36kB [00:00, 2.39MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Downloading: 2.36kB [00:00, 2.65MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Downloading: 2.36kB [00:00, 1.31MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Downloading: 2.36kB [00:00, 2.25MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Downloading: 5.27kB [00:00, 4.88MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Downloading: 2.36kB [00:00, 2.64MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/8.12M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Downloading: 2.36kB [00:00, 2.09MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:44:11 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.18.4/datasets/squad/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp3_pqbamm\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading:  81%|████████  | 6.57M/8.12M [00:00<00:00, 65.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading: 2.36kB [00:00, 2.66MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:44:11 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.18.4/datasets/squad/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/b5539d7166a04b5bc5b09671cd98d601201b258d7d116428b61ac54c6bbb8de6.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:44:11 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/b5539d7166a04b5bc5b09671cd98d601201b258d7d116428b61ac54c6bbb8de6.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:44:11 - INFO - datasets.builder - No config specified, defaulting to first: squad/plain_text\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:44:11 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Downloading: 2.36kB [00:00, 2.62MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading: 14.6MB [00:00, 74.2MB/s]                            #033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Downloading: 2.36kB [00:00, 1.56MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading: 22.5MB [00:00, 76.3MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading: 30.3MB [00:00, 75.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 50%|█████     | 1/2 [00:00<00:00,  1.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Downloading: 2.36kB [00:00, 2.15MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.05M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/8.12M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading: 4.85MB [00:00, 70.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015100%|██████████| 2/2 [00:01<00:00,  1.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015100%|██████████| 2/2 [00:01<00:00,  1.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:   2%|▏         | 182k/8.12M [00:00<00:04, 1.81MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 1676.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#0150 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:   7%|▋         | 575k/8.12M [00:00<00:02, 2.98MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:  19%|█▊        | 1.51M/8.12M [00:00<00:01, 5.79MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:  41%|████▏     | 3.36M/8.12M [00:00<00:00, 10.7MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:  90%|████████▉ | 7.28M/8.12M [00:00<00:00, 20.9MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading: 13.1MB [00:00, 33.5MB/s]                            #033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#0151 examples [00:00,  1.84 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading: 19.5MB [00:00, 43.5MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#0151837 examples [00:00, 3821.41 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading: 25.9MB [00:00, 49.8MB/s][1,mpirank:4,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#0153732 examples [00:00, 7287.36 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading: 30.3MB [00:00, 34.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 50%|█████     | 1/2 [00:01<00:01,  1.93s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#0155669 examples [00:00, 10265.67 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#0157312 examples [00:01, 8811.87 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.05M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#0159239 examples [00:01, 11074.41 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:  26%|██▌       | 275k/1.05M [00:00<00:00, 2.73MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 721k/1.05M [00:00<00:00, 3.67MB/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01510755 examples [00:01, 9942.25 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading: 1.85MB [00:00, 7.11MB/s]                           #033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01512715 examples [00:01, 12046.12 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading: 4.25MB [00:00, 13.7MB/s][1,mpirank:4,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading: 4.85MB [00:00, 11.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01514633 examples [00:01, 13739.41 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 1708.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#0150 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01516523 examples [00:01, 15045.07 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01518442 examples [00:01, 16145.37 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01520216 examples [00:01, 15291.89 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01522182 examples [00:01, 16451.62 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01524167 examples [00:02, 17386.59 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#0151 examples [00:00,  1.88 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01526128 examples [00:02, 18011.73 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#0151860 examples [00:00, 3931.53 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01528096 examples [00:02, 18489.48 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#0153783 examples [00:00, 7479.32 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01530000 examples [00:02, 16394.93 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#0155711 examples [00:00, 10414.87 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01531712 examples [00:02, 12370.27 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#0157363 examples [00:01, 8919.23 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01533675 examples [00:02, 13980.21 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#0159307 examples [00:01, 11207.96 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01535628 examples [00:02, 15314.50 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01510834 examples [00:01, 10084.08 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01537572 examples [00:02, 16367.89 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01512809 examples [00:01, 12202.05 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01539515 examples [00:03, 17186.34 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01514750 examples [00:01, 13918.83 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01541337 examples [00:03, 15791.12 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01516695 examples [00:01, 15322.76 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01543313 examples [00:03, 16834.67 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01518643 examples [00:01, 16429.46 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01545265 examples [00:03, 17566.84 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01520445 examples [00:01, 15595.20 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01547219 examples [00:03, 18117.80 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01522429 examples [00:01, 16726.16 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01549178 examples [00:03, 18536.55 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01524389 examples [00:02, 17518.08 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01551067 examples [00:03, 16471.58 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01526342 examples [00:02, 18086.01 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01553037 examples [00:03, 17334.89 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01528310 examples [00:02, 18542.01 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01530206 examples [00:02, 16490.26 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01554827 examples [00:04, 12726.54 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01556768 examples [00:04, 14219.51 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01531925 examples [00:02, 12581.32 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01558718 examples [00:04, 15497.01 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01533899 examples [00:02, 14188.38 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01560436 examples [00:04, 14843.67 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01535866 examples [00:02, 15519.49 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01562417 examples [00:04, 16108.17 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01537835 examples [00:02, 16592.45 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01564396 examples [00:04, 17089.74 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01539817 examples [00:03, 17459.89 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01566352 examples [00:04, 17770.68 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01541662 examples [00:03, 16042.22 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01568283 examples [00:04, 18203.35 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01543639 examples [00:03, 17024.40 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01570153 examples [00:04, 16247.48 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01545612 examples [00:03, 17763.76 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01572139 examples [00:05, 17210.88 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01547581 examples [00:03, 18304.21 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01574127 examples [00:05, 17947.36 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01549567 examples [00:03, 18749.03 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01576090 examples [00:05, 18421.51 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01551476 examples [00:03, 16642.11 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01578059 examples [00:05, 18785.03 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01553425 examples [00:03, 17404.02 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01579966 examples [00:05, 13219.72 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01555221 examples [00:04, 12881.57 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01581533 examples [00:05, 12905.41 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01557147 examples [00:04, 14314.38 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01583510 examples [00:05, 14498.02 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01559109 examples [00:04, 15604.10 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01585487 examples [00:05, 15809.07 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01560832 examples [00:04, 14621.46 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#01587440 examples [00:06, 16783.77 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01562819 examples [00:04, 15939.62 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#0150 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01564795 examples [00:04, 16948.31 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015907 examples [00:00, 9063.81 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01566764 examples [00:04, 17699.40 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#0152581 examples [00:00, 13575.04 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01568729 examples [00:04, 18246.72 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#0154085 examples [00:00, 14242.14 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#0155775 examples [00:00, 15289.08 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01570607 examples [00:04, 16631.78 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#0157449 examples [00:00, 15811.49 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01572599 examples [00:05, 17518.57 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#0159130 examples [00:00, 16147.81 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01574573 examples [00:05, 18135.39 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01576543 examples [00:05, 18578.20 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:06/03/2022 21:44:19 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 331.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 419.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:06/03/2022 21:44:19 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 429.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:06/03/2022 21:44:19 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:06/03/2022 21:44:19 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:06/03/2022 21:44:19 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 386.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 396.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 241.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:06/03/2022 21:44:19 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:06/03/2022 21:44:19 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 425.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 427.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01578482 examples [00:05, 13318.13 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01580045 examples [00:05, 12994.03 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01582022 examples [00:05, 14570.48 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01583990 examples [00:05, 15844.44 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/434 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Downloading: 100%|██████████| 434/434 [00:00<00:00, 355kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#01585968 examples [00:05, 16877.67 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#0150 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015923 examples [00:00, 9226.31 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#0152622 examples [00:00, 13791.94 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#0154247 examples [00:00, 14912.10 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 28.9kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#0155950 examples [00:00, 15747.32 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#0157637 examples [00:00, 16150.49 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#0159334 examples [00:00, 16427.08 examples/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:06/03/2022 21:44:21 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 301.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 426.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:06/03/2022 21:44:21 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:06/03/2022 21:44:21 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 424.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 399.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:06/03/2022 21:44:21 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:06/03/2022 21:44:21 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 402.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 430.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:44:21 - INFO - datasets.builder - Overwrite dataset info from restored data version.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:44:21 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:44:21 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:44:21 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:06/03/2022 21:44:21 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 435.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015100%|██████████| 2/2 [00:00<00:00, 455.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/434 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Downloading: 100%|██████████| 434/434 [00:00<00:00, 509kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:648] 2022-06-03 21:44:21,612 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:648] 2022-06-03 21:44:21,612 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:684] 2022-06-03 21:44:21,613 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"classifier_dropout\": null,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.17.0\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:684] 2022-06-03 21:44:21,613 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"classifier_dropout\": null,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.17.0\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Downloading:  16%|█▌        | 36.0k/226k [00:00<00:00, 271kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Downloading:  80%|███████▉  | 180k/226k [00:00<00:00, 842kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Downloading: 100%|██████████| 226k/226k [00:00<00:00, 845kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 38.9kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:648] 2022-06-03 21:44:22,562 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:648] 2022-06-03 21:44:22,562 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:684] 2022-06-03 21:44:22,563 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"classifier_dropout\": null,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.17.0\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:684] 2022-06-03 21:44:22,563 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"classifier_dropout\": null,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.17.0\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Downloading:   6%|▌         | 28.0k/455k [00:00<00:01, 225kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Downloading:  42%|████▏     | 193k/455k [00:00<00:00, 968kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Downloading: 100%|██████████| 455k/455k [00:00<00:00, 1.45MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:  35%|███▌      | 80.0k/226k [00:00<00:00, 729kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading:  92%|█████████▏| 208k/226k [00:00<00:00, 1.04MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Downloading: 100%|██████████| 226k/226k [00:00<00:00, 1.07MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|file_utils.py:2215] 2022-06-03 21:44:24,030 >> https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjtxslf53\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|file_utils.py:2215] 2022-06-03 21:44:24,030 >> https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjtxslf53\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading:   7%|▋         | 33.0k/455k [00:00<00:01, 257kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading:  39%|███▉      | 177k/455k [00:00<00:00, 850kB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading: 100%|██████████| 455k/455k [00:00<00:00, 1.40MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|file_utils.py:2219] 2022-06-03 21:44:24,669 >> storing https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|file_utils.py:2219] 2022-06-03 21:44:24,669 >> storing https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|file_utils.py:2227] 2022-06-03 21:44:24,669 >> creating metadata file for /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|file_utils.py:2227] 2022-06-03 21:44:24,669 >> creating metadata file for /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   0%|          | 2.66M/1.25G [00:00<00:48, 27.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   1%|          | 8.36M/1.25G [00:00<00:28, 46.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   1%|          | 15.0M/1.25G [00:00<00:23, 57.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   2%|▏         | 21.9M/1.25G [00:00<00:20, 63.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   2%|▏         | 28.8M/1.25G [00:00<00:19, 66.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   3%|▎         | 35.7M/1.25G [00:00<00:19, 68.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   3%|▎         | 42.7M/1.25G [00:00<00:18, 69.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   4%|▍         | 49.7M/1.25G [00:00<00:18, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1786] 2022-06-03 21:44:25,626 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1786] 2022-06-03 21:44:25,627 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1786] 2022-06-03 21:44:25,626 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b6fcd4f8304ce7ce4a5462db5a53616f8464a083bbe20508ea92d5e07229086f.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1786] 2022-06-03 21:44:25,627 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/33ee0bae279476c742373ecfd5a127d27372fbb9e2f5a84ccb38bbd72775f296.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1786] 2022-06-03 21:44:25,627 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1786] 2022-06-03 21:44:25,627 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1786] 2022-06-03 21:44:25,627 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:1786] 2022-06-03 21:44:25,627 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2a60ba33fe418e6652d2e5a6a40b189b0d6ca8a6a89e32a22bb5caf8d95982fe.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1786] 2022-06-03 21:44:25,627 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:1786] 2022-06-03 21:44:25,627 >> loading file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/2a60ba33fe418e6652d2e5a6a40b189b0d6ca8a6a89e32a22bb5caf8d95982fe.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   4%|▍         | 56.7M/1.25G [00:00<00:17, 71.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   5%|▍         | 63.8M/1.25G [00:01<00:17, 72.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   6%|▌         | 71.0M/1.25G [00:01<00:17, 73.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:648] 2022-06-03 21:44:25,910 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:648] 2022-06-03 21:44:25,910 >> loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/86b0883704d931817042e5c354032897e1dd6ccd5c2260bf0f6d4a15a6e5a232.d4d29047141693194a1c377423a9a58b3ffdd39ed177b3a9b2ac9bce9e6638d9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:684] 2022-06-03 21:44:25,911 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  ],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"classifier_dropout\": null,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"transformers_version\": \"4.17.0\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:684] 2022-06-03 21:44:25,911 >> Model config BertConfig {\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:    \"BertForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  ],\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"classifier_dropout\": null,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"hidden_size\": 1024,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"intermediate_size\": 4096,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"layer_norm_eps\": 1e-12,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"max_position_embeddings\": 512,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"model_type\": \"bert\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"num_attention_heads\": 16,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"num_hidden_layers\": 24,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"position_embedding_type\": \"absolute\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"transformers_version\": \"4.17.0\",\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"type_vocab_size\": 2,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"use_cache\": true,\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   6%|▌         | 78.1M/1.25G [00:01<00:17, 73.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   7%|▋         | 85.3M/1.25G [00:01<00:16, 74.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   7%|▋         | 92.4M/1.25G [00:01<00:16, 74.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   8%|▊         | 99.5M/1.25G [00:01<00:16, 74.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   8%|▊         | 107M/1.25G [00:01<00:16, 74.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   0%|          | 2.92M/1.25G [00:00<00:43, 30.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   9%|▉         | 114M/1.25G [00:01<00:16, 74.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   1%|          | 7.81M/1.25G [00:00<00:31, 42.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:   9%|▉         | 121M/1.25G [00:01<00:16, 74.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   1%|          | 13.7M/1.25G [00:00<00:25, 51.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  10%|▉         | 128M/1.25G [00:01<00:16, 75.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   2%|▏         | 19.7M/1.25G [00:00<00:23, 55.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  11%|█         | 136M/1.25G [00:02<00:15, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   2%|▏         | 25.8M/1.25G [00:00<00:22, 58.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  11%|█         | 143M/1.25G [00:02<00:15, 75.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   2%|▏         | 31.9M/1.25G [00:00<00:21, 60.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  12%|█▏        | 150M/1.25G [00:02<00:15, 75.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   3%|▎         | 38.1M/1.25G [00:00<00:21, 61.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  12%|█▏        | 157M/1.25G [00:02<00:15, 75.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   3%|▎         | 44.2M/1.25G [00:00<00:20, 62.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  13%|█▎        | 164M/1.25G [00:02<00:15, 75.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   4%|▍         | 50.3M/1.25G [00:00<00:20, 63.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  13%|█▎        | 172M/1.25G [00:02<00:15, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   4%|▍         | 56.5M/1.25G [00:01<00:20, 63.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  14%|█▍        | 179M/1.25G [00:02<00:15, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   5%|▍         | 62.8M/1.25G [00:01<00:19, 64.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  15%|█▍        | 186M/1.25G [00:02<00:15, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   5%|▌         | 68.9M/1.25G [00:01<00:19, 64.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  15%|█▌        | 193M/1.25G [00:02<00:15, 75.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   6%|▌         | 75.2M/1.25G [00:01<00:19, 64.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  16%|█▌        | 201M/1.25G [00:02<00:14, 75.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   6%|▋         | 81.4M/1.25G [00:01<00:19, 64.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  16%|█▌        | 208M/1.25G [00:03<00:14, 76.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   7%|▋         | 87.6M/1.25G [00:01<00:19, 64.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  17%|█▋        | 215M/1.25G [00:03<00:14, 75.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   7%|▋         | 93.7M/1.25G [00:01<00:19, 64.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  17%|█▋        | 222M/1.25G [00:03<00:14, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   8%|▊         | 99.9M/1.25G [00:01<00:19, 64.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  18%|█▊        | 230M/1.25G [00:03<00:16, 68.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   8%|▊         | 107M/1.25G [00:01<00:18, 67.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   9%|▉         | 114M/1.25G [00:01<00:17, 69.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  18%|█▊        | 236M/1.25G [00:03<00:17, 63.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:   9%|▉         | 121M/1.25G [00:02<00:17, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  19%|█▉        | 244M/1.25G [00:03<00:16, 66.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  10%|█         | 129M/1.25G [00:02<00:16, 72.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  20%|█▉        | 251M/1.25G [00:03<00:15, 69.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  11%|█         | 136M/1.25G [00:02<00:16, 73.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  20%|██        | 258M/1.25G [00:03<00:15, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  11%|█         | 143M/1.25G [00:02<00:16, 74.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  21%|██        | 266M/1.25G [00:03<00:14, 73.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  12%|█▏        | 150M/1.25G [00:02<00:15, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  21%|██▏       | 273M/1.25G [00:03<00:14, 73.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  12%|█▏        | 158M/1.25G [00:02<00:15, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  22%|██▏       | 280M/1.25G [00:04<00:14, 74.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  13%|█▎        | 165M/1.25G [00:02<00:15, 76.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  22%|██▏       | 288M/1.25G [00:04<00:13, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  13%|█▎        | 172M/1.25G [00:02<00:15, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  23%|██▎       | 295M/1.25G [00:04<00:13, 76.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  14%|█▍        | 180M/1.25G [00:02<00:15, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  24%|██▎       | 302M/1.25G [00:04<00:13, 76.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  24%|██▍       | 310M/1.25G [00:04<00:13, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  15%|█▍        | 187M/1.25G [00:02<00:15, 76.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  25%|██▍       | 317M/1.25G [00:04<00:13, 77.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  15%|█▌        | 194M/1.25G [00:03<00:14, 76.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  16%|█▌        | 201M/1.25G [00:03<00:14, 76.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  25%|██▌       | 325M/1.25G [00:04<00:15, 63.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  16%|█▋        | 209M/1.25G [00:03<00:14, 76.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  26%|██▌       | 332M/1.25G [00:04<00:14, 67.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  17%|█▋        | 216M/1.25G [00:03<00:14, 76.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  26%|██▋       | 339M/1.25G [00:04<00:14, 69.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  17%|█▋        | 223M/1.25G [00:03<00:14, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  27%|██▋       | 347M/1.25G [00:05<00:13, 71.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  18%|█▊        | 231M/1.25G [00:03<00:14, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  28%|██▊       | 354M/1.25G [00:05<00:13, 73.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  19%|█▊        | 238M/1.25G [00:03<00:14, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  28%|██▊       | 361M/1.25G [00:05<00:13, 74.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  19%|█▉        | 245M/1.25G [00:03<00:14, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  29%|██▉       | 369M/1.25G [00:05<00:12, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  20%|█▉        | 253M/1.25G [00:03<00:14, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  29%|██▉       | 376M/1.25G [00:05<00:12, 76.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  20%|██        | 260M/1.25G [00:03<00:13, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  30%|██▉       | 384M/1.25G [00:05<00:12, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  21%|██        | 267M/1.25G [00:04<00:13, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  30%|███       | 391M/1.25G [00:05<00:12, 76.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  21%|██▏       | 275M/1.25G [00:04<00:13, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  31%|███       | 398M/1.25G [00:05<00:12, 76.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  22%|██▏       | 282M/1.25G [00:04<00:13, 76.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  32%|███▏      | 406M/1.25G [00:05<00:11, 77.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  23%|██▎       | 289M/1.25G [00:04<00:13, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  32%|███▏      | 413M/1.25G [00:05<00:11, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  23%|██▎       | 297M/1.25G [00:04<00:13, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  33%|███▎      | 420M/1.25G [00:06<00:11, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  24%|██▎       | 304M/1.25G [00:04<00:13, 76.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  33%|███▎      | 428M/1.25G [00:06<00:11, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  24%|██▍       | 311M/1.25G [00:04<00:13, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  34%|███▍      | 435M/1.25G [00:06<00:11, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  25%|██▍       | 319M/1.25G [00:04<00:13, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  35%|███▍      | 443M/1.25G [00:06<00:11, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  25%|██▌       | 326M/1.25G [00:04<00:13, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  35%|███▌      | 450M/1.25G [00:06<00:11, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  26%|██▌       | 333M/1.25G [00:04<00:13, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  36%|███▌      | 457M/1.25G [00:06<00:11, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  27%|██▋       | 341M/1.25G [00:05<00:12, 76.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  36%|███▌      | 465M/1.25G [00:06<00:11, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  27%|██▋       | 348M/1.25G [00:05<00:12, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  37%|███▋      | 472M/1.25G [00:06<00:11, 77.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  28%|██▊       | 355M/1.25G [00:05<00:12, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  37%|███▋      | 479M/1.25G [00:06<00:10, 77.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  28%|██▊       | 363M/1.25G [00:05<00:12, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  38%|███▊      | 487M/1.25G [00:06<00:10, 77.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  29%|██▉       | 370M/1.25G [00:05<00:12, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  39%|███▊      | 494M/1.25G [00:07<00:10, 77.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  29%|██▉       | 378M/1.25G [00:05<00:12, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  39%|███▉      | 502M/1.25G [00:07<00:10, 77.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  30%|███       | 385M/1.25G [00:05<00:12, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  31%|███       | 392M/1.25G [00:05<00:12, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  40%|███▉      | 509M/1.25G [00:07<00:13, 60.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  31%|███       | 400M/1.25G [00:05<00:11, 77.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  40%|████      | 515M/1.25G [00:07<00:13, 58.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  32%|███▏      | 407M/1.25G [00:05<00:11, 77.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  41%|████      | 522M/1.25G [00:07<00:12, 61.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  32%|███▏      | 415M/1.25G [00:06<00:11, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  41%|████▏     | 529M/1.25G [00:07<00:12, 64.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  33%|███▎      | 422M/1.25G [00:06<00:11, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  42%|████▏     | 537M/1.25G [00:07<00:11, 67.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  33%|███▎      | 429M/1.25G [00:06<00:11, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  42%|████▏     | 544M/1.25G [00:07<00:11, 70.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  34%|███▍      | 437M/1.25G [00:06<00:11, 77.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  43%|████▎     | 551M/1.25G [00:07<00:10, 72.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  35%|███▍      | 444M/1.25G [00:06<00:11, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  44%|████▎     | 558M/1.25G [00:08<00:10, 72.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  35%|███▌      | 451M/1.25G [00:06<00:11, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  44%|████▍     | 565M/1.25G [00:08<00:10, 73.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  36%|███▌      | 459M/1.25G [00:06<00:11, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  45%|████▍     | 573M/1.25G [00:08<00:10, 74.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  36%|███▋      | 466M/1.25G [00:06<00:11, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  45%|████▌     | 580M/1.25G [00:08<00:09, 74.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  37%|███▋      | 473M/1.25G [00:06<00:10, 77.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  46%|████▌     | 587M/1.25G [00:08<00:09, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  37%|███▋      | 481M/1.25G [00:06<00:10, 76.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  46%|████▋     | 595M/1.25G [00:08<00:09, 76.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  38%|███▊      | 488M/1.25G [00:07<00:10, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  47%|████▋     | 602M/1.25G [00:08<00:09, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  39%|███▊      | 496M/1.25G [00:07<00:10, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  48%|████▊     | 609M/1.25G [00:08<00:09, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  39%|███▉      | 503M/1.25G [00:07<00:11, 74.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  48%|████▊     | 617M/1.25G [00:08<00:09, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  40%|███▉      | 510M/1.25G [00:07<00:11, 72.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  49%|████▊     | 624M/1.25G [00:08<00:08, 77.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  40%|████      | 517M/1.25G [00:07<00:11, 71.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  49%|████▉     | 632M/1.25G [00:09<00:08, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  41%|████      | 524M/1.25G [00:07<00:11, 70.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  50%|████▉     | 639M/1.25G [00:09<00:08, 77.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  41%|████▏     | 530M/1.25G [00:07<00:11, 70.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  50%|█████     | 646M/1.25G [00:09<00:08, 77.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  42%|████▏     | 537M/1.25G [00:07<00:11, 69.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  51%|█████     | 654M/1.25G [00:09<00:08, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  42%|████▏     | 544M/1.25G [00:07<00:11, 69.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  52%|█████▏    | 661M/1.25G [00:09<00:08, 76.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  43%|████▎     | 550M/1.25G [00:07<00:11, 69.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  52%|█████▏    | 668M/1.25G [00:09<00:08, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  43%|████▎     | 557M/1.25G [00:08<00:11, 68.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  53%|█████▎    | 676M/1.25G [00:09<00:08, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  44%|████▍     | 564M/1.25G [00:08<00:10, 68.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  53%|█████▎    | 683M/1.25G [00:09<00:08, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  44%|████▍     | 570M/1.25G [00:08<00:10, 68.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  54%|█████▍    | 690M/1.25G [00:09<00:08, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  45%|████▍     | 577M/1.25G [00:08<00:10, 69.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  54%|█████▍    | 698M/1.25G [00:09<00:07, 76.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  45%|████▌     | 583M/1.25G [00:08<00:10, 69.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  55%|█████▍    | 705M/1.25G [00:10<00:07, 76.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  46%|████▌     | 590M/1.25G [00:08<00:10, 68.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  56%|█████▌    | 712M/1.25G [00:10<00:07, 76.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  47%|████▋     | 596M/1.25G [00:08<00:10, 68.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  56%|█████▌    | 719M/1.25G [00:10<00:07, 74.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  47%|████▋     | 603M/1.25G [00:08<00:10, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  57%|█████▋    | 727M/1.25G [00:10<00:07, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  48%|████▊     | 610M/1.25G [00:08<00:10, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  57%|█████▋    | 734M/1.25G [00:10<00:07, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  48%|████▊     | 616M/1.25G [00:08<00:10, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  58%|█████▊    | 741M/1.25G [00:10<00:07, 76.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  49%|████▊     | 623M/1.25G [00:09<00:10, 68.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  58%|█████▊    | 749M/1.25G [00:10<00:07, 76.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  49%|████▉     | 629M/1.25G [00:09<00:09, 68.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  59%|█████▉    | 756M/1.25G [00:10<00:07, 76.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  50%|████▉     | 636M/1.25G [00:09<00:09, 68.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  60%|█████▉    | 764M/1.25G [00:10<00:07, 76.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  50%|█████     | 642M/1.25G [00:09<00:09, 68.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  60%|██████    | 771M/1.25G [00:10<00:07, 75.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  51%|█████     | 649M/1.25G [00:09<00:09, 69.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  61%|██████    | 778M/1.25G [00:11<00:07, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  51%|█████     | 656M/1.25G [00:09<00:09, 68.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  61%|██████    | 785M/1.25G [00:11<00:06, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 662M/1.25G [00:09<00:09, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  62%|██████▏   | 792M/1.25G [00:11<00:06, 75.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  52%|█████▏    | 669M/1.25G [00:09<00:09, 68.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  62%|██████▏   | 800M/1.25G [00:11<00:06, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 675M/1.25G [00:09<00:09, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  63%|██████▎   | 807M/1.25G [00:11<00:06, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  53%|█████▎    | 682M/1.25G [00:09<00:09, 68.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  63%|██████▎   | 814M/1.25G [00:11<00:06, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  54%|█████▎    | 688M/1.25G [00:10<00:09, 68.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  64%|██████▍   | 821M/1.25G [00:11<00:06, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  54%|█████▍    | 695M/1.25G [00:10<00:08, 68.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  65%|██████▍   | 828M/1.25G [00:11<00:06, 74.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  55%|█████▍    | 702M/1.25G [00:10<00:08, 69.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  65%|██████▌   | 836M/1.25G [00:11<00:06, 74.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  55%|█████▌    | 708M/1.25G [00:10<00:08, 68.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  66%|██████▌   | 843M/1.25G [00:11<00:06, 74.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  56%|█████▌    | 715M/1.25G [00:10<00:08, 66.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  66%|██████▋   | 850M/1.25G [00:12<00:06, 75.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  56%|█████▌    | 721M/1.25G [00:10<00:08, 66.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  67%|██████▋   | 857M/1.25G [00:12<00:05, 75.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 728M/1.25G [00:10<00:08, 66.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  67%|██████▋   | 864M/1.25G [00:12<00:05, 75.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  57%|█████▋    | 734M/1.25G [00:10<00:08, 67.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  68%|██████▊   | 871M/1.25G [00:12<00:05, 75.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 741M/1.25G [00:10<00:08, 68.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  68%|██████▊   | 879M/1.25G [00:12<00:05, 74.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  58%|█████▊    | 748M/1.25G [00:10<00:08, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  69%|██████▉   | 886M/1.25G [00:12<00:05, 74.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  59%|█████▉    | 754M/1.25G [00:11<00:08, 68.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  70%|██████▉   | 893M/1.25G [00:12<00:05, 75.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  59%|█████▉    | 761M/1.25G [00:11<00:07, 69.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  70%|███████   | 900M/1.25G [00:12<00:05, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  60%|█████▉    | 767M/1.25G [00:11<00:07, 69.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  71%|███████   | 907M/1.25G [00:12<00:05, 75.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  60%|██████    | 774M/1.25G [00:11<00:07, 69.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  61%|██████    | 781M/1.25G [00:11<00:07, 69.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  71%|███████▏  | 914M/1.25G [00:13<00:06, 58.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  61%|██████▏   | 787M/1.25G [00:11<00:07, 69.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  72%|███████▏  | 922M/1.25G [00:13<00:06, 62.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 794M/1.25G [00:11<00:07, 69.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  72%|███████▏  | 929M/1.25G [00:13<00:05, 66.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  62%|██████▏   | 800M/1.25G [00:11<00:07, 69.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  73%|███████▎  | 936M/1.25G [00:13<00:05, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 807M/1.25G [00:11<00:07, 68.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  74%|███████▎  | 943M/1.25G [00:13<00:05, 70.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  63%|██████▎   | 814M/1.25G [00:11<00:07, 69.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  74%|███████▍  | 950M/1.25G [00:13<00:04, 71.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  64%|██████▍   | 820M/1.25G [00:12<00:07, 69.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  75%|███████▍  | 958M/1.25G [00:13<00:04, 73.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  64%|██████▍   | 827M/1.25G [00:12<00:06, 69.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  75%|███████▌  | 965M/1.25G [00:13<00:04, 74.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  65%|██████▍   | 833M/1.25G [00:12<00:06, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  76%|███████▌  | 972M/1.25G [00:13<00:04, 74.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  65%|██████▌   | 840M/1.25G [00:12<00:06, 68.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  76%|███████▋  | 980M/1.25G [00:13<00:04, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  66%|██████▌   | 847M/1.25G [00:12<00:06, 68.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  77%|███████▋  | 987M/1.25G [00:14<00:04, 75.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 853M/1.25G [00:12<00:06, 68.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  77%|███████▋  | 994M/1.25G [00:14<00:04, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  67%|██████▋   | 860M/1.25G [00:12<00:06, 68.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  78%|███████▊  | 0.98G/1.25G [00:14<00:03, 75.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 866M/1.25G [00:12<00:06, 68.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  79%|███████▊  | 0.99G/1.25G [00:14<00:03, 76.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  68%|██████▊   | 873M/1.25G [00:12<00:06, 68.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  79%|███████▉  | 0.99G/1.25G [00:14<00:03, 76.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  69%|██████▊   | 879M/1.25G [00:12<00:06, 68.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  80%|███████▉  | 1.00G/1.25G [00:14<00:03, 76.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  69%|██████▉   | 886M/1.25G [00:13<00:06, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  80%|████████  | 1.01G/1.25G [00:14<00:03, 76.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  70%|██████▉   | 893M/1.25G [00:13<00:05, 68.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  81%|████████  | 1.01G/1.25G [00:14<00:03, 76.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  70%|███████   | 899M/1.25G [00:13<00:06, 67.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  81%|████████▏ | 1.02G/1.25G [00:14<00:03, 76.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  71%|███████   | 906M/1.25G [00:13<00:05, 67.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  82%|████████▏ | 1.03G/1.25G [00:14<00:03, 76.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  71%|███████   | 912M/1.25G [00:13<00:05, 67.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  83%|████████▎ | 1.03G/1.25G [00:15<00:03, 75.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 919M/1.25G [00:13<00:05, 67.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  83%|████████▎ | 1.04G/1.25G [00:15<00:03, 75.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  72%|███████▏  | 925M/1.25G [00:13<00:05, 68.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  84%|████████▎ | 1.05G/1.25G [00:15<00:02, 75.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 932M/1.25G [00:13<00:05, 68.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  84%|████████▍ | 1.06G/1.25G [00:15<00:02, 75.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  73%|███████▎  | 938M/1.25G [00:13<00:05, 68.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  85%|████████▍ | 1.06G/1.25G [00:15<00:02, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  74%|███████▎  | 945M/1.25G [00:13<00:05, 68.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  85%|████████▌ | 1.07G/1.25G [00:15<00:02, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  74%|███████▍  | 951M/1.25G [00:14<00:05, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  86%|████████▌ | 1.08G/1.25G [00:15<00:02, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  75%|███████▍  | 958M/1.25G [00:14<00:04, 68.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  87%|████████▋ | 1.08G/1.25G [00:15<00:02, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  75%|███████▌  | 965M/1.25G [00:14<00:04, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  87%|████████▋ | 1.09G/1.25G [00:15<00:02, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  76%|███████▌  | 971M/1.25G [00:14<00:04, 68.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  88%|████████▊ | 1.10G/1.25G [00:16<00:02, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  76%|███████▌  | 978M/1.25G [00:14<00:04, 68.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  88%|████████▊ | 1.11G/1.25G [00:16<00:02, 75.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  77%|███████▋  | 984M/1.25G [00:14<00:04, 68.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  89%|████████▉ | 1.11G/1.25G [00:16<00:02, 74.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  77%|███████▋  | 992M/1.25G [00:14<00:04, 70.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  89%|████████▉ | 1.12G/1.25G [00:16<00:01, 72.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  78%|███████▊  | 999M/1.25G [00:14<00:04, 72.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  90%|████████▉ | 1.13G/1.25G [00:16<00:01, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  78%|███████▊  | 0.98G/1.25G [00:14<00:03, 73.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  90%|█████████ | 1.13G/1.25G [00:16<00:01, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  79%|███████▉  | 0.99G/1.25G [00:14<00:03, 74.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  91%|█████████ | 1.14G/1.25G [00:16<00:01, 71.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  80%|███████▉  | 1.00G/1.25G [00:15<00:03, 74.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  91%|█████████▏| 1.15G/1.25G [00:16<00:01, 71.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  80%|████████  | 1.00G/1.25G [00:15<00:03, 75.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  92%|█████████▏| 1.15G/1.25G [00:16<00:01, 70.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  81%|████████  | 1.01G/1.25G [00:15<00:03, 75.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  93%|█████████▎| 1.16G/1.25G [00:16<00:01, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  81%|████████▏ | 1.02G/1.25G [00:15<00:03, 75.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  93%|█████████▎| 1.17G/1.25G [00:17<00:01, 71.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  82%|████████▏ | 1.02G/1.25G [00:15<00:03, 75.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  94%|█████████▎| 1.17G/1.25G [00:17<00:01, 70.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  82%|████████▏ | 1.03G/1.25G [00:15<00:03, 75.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  94%|█████████▍| 1.18G/1.25G [00:17<00:01, 70.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  83%|████████▎ | 1.04G/1.25G [00:15<00:03, 75.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  95%|█████████▍| 1.19G/1.25G [00:17<00:01, 70.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  84%|████████▎ | 1.05G/1.25G [00:15<00:02, 75.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  95%|█████████▌| 1.19G/1.25G [00:17<00:00, 71.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  84%|████████▍ | 1.05G/1.25G [00:15<00:02, 73.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  96%|█████████▌| 1.20G/1.25G [00:17<00:00, 70.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  85%|████████▍ | 1.06G/1.25G [00:16<00:02, 71.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  96%|█████████▌| 1.21G/1.25G [00:17<00:00, 70.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  85%|████████▌ | 1.07G/1.25G [00:16<00:02, 70.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  97%|█████████▋| 1.21G/1.25G [00:17<00:00, 70.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  86%|████████▌ | 1.07G/1.25G [00:16<00:02, 70.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  97%|█████████▋| 1.22G/1.25G [00:17<00:00, 72.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  86%|████████▌ | 1.08G/1.25G [00:16<00:02, 69.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  98%|█████████▊| 1.23G/1.25G [00:17<00:00, 73.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  87%|████████▋ | 1.09G/1.25G [00:16<00:02, 69.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  98%|█████████▊| 1.23G/1.25G [00:18<00:00, 73.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  87%|████████▋ | 1.09G/1.25G [00:16<00:02, 68.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading:  99%|█████████▉| 1.24G/1.25G [00:18<00:00, 73.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  88%|████████▊ | 1.10G/1.25G [00:16<00:02, 68.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading: 100%|█████████▉| 1.25G/1.25G [00:18<00:00, 74.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  88%|████████▊ | 1.11G/1.25G [00:16<00:02, 67.9MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Downloading: 100%|██████████| 1.25G/1.25G [00:18<00:00, 73.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  89%|████████▉ | 1.11G/1.25G [00:16<00:02, 67.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  89%|████████▉ | 1.12G/1.25G [00:16<00:02, 67.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  90%|████████▉ | 1.12G/1.25G [00:17<00:02, 67.1MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  90%|█████████ | 1.13G/1.25G [00:17<00:01, 67.2MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  91%|█████████ | 1.14G/1.25G [00:17<00:01, 67.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  91%|█████████▏| 1.14G/1.25G [00:17<00:01, 67.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  92%|█████████▏| 1.15G/1.25G [00:17<00:01, 68.0MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  92%|█████████▏| 1.16G/1.25G [00:17<00:01, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  93%|█████████▎| 1.16G/1.25G [00:17<00:01, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  93%|█████████▎| 1.17G/1.25G [00:17<00:01, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  94%|█████████▍| 1.18G/1.25G [00:17<00:01, 68.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  94%|█████████▍| 1.18G/1.25G [00:17<00:01, 68.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  95%|█████████▍| 1.19G/1.25G [00:18<00:01, 68.7MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  95%|█████████▌| 1.19G/1.25G [00:18<00:00, 68.8MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  96%|█████████▌| 1.20G/1.25G [00:18<00:00, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  96%|█████████▋| 1.21G/1.25G [00:18<00:00, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  97%|█████████▋| 1.21G/1.25G [00:18<00:00, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  97%|█████████▋| 1.22G/1.25G [00:18<00:00, 68.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  98%|█████████▊| 1.23G/1.25G [00:18<00:00, 68.5MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  98%|█████████▊| 1.23G/1.25G [00:18<00:00, 68.3MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  99%|█████████▉| 1.24G/1.25G [00:18<00:00, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading:  99%|█████████▉| 1.25G/1.25G [00:18<00:00, 68.4MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading: 100%|█████████▉| 1.25G/1.25G [00:19<00:00, 68.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Downloading: 100%|██████████| 1.25G/1.25G [00:19<00:00, 70.6MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|modeling_utils.py:1431] 2022-06-03 21:44:45,389 >> loading weights file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd59edc8ee9750d18ad7f5c4d97b4040bddfb6ccd64c37c421ffae14656dc51c.0bf22e1ed76b044bda5c02e02fdfc0bdf5a6e1827e76950cc9f43e1919ad896f\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|modeling_utils.py:1431] 2022-06-03 21:44:45,389 >> loading weights file https://huggingface.co/bert-large-uncased-whole-word-masking/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd59edc8ee9750d18ad7f5c4d97b4040bddfb6ccd64c37c421ffae14656dc51c.0bf22e1ed76b044bda5c02e02fdfc0bdf5a6e1827e76950cc9f43e1919ad896f\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,444 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,444 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,444 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,444 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,446 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,446 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,446 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,446 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,456 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,456 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,456 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,456 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,456 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,457 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,456 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,457 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,466 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,466 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,466 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,466 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,472 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,472 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,472 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,472 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,510 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,510 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,510 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,510 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,556 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,557 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:48,556 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:48,557 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,739 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,739 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,739 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,739 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,753 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,753 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,753 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,753 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,772 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,773 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,772 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,773 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,781 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,781 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,781 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,781 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,808 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,808 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,808 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,808 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,833 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,833 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,833 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,833 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,924 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,924 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,924 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,924 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,997 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,997 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[WARNING|modeling_utils.py:1693] 2022-06-03 21:44:50,997 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[WARNING|modeling_utils.py:1704] 2022-06-03 21:44:50,997 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:44:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-6fbcccab18dd0465.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<00:45,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:00<00:36,  2.34ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:35,  2.40ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:01<00:36,  2.33ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:33,  2.51ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:02<00:31,  2.64ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:02<00:29,  2.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:03<00:29,  2.67ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:03<00:29,  2.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:03<00:28,  2.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:04<00:27,  2.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:04<00:28,  2.71ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:04<00:27,  2.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:05<00:26,  2.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:05<00:27,  2.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:06<00:25,  2.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:06<00:24,  2.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:06<00:23,  2.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:07<00:25,  2.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:07<00:24,  2.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:07<00:23,  2.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:08<00:22,  2.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:08<00:24,  2.63ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:08<00:24,  2.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:09<00:23,  2.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:09<00:22,  2.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:10<00:23,  2.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:10<00:22,  2.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:10<00:22,  2.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:11<00:23,  2.50ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:11<00:22,  2.52ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:12<00:21,  2.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:12<00:21,  2.54ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:12<00:22,  2.40ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:13<00:21,  2.48ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:13<00:21,  2.46ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:14<00:20,  2.54ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:14<00:20,  2.42ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:14<00:19,  2.50ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:15<00:18,  2.58ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:15<00:17,  2.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:16<00:17,  2.56ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:16<00:16,  2.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:16<00:16,  2.69ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:17<00:17,  2.52ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:17<00:16,  2.57ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:17<00:15,  2.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:18<00:15,  2.60ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:18<00:17,  2.28ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:19<00:15,  2.41ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:19<00:14,  2.50ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:20<00:14,  2.53ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:20<00:14,  2.41ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:20<00:13,  2.52ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:21<00:12,  2.57ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:21<00:12,  2.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:22<00:12,  2.47ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:22<00:11,  2.56ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:22<00:11,  2.63ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:23<00:11,  2.38ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:23<00:10,  2.48ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:23<00:10,  2.52ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:24<00:09,  2.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:24<00:09,  2.50ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:25<00:09,  2.49ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:25<00:08,  2.53ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:25<00:08,  2.58ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:26<00:07,  2.52ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:26<00:07,  2.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:27<00:06,  2.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:27<00:06,  2.54ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:27<00:06,  2.61ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:28<00:05,  2.63ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:28<00:05,  2.64ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:29<00:05,  2.55ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:29<00:04,  2.63ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:29<00:04,  2.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:30<00:03,  2.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:30<00:03,  2.55ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:30<00:03,  2.60ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:31<00:02,  2.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:31<00:02,  2.64ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:32<00:02,  2.42ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:32<00:01,  2.50ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:32<00:01,  2.60ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:33<00:00,  2.47ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:33<00:00,  2.56ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:33<00:00,  2.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:33<00:00,  2.59ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:06/03/2022 21:45:24 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-6fbcccab18dd0465.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:06/03/2022 21:45:24 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-6fbcccab18dd0465.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:06/03/2022 21:45:24 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-6fbcccab18dd0465.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:06/03/2022 21:45:24 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-6fbcccab18dd0465.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:06/03/2022 21:45:24 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-6fbcccab18dd0465.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:06/03/2022 21:45:24 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-6fbcccab18dd0465.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s][1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:06/03/2022 21:45:24 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-6fbcccab18dd0465.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:45:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-0cd3ef4c4221ec58.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<00:55,  1.58ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<01:03,  1.37ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<01:05,  1.34ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<01:06,  1.31ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<01:08,  1.28ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<01:08,  1.27ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<01:09,  1.25ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   1%|          | 1/88 [00:00<01:09,  1.25ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:47,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:51,  1.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:51,  1.67ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:51,  1.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:51,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:51,  1.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:51,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   2%|▏         | 2/88 [00:01<00:51,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:43,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:44,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:45,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:45,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:45,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:46,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:46,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   3%|▎         | 3/88 [00:01<00:47,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:44,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:46,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:46,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:46,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:47,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:48,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:48,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   5%|▍         | 4/88 [00:02<00:49,  1.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:43,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:02<00:27,  2.72s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:42,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:45,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:44,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:44,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:45,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:45,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   6%|▌         | 5/88 [00:02<00:46,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:41,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:39,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:41,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:42,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:43,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:42,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:43,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   7%|▋         | 6/88 [00:03<00:44,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:39,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:40,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:40,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:41,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:41,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:40,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:41,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   8%|▊         | 7/88 [00:03<00:41,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:39,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:39,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:41,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:42,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:42,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:42,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:43,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:   9%|▉         | 8/88 [00:04<00:43,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:37,  2.09ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:39,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:41,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:42,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:41,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:41,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:04<00:35,  2.19ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:42,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  10%|█         | 9/88 [00:04<00:43,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:05<00:36,  2.16ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:05<00:37,  2.07ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:23,  2.64s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:05<00:39,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:05<00:39,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:05<00:39,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:05<00:39,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  11%|█▏        | 10/88 [00:05<00:39,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:33,  2.31ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:39,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:34,  2.21ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:37,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:37,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:38,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:38,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  12%|█▎        | 11/88 [00:05<00:39,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:37,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:36,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:34,  2.19ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:38,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:34,  2.18ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:33,  2.23ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:39,  1.92ba/s][1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:39,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:31,  2.39ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:40,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  14%|█▎        | 12/88 [00:06<00:40,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:06<00:34,  2.17ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:37,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:37,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:06<00:33,  2.18ba/s][1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:37,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:37,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  15%|█▍        | 13/88 [00:06<00:38,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:06<00:32,  2.26ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:07<00:35,  2.07ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:07<00:35,  2.07ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:07<00:36,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:07<00:36,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  16%|█▌        | 14/88 [00:07<00:36,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:38,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:38,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:37,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:36,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:37,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:07<00:34,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:36,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:07<00:21,  2.64s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:38,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:07<00:34,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:07<00:33,  2.15ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  17%|█▋        | 15/88 [00:07<00:38,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:08<00:35,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:08<00:35,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:33,  2.14ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:08<00:35,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:08<00:35,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:32,  2.17ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  18%|█▊        | 16/88 [00:08<00:35,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:31,  2.22ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:33,  2.11ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:34,  2.07ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:34,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:33,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  19%|█▉        | 17/88 [00:08<00:33,  2.11ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:08<00:32,  2.16ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:08<00:32,  2.18ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:08<00:31,  2.21ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:09<00:32,  2.12ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:09<00:32,  2.13ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:09<00:33,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:09<00:33,  2.12ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  20%|██        | 18/88 [00:09<00:33,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:33,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:35,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:37,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:09<00:31,  2.14ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:34,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:09<00:32,  2.11ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:35,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:35,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:36,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  22%|██▏       | 19/88 [00:09<00:36,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:09<00:34,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:31,  2.14ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:10<00:33,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:10<00:33,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:10<00:33,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:10<00:33,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  23%|██▎       | 20/88 [00:10<00:34,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:32,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:33,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:10<00:30,  2.18ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:10<00:18,  2.70s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:32,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:32,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:10<00:31,  2.11ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:32,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:33,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  24%|██▍       | 21/88 [00:10<00:33,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:10<00:31,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:11<00:30,  2.18ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:30,  2.11ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:11<00:31,  2.11ba/s][1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:11<00:31,  2.12ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:11<00:31,  2.08ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  25%|██▌       | 22/88 [00:11<00:32,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:33,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:32,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:11<00:30,  2.11ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:31,  2.09ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:11<00:30,  2.07ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:33,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:33,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:33,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  26%|██▌       | 23/88 [00:11<00:33,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:11<00:32,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:30,  2.09ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:12<00:31,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:30,  2.07ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:12<00:32,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:12<00:32,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:12<00:32,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  27%|██▋       | 24/88 [00:12<00:32,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:30,  2.09ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:29,  2.15ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:12<00:28,  2.16ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:12<00:32,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:30,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:30,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:30,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  28%|██▊       | 25/88 [00:12<00:31,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:12<00:29,  2.07ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:13<00:29,  2.12ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:13<00:30,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:31,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:13<00:30,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:13<00:30,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:30,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  30%|██▉       | 26/88 [00:13<00:31,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:32,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:30,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:13<00:30,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:13<00:30,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:33,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:13<00:29,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:33,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:13<00:32,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  31%|███       | 27/88 [00:14<00:34,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:14<00:30,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:14<00:17,  3.00s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:28,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:30,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:14<00:31,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:14<00:32,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:14<00:32,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:30,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  32%|███▏      | 28/88 [00:14<00:34,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:30,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:29,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:30,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:14<00:30,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:14<00:32,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:15<00:33,  1.71ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  33%|███▎      | 29/88 [00:15<00:31,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:15<00:32,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:15<00:32,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:15<00:30,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:15<00:30,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:15<00:31,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:15<00:30,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:15<00:32,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:15<00:33,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  34%|███▍      | 30/88 [00:15<00:33,  1.71ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:15<00:31,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:15<00:29,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:16<00:30,  1.85ba/s][1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:16<00:30,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:16<00:28,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:16<00:31,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:16<00:32,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  35%|███▌      | 31/88 [00:16<00:32,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:16<00:30,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:16<00:29,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:16<00:28,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:16<00:27,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:16<00:29,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:16<00:30,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:16<00:30,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  36%|███▋      | 32/88 [00:16<00:31,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:17<00:28,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:17<00:30,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:17<00:14,  2.94s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:17<00:29,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:17<00:28,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:17<00:28,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:17<00:29,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  38%|███▊      | 33/88 [00:17<00:29,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:17<00:32,  1.67ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:17<00:30,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:17<00:28,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:17<00:28,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:17<00:31,  1.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:17<00:28,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:17<00:31,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:17<00:30,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  39%|███▊      | 34/88 [00:17<00:31,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:18<00:29,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:18<00:29,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:18<00:28,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:18<00:27,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:18<00:27,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:18<00:30,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:18<00:30,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  40%|███▉      | 35/88 [00:18<00:30,  1.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:18<00:27,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:18<00:26,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:18<00:26,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:18<00:26,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:18<00:28,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:18<00:28,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:18<00:28,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  41%|████      | 36/88 [00:18<00:28,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:19<00:26,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:19<00:26,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:19<00:28,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:19<00:28,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:19<00:26,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:19<00:26,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:19<00:28,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  42%|████▏     | 37/88 [00:19<00:27,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:19<00:26,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:19<00:27,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:19<00:29,  1.71ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:19<00:27,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:19<00:27,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:19<00:26,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:20<00:27,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  43%|████▎     | 38/88 [00:20<00:28,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:20<00:11,  2.99s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:20<00:25,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:20<00:26,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:20<00:27,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:20<00:26,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:20<00:27,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:20<00:26,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:20<00:26,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  44%|████▍     | 39/88 [00:20<00:26,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:20<00:25,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:20<00:25,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:20<00:24,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:21<00:23,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:21<00:25,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:21<00:25,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:21<00:27,  1.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  45%|████▌     | 40/88 [00:21<00:24,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:21<00:24,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:21<00:25,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:21<00:22,  2.06ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:21<00:24,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:21<00:25,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  47%|████▋     | 41/88 [00:21<00:24,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:21<00:25,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:21<00:27,  1.67ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:21<00:23,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:22<00:22,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:22<00:24,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:22<00:26,  1.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:22<00:25,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:22<00:24,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  48%|████▊     | 42/88 [00:22<00:25,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:22<00:24,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:22<00:22,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:22<00:22,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:22<00:24,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:22<00:23,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:22<00:25,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:22<00:23,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:22<00:23,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  49%|████▉     | 43/88 [00:22<00:24,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:22<00:22,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:23<00:08,  2.96s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:23<00:22,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:23<00:23,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:23<00:22,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:23<00:23,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:23<00:23,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  50%|█████     | 44/88 [00:23<00:23,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:23<00:25,  1.69ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:23<00:21,  1.96ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:23<00:22,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:23<00:23,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:23<00:23,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:23<00:23,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:23<00:24,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  51%|█████     | 45/88 [00:23<00:24,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:23<00:23,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:23<00:19,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:24<00:21,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:24<00:22,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:24<00:18,  2.13ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:24<00:22,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:24<00:23,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:24<00:23,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:24<00:21,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  52%|█████▏    | 46/88 [00:24<00:24,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:24<00:19,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:24<00:21,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:24<00:20,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:24<00:22,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:24<00:22,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:24<00:22,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  53%|█████▎    | 47/88 [00:24<00:22,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:24<00:19,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:25<00:20,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:25<00:19,  2.05ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:25<00:20,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:25<00:20,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:25<00:21,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  55%|█████▍    | 48/88 [00:25<00:21,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:25<00:20,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:25<00:22,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:25<00:19,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:25<00:20,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:25<00:21,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:25<00:22,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:25<00:19,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:25<00:22,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:25<00:20,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  56%|█████▌    | 49/88 [00:25<00:22,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:26<00:05,  2.99s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:26<00:19,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:26<00:19,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:26<00:20,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:26<00:21,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:26<00:21,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:26<00:19,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:26<00:20,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  57%|█████▋    | 50/88 [00:26<00:22,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:26<00:17,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:26<00:19,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:26<00:19,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:27<00:20,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:27<00:20,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  58%|█████▊    | 51/88 [00:27<00:21,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:27<00:19,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:27<00:19,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:27<00:18,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:27<00:19,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:27<00:18,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:27<00:18,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:27<00:17,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:27<00:19,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  59%|█████▉    | 52/88 [00:27<00:19,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:27<00:20,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:27<00:17,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:27<00:19,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:28<00:17,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:28<00:19,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:28<00:18,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:28<00:16,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:28<00:20,  1.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:28<00:19,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  60%|██████    | 53/88 [00:28<00:20,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:28<00:17,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:28<00:18,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:28<00:17,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:28<00:16,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:28<00:18,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:28<00:19,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:28<00:18,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:28<00:16,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  61%|██████▏   | 54/88 [00:28<00:20,  1.67ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:28<00:02,  2.91s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:29<00:17,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:29<00:16,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:29<00:17,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:29<00:17,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:29<00:15,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:29<00:18,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:29<00:16,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  62%|██████▎   | 55/88 [00:29<00:18,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:29<00:16,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:29<00:16,  1.99ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:29<00:16,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:29<00:15,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:29<00:17,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:29<00:16,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  64%|██████▎   | 56/88 [00:29<00:17,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:29<00:16,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:30<00:16,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:30<00:16,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:30<00:17,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:30<00:15,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:30<00:15,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:30<00:00,  2.50s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:30<00:00,  2.77s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:30<00:16,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:30<00:18,  1.69ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  65%|██████▍   | 57/88 [00:30<00:19,  1.63ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:30<00:16,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:30<00:16,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:30<00:15,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:30<00:15,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:30<00:15,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:31<00:16,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  66%|██████▌   | 58/88 [00:31<00:17,  1.76ba/s][1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:31<00:16,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:31<00:15,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:31<00:15,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:31<00:15,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:31<00:14,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:31<00:15,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  67%|██████▋   | 59/88 [00:31<00:15,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:31<00:16,  1.67ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:31<00:16,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:31<00:14,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:31<00:15,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:32<00:13,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:32<00:15,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:32<00:14,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:32<00:14,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:32<00:15,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  68%|██████▊   | 60/88 [00:32<00:16,  1.65ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:32<00:14,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:32<00:13,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:32<00:13,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:32<00:13,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:32<00:13,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:32<00:13,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:32<00:15,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  69%|██████▉   | 61/88 [00:32<00:15,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:32<00:13,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:32<00:13,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:33<00:13,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:33<00:13,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:33<00:13,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:33<00:14,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  70%|███████   | 62/88 [00:33<00:14,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:33<00:13,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:33<00:13,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:33<00:12,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:33<00:13,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:33<00:12,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:33<00:12,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:33<00:13,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:33<00:12,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  72%|███████▏  | 63/88 [00:33<00:13,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:33<00:13,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:34<00:13,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:34<00:12,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:34<00:11,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:34<00:12,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:34<00:11,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:34<00:13,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  73%|███████▎  | 64/88 [00:34<00:13,  1.75ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:34<00:12,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:34<00:12,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:34<00:11,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:34<00:11,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:34<00:11,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:34<00:11,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:34<00:12,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  74%|███████▍  | 65/88 [00:35<00:13,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:35<00:12,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:35<00:11,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:35<00:10,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:35<00:11,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:35<00:12,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  75%|███████▌  | 66/88 [00:35<00:11,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:35<00:12,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:35<00:11,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:35<00:11,  1.67ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:35<00:11,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:35<00:10,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:35<00:10,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:35<00:10,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  76%|███████▌  | 67/88 [00:35<00:11,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:36<00:10,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:36<00:11,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:36<00:11,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:36<00:11,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:36<00:10,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:36<00:10,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:36<00:10,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:36<00:11,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  77%|███████▋  | 68/88 [00:36<00:11,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:36<00:09,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:36<00:10,  1.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:36<00:09,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:36<00:10,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:36<00:09,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:37<00:10,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  78%|███████▊  | 69/88 [00:37<00:10,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:37<00:09,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:37<00:10,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:37<00:09,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:37<00:09,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:37<00:09,  1.71ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:37<00:09,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:37<00:09,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  80%|███████▉  | 70/88 [00:37<00:09,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:37<00:09,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:37<00:09,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:37<00:09,  1.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:37<00:09,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:37<00:08,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:38<00:09,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:38<00:09,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:38<00:07,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  81%|████████  | 71/88 [00:38<00:09,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:38<00:08,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:38<00:08,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:38<00:07,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:38<00:09,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:38<00:08,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:38<00:08,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:38<00:07,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  82%|████████▏ | 72/88 [00:38<00:09,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:38<00:08,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:38<00:07,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:38<00:08,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:38<00:07,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:39<00:07,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:39<00:07,  1.94ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:39<00:07,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:39<00:07,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  83%|████████▎ | 73/88 [00:39<00:08,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:39<00:07,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:39<00:06,  2.04ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:39<00:07,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:39<00:08,  1.61ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:39<00:07,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:39<00:06,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  84%|████████▍ | 74/88 [00:39<00:07,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:39<00:07,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:40<00:06,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:40<00:06,  1.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:40<00:07,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:40<00:06,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:40<00:06,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:40<00:05,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:40<00:06,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:40<00:05,  1.97ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:40<00:06,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  85%|████████▌ | 75/88 [00:40<00:07,  1.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:40<00:06,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:40<00:05,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:40<00:06,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:40<00:05,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:41<00:05,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:41<00:05,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:41<00:05,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:41<00:05,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  86%|████████▋ | 76/88 [00:41<00:07,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:41<00:05,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:41<00:05,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:41<00:05,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:41<00:04,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:41<00:05,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:41<00:05,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:41<00:04,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:41<00:05,  1.98ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  88%|████████▊ | 77/88 [00:41<00:06,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:41<00:05,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:42<00:04,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:42<00:05,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:42<00:04,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:42<00:04,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:42<00:05,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  89%|████████▊ | 78/88 [00:42<00:05,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:42<00:04,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:42<00:05,  1.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:42<00:03,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:42<00:04,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:42<00:03,  1.93ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:42<00:03,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:42<00:04,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:42<00:03,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:43<00:04,  1.76ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  90%|████████▉ | 79/88 [00:43<00:05,  1.62ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:43<00:03,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:43<00:03,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:43<00:03,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:43<00:03,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:43<00:03,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:43<00:03,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:43<00:03,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  91%|█████████ | 80/88 [00:43<00:04,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:43<00:03,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:43<00:02,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:43<00:03,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:43<00:02,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:43<00:02,  1.72ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:44<00:03,  1.95ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:44<00:02,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  92%|█████████▏| 81/88 [00:44<00:03,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:44<00:02,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:44<00:02,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:44<00:02,  1.71ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:44<00:02,  1.81ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  93%|█████████▎| 82/88 [00:44<00:03,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:44<00:02,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:44<00:03,  1.66ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:44<00:02,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:44<00:01,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:44<00:02,  1.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:45<00:01,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:45<00:01,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:45<00:01,  1.67ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  94%|█████████▍| 83/88 [00:45<00:02,  1.80ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:45<00:02,  1.68ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:45<00:02,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:45<00:01,  1.78ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:45<00:01,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:45<00:01,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:45<00:01,  1.84ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  95%|█████████▌| 84/88 [00:45<00:02,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:45<00:01,  1.73ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:45<00:01,  1.70ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:45<00:01,  1.82ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:46<00:00,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:46<00:01,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  97%|█████████▋| 85/88 [00:46<00:01,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:46<00:00,  1.79ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:46<00:00,  1.85ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:46<00:00,  1.77ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:46<00:01,  1.71ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:46<00:01,  1.74ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:46<00:00,  2.00ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:46<00:00,  1.90ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:46<00:00,  2.03ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:46<00:00,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:46<00:00,  2.10ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:46<00:00,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:46<00:00,  2.01ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:46<00:00,  1.89ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:46<00:00,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:46<00:00,  1.91ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  98%|█████████▊| 86/88 [00:46<00:01,  1.83ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:46<00:00,  1.92ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:46<00:00,  2.22ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:46<00:00,  1.88ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:46<00:00,  2.25ba/s][1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:46<00:00,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:47<00:00,  2.28ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:47<00:00,  1.87ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:47<00:00,  2.02ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:47<00:00,  2.44ba/s][1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on train dataset: 100%|██████████| 88/88 [00:47<00:00,  1.86ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s][1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s][1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s][1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:46:12 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.18.4/metrics/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpx40gqnsp\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.72k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading: 4.50kB [00:00, 2.68MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:46:13 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.18.4/metrics/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/a4baaf2dafad800b6ec3d1f808759a6b210306ab3e93c096ec6af814df792a8e.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:46:13 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a4baaf2dafad800b6ec3d1f808759a6b210306ab3e93c096ec6af814df792a8e.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:46:13 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.18.4/metrics/squad/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpt_pr8_db\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Downloading: 3.31kB [00:00, 2.10MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:46:13 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.18.4/metrics/squad/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/480146b4006337eae1e243e07b1a8a596d295c7aeaac2bd5d8a2f654c2189c26.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:46:13 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/480146b4006337eae1e243e07b1a8a596d295c7aeaac2bd5d8a2f654c2189c26.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:430] 2022-06-03 21:46:14,156 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:430] 2022-06-03 21:46:14,156 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:457] 2022-06-03 21:46:14,157 >> Using amp half precision backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:457] 2022-06-03 21:46:14,157 >> Using amp half precision backend\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:02<00:27,  2.73s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:02<00:27,  2.80s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:02<00:28,  2.82s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:02<00:28,  2.85s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:02<00:28,  2.88s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:02<00:28,  2.88s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:02<00:28,  2.89s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:02<00:29,  2.96s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:03<00:30,  3.01s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:03<00:30,  3.02s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:03<00:30,  3.02s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:03<00:30,  3.03s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:03<00:30,  3.06s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:03<00:31,  3.19s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:   9%|▉         | 1/11 [00:03<00:32,  3.21s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:23,  2.66s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:23,  2.65s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:24,  2.74s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:24,  2.72s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:24,  2.76s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:25,  2.79s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:25,  2.80s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:25,  2.80s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:25,  2.80s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:25,  2.81s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:26,  2.89s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:25,  2.86s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:26,  2.96s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:05<00:26,  2.94s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  18%|█▊        | 2/11 [00:06<00:27,  3.06s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:07<00:21,  2.66s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:08<00:21,  2.71s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:08<00:21,  2.71s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:08<00:21,  2.73s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:08<00:22,  2.76s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:08<00:22,  2.79s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:08<00:22,  2.79s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:08<00:22,  2.79s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:08<00:22,  2.78s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:08<00:22,  2.81s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:08<00:22,  2.82s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:08<00:22,  2.86s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:08<00:23,  2.88s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:08<00:23,  2.93s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  27%|██▋       | 3/11 [00:09<00:24,  3.01s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:10<00:19,  2.74s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:10<00:19,  2.76s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:10<00:19,  2.76s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:11<00:19,  2.77s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:11<00:19,  2.79s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:11<00:20,  2.88s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:11<00:20,  2.86s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:11<00:20,  2.86s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:11<00:20,  2.91s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:11<00:20,  2.86s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:11<00:20,  2.86s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:11<00:20,  2.91s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:11<00:20,  2.95s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:11<00:20,  2.97s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  36%|███▋      | 4/11 [00:12<00:21,  3.09s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:14<00:18,  3.09s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:14<00:18,  3.06s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:14<00:18,  3.05s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:14<00:19,  3.18s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:15<00:18,  3.15s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:15<00:18,  3.11s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:15<00:18,  3.16s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:15<00:19,  3.24s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:15<00:19,  3.27s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:15<00:19,  3.30s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:15<00:19,  3.25s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:15<00:20,  3.34s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:15<00:19,  3.29s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:15<00:19,  3.29s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  45%|████▌     | 5/11 [00:15<00:19,  3.31s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:17<00:15,  3.03s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:17<00:15,  3.01s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:17<00:15,  3.10s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:18<00:15,  3.10s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:18<00:15,  3.16s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:18<00:15,  3.06s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:18<00:15,  3.09s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:18<00:15,  3.18s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:18<00:15,  3.17s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:18<00:16,  3.23s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:18<00:15,  3.16s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:18<00:16,  3.25s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:18<00:16,  3.26s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:18<00:16,  3.21s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  55%|█████▍    | 6/11 [00:18<00:15,  3.20s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:20<00:12,  3.04s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:20<00:12,  3.02s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:21<00:12,  3.09s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:21<00:12,  3.19s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:21<00:12,  3.07s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:21<00:12,  3.11s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:21<00:12,  3.21s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:21<00:12,  3.16s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:21<00:12,  3.19s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:21<00:12,  3.22s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:21<00:13,  3.27s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:21<00:12,  3.24s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:22<00:13,  3.29s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:22<00:12,  3.18s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  64%|██████▎   | 7/11 [00:22<00:13,  3.26s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:23<00:08,  2.99s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:23<00:08,  2.98s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:23<00:09,  3.03s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:24<00:09,  3.10s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:24<00:09,  3.10s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:24<00:09,  3.10s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:24<00:09,  3.16s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:24<00:09,  3.15s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:24<00:09,  3.12s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:24<00:09,  3.16s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:24<00:09,  3.18s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:24<00:09,  3.21s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:25<00:09,  3.09s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:25<00:09,  3.23s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  73%|███████▎  | 8/11 [00:25<00:09,  3.20s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:26<00:06,  3.02s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:26<00:06,  3.00s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:27<00:06,  3.04s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:27<00:06,  3.09s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:27<00:06,  3.04s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:27<00:06,  3.16s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:27<00:06,  3.20s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:27<00:06,  3.17s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:27<00:06,  3.13s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:27<00:06,  3.16s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:28<00:06,  3.09s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:28<00:06,  3.22s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:28<00:06,  3.25s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:28<00:06,  3.20s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  82%|████████▏ | 9/11 [00:28<00:06,  3.29s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:29<00:02,  2.95s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:29<00:02,  2.95s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:29<00:03,  3.00s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:29<00:02,  2.99s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:30<00:02,  2.98s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:30<00:03,  3.06s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:30<00:03,  3.11s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:30<00:03,  3.02s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:30<00:03,  3.13s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:30<00:03,  3.05s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:30<00:02,  2.99s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:30<00:00,  2.54s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:30<00:00,  2.81s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:30<00:00,  2.53s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:30<00:00,  2.82s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:31<00:03,  3.11s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:31<00:03,  3.12s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:31<00:03,  3.14s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.72k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Downloading: 4.50kB [00:00, 2.35MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:31<00:03,  3.11s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:31<00:00,  2.57s/ba][1,mpirank:5,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:31<00:00,  2.87s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:31<00:00,  2.58s/ba][1,mpirank:4,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:31<00:00,  2.89s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.66s/ba][1,mpirank:8,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.93s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.79s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.93s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.67s/ba][1,mpirank:2,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.93s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.59s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.93s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.67s/ba][1,mpirank:7,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.95s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Downloading: 3.31kB [00:00, 1.53MB/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.64s/ba][1,mpirank:11,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.95s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.60s/ba][1,mpirank:13,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.96s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.63s/ba][1,mpirank:9,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.97s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.67s/ba][1,mpirank:12,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.99s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  2.72s/ba][1,mpirank:10,algo-2]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:32<00:00,  3.00s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:33<00:00,  2.67s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Running tokenizer on validation dataset: 100%|██████████| 11/11 [00:33<00:00,  3.00s/ba]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1279] 2022-06-03 21:46:47,974 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1280] 2022-06-03 21:46:47,974 >>   Num examples = 88524\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1281] 2022-06-03 21:46:47,974 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1282] 2022-06-03 21:46:47,974 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1283] 2022-06-03 21:46:47,974 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1279] 2022-06-03 21:46:47,974 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1280] 2022-06-03 21:46:47,974 >>   Num examples = 88524\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1281] 2022-06-03 21:46:47,974 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1282] 2022-06-03 21:46:47,974 >>   Instantaneous batch size per device = 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1283] 2022-06-03 21:46:47,974 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1284] 2022-06-03 21:46:47,974 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1285] 2022-06-03 21:46:47,974 >>   Total optimization steps = 100\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1284] 2022-06-03 21:46:47,974 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1285] 2022-06-03 21:46:47,974 >>   Total optimization steps = 100\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/100 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.290 algo-1:119 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.292 algo-1:123 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.294 algo-1:129 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.294 algo-1:125 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.295 algo-1:127 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.316 algo-1:121 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.316 algo-1:594 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.318 algo-1:117 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.348 algo-2:133 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.377 algo-2:128 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.377 algo-2:135 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.378 algo-2:131 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.382 algo-2:124 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.382 algo-2:126 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.389 algo-2:601 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.394 algo-2:136 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220512-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.494 algo-1:127 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.494 algo-1:123 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.495 algo-1:119 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.495 algo-1:594 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.495 algo-1:117 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.495 algo-1:121 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.495 algo-1:125 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.496 algo-1:129 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.496 algo-1:127 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.496 algo-1:123 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.496 algo-1:127 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.496 algo-1:123 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.496 algo-1:119 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:594 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:119 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:117 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:127 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:123 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:127 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:123 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:125 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:117 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:594 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:129 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:121 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:125 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:119 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.497 algo-1:129 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.498 algo-1:119 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.498 algo-1:117 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.498 algo-1:117 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.498 algo-1:121 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.498 algo-1:594 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.498 algo-1:125 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.498 algo-1:594 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.498 algo-1:125 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.498 algo-1:129 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.498 algo-1:129 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.499 algo-1:121 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.499 algo-1:121 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.549 algo-2:133 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.549 algo-2:135 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.549 algo-2:126 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.549 algo-2:128 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.549 algo-2:131 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.549 algo-2:124 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.550 algo-2:133 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.550 algo-2:135 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.550 algo-2:126 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.550 algo-2:131 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.550 algo-2:128 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.550 algo-2:124 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:135 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:131 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:126 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:133 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:128 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:124 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:131 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:135 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:131 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:133 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:135 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:133 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:124 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:126 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:128 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:124 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:126 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.551 algo-2:128 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.556 algo-2:601 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.558 algo-2:601 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.558 algo-2:601 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.559 algo-2:601 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.559 algo-2:601 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.568 algo-2:136 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.570 algo-2:136 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.570 algo-2:136 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.571 algo-2:136 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.571 algo-2:136 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.718 algo-1:123 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.718 algo-1:123 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.718 algo-1:123 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.718 algo-1:123 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.718 algo-1:123 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:594 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:594 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:594 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:594 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:594 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.719 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.720 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.721 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:127 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:127 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:127 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:127 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:127 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.722 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.723 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.724 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:125 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:125 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:125 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:125 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:125 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.725 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.726 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.727 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:119 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:119 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.728 algo-1:119 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:119 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:119 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:129 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:129 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:129 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:129 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:129 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.729 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:121 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:121 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:121 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:121 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:121 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.730 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.731 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.732 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.733 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.734 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.735 algo-1:117 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.736 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.737 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.738 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.739 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:123 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:123 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:123 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:123 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:123 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.740 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:127 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.741 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.742 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.743 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.744 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.745 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.746 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:125 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:125 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:125 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:125 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:125 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.747 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:129 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:129 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:129 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:129 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:129 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.748 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.749 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:594 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:594 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:594 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.750 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:594 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:594 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.751 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.752 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.753 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.754 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:121 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:121 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:121 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:121 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:121 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:117 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.755 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.756 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.756 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.756 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.756 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.756 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.756 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.756 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.756 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.756 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.756 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.756 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.756 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.756 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.757 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.757 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.757 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.757 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.757 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.757 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.757 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.757 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.757 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.757 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.757 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.757 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.757 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.757 algo-1:119 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.758 algo-1:119 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.758 algo-1:119 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.758 algo-1:119 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2022-06-03 21:46:48.758 algo-1:119 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.783 algo-2:124 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.783 algo-2:124 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.783 algo-2:124 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.783 algo-2:124 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.783 algo-2:124 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.783 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.783 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.783 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.783 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.784 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.785 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.786 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.787 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.788 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.789 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:135 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:135 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:135 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:135 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:135 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.790 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.791 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.792 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:601 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:601 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:601 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:601 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:601 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.793 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.794 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.795 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:133 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:133 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:133 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:133 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.796 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.797 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.798 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.799 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.800 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:124 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.801 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.802 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.803 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:136 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:136 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:136 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:136 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.804 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:136 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.805 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:126 INFO hook.py:560] name:module.bert.embeddings.word_embeddings.weight count_params:31254528\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.806 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:126 INFO hook.py:560] name:module.bert.embeddings.position_embeddings.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:126 INFO hook.py:560] name:module.bert.embeddings.token_type_embeddings.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:126 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:126 INFO hook.py:560] name:module.bert.embeddings.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.807 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.0.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.808 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.1.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.809 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.810 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.2.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.811 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.3.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.812 algo-2:131 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:131 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:131 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:131 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:131 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.4.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.813 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:601 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:601 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.5.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:601 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:601 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:601 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.814 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.6.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.815 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.7.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.816 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.8.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.9.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:128 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.817 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.10.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:133 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:133 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.818 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:133 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:133 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:133 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.819 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.11.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:135 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:135 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:135 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.820 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:135 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:135 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.12.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.821 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.13.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.822 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.14.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.823 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.824 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.15.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.825 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.16.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.826 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.17.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.827 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.828 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.18.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.829 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.19.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.830 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.20.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:126 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.831 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.832 algo-2:126 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.832 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.832 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2022-06-03 21:46:48.832 algo-2:126 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.832 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.832 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.832 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.832 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.832 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.832 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.832 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.832 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.832 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.833 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.833 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.833 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.833 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.21.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.833 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.833 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.833 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.833 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.833 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.833 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.833 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.833 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.833 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.834 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.834 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.834 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.834 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.834 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.834 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.834 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.22.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.834 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.834 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.query.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.834 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.834 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.key.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.834 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.834 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.self.value.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.835 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.835 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.835 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.835 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.attention.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.835 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.835 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.intermediate.dense.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.835 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.835 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.output.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.835 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.835 algo-2:136 INFO hook.py:560] name:module.bert.encoder.layer.23.output.LayerNorm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.835 algo-2:136 INFO hook.py:560] name:module.qa_outputs.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.835 algo-2:136 INFO hook.py:560] name:module.qa_outputs.bias count_params:2\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.836 algo-2:136 INFO hook.py:562] Total Trainable Params: 334094338\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2022-06-03 21:46:48.836 algo-2:136 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 1/100 [00:03<05:26,  3.30s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 2/100 [00:04<03:24,  2.09s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 3/100 [00:05<02:44,  1.70s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 4/100 [00:07<02:31,  1.58s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▌         | 5/100 [00:08<02:17,  1.45s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 6/100 [00:09<02:05,  1.34s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 7/100 [00:10<01:57,  1.26s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 8/100 [00:11<01:52,  1.23s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 9/100 [00:12<01:47,  1.18s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 10/100 [00:13<01:44,  1.16s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 11/100 [00:15<01:43,  1.16s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 12/100 [00:16<01:39,  1.13s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 13/100 [00:17<01:41,  1.16s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 14/100 [00:18<01:38,  1.15s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 15/100 [00:19<01:37,  1.15s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 16/100 [00:20<01:33,  1.11s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 17/100 [00:21<01:32,  1.11s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 18/100 [00:23<01:36,  1.18s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 19/100 [00:24<01:34,  1.17s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 20/100 [00:25<01:31,  1.15s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 21/100 [00:26<01:30,  1.15s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 22/100 [00:27<01:29,  1.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 23/100 [00:28<01:27,  1.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 24/100 [00:29<01:26,  1.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▌       | 25/100 [00:31<01:25,  1.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 26/100 [00:32<01:25,  1.16s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 27/100 [00:33<01:23,  1.15s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 28/100 [00:34<01:21,  1.13s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 29/100 [00:35<01:21,  1.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 30/100 [00:36<01:19,  1.13s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 31/100 [00:37<01:16,  1.11s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 32/100 [00:39<01:17,  1.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 33/100 [00:40<01:15,  1.13s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 34/100 [00:41<01:14,  1.12s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▌      | 35/100 [00:42<01:12,  1.12s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 36/100 [00:43<01:11,  1.12s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 37/100 [00:44<01:10,  1.11s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 38/100 [00:45<01:09,  1.12s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 39/100 [00:46<01:07,  1.11s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 40/100 [00:47<01:06,  1.12s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 41/100 [00:49<01:05,  1.11s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 42/100 [00:50<01:04,  1.11s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 43/100 [00:51<01:03,  1.11s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 44/100 [00:52<01:01,  1.11s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▌     | 45/100 [00:53<01:02,  1.13s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 46/100 [00:54<01:01,  1.13s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 47/100 [00:56<01:03,  1.19s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 48/100 [00:57<01:01,  1.19s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 49/100 [00:58<01:00,  1.18s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 50/100 [00:59<00:58,  1.18s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 51/100 [01:00<00:56,  1.16s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 52/100 [01:01<00:54,  1.15s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 53/100 [01:02<00:53,  1.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 54/100 [01:04<00:52,  1.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 55/100 [01:05<00:51,  1.15s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 56/100 [01:06<00:50,  1.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 57/100 [01:07<00:51,  1.20s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 58/100 [01:08<00:50,  1.20s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 59/100 [01:09<00:47,  1.17s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 60/100 [01:11<00:45,  1.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 61/100 [01:12<00:43,  1.12s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 62/100 [01:13<00:43,  1.13s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 63/100 [01:14<00:41,  1.13s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 64/100 [01:15<00:42,  1.17s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 65/100 [01:16<00:40,  1.17s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 66/100 [01:17<00:39,  1.17s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 67/100 [01:19<00:38,  1.17s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 68/100 [01:20<00:38,  1.20s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 69/100 [01:21<00:36,  1.19s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 70/100 [01:22<00:35,  1.18s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 71/100 [01:23<00:34,  1.18s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 72/100 [01:25<00:34,  1.24s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 73/100 [01:26<00:34,  1.29s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 74/100 [01:27<00:32,  1.25s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 75/100 [01:29<00:30,  1.24s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 76/100 [01:30<00:30,  1.26s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 77/100 [01:31<00:28,  1.25s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 78/100 [01:32<00:27,  1.24s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 79/100 [01:34<00:25,  1.23s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 80/100 [01:35<00:24,  1.22s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 81/100 [01:36<00:23,  1.22s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 82/100 [01:37<00:21,  1.20s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 83/100 [01:38<00:20,  1.19s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 84/100 [01:39<00:19,  1.20s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▌ | 85/100 [01:41<00:18,  1.26s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 86/100 [01:42<00:18,  1.33s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 87/100 [01:44<00:17,  1.33s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 88/100 [01:45<00:15,  1.29s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 89/100 [01:46<00:14,  1.33s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 90/100 [01:48<00:13,  1.31s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 91/100 [01:49<00:11,  1.29s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 92/100 [01:50<00:10,  1.26s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 93/100 [01:51<00:08,  1.26s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 94/100 [01:53<00:07,  1.24s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 95/100 [01:54<00:06,  1.25s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 96/100 [01:55<00:04,  1.23s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 97/100 [01:56<00:03,  1.23s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 98/100 [01:57<00:02,  1.24s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 99/100 [01:59<00:01,  1.31s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 100/100 [02:00<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:1508] 2022-06-03 21:48:48,647 >> \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:1508] 2022-06-03 21:48:48,647 >> \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Training completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'train_runtime': 120.6733, 'train_samples_per_second': 53.036, 'train_steps_per_second': 0.829, 'train_loss': 2.111164855957031, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015                                                 #015[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 100/100 [02:00<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 100/100 [02:00<00:00,  1.21s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2139] 2022-06-03 21:48:48,708 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2139] 2022-06-03 21:48:48,708 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|configuration_utils.py:439] 2022-06-03 21:48:48,709 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|configuration_utils.py:439] 2022-06-03 21:48:48,709 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|modeling_utils.py:1084] 2022-06-03 21:48:50,546 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|modeling_utils.py:1084] 2022-06-03 21:48:50,546 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:2094] 2022-06-03 21:48:50,547 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:2094] 2022-06-03 21:48:50,547 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|tokenization_utils_base.py:2100] 2022-06-03 21:48:50,547 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|tokenization_utils_base.py:2100] 2022-06-03 21:48:50,547 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:***** train metrics *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  epoch                    =       0.07\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  train_loss               =     2.1112\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  train_runtime            = 0:02:00.67\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  train_samples            =      88524\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  train_samples_per_second =     53.036\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  train_steps_per_second   =      0.829\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:48:50 - INFO - __main__ - *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:570] 2022-06-03 21:48:50,589 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:570] 2022-06-03 21:48:50,589 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2389] 2022-06-03 21:48:50,592 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2391] 2022-06-03 21:48:50,592 >>   Num examples = 10784\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|trainer.py:2394] 2022-06-03 21:48:50,592 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2389] 2022-06-03 21:48:50,592 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2391] 2022-06-03 21:48:50,592 >>   Num examples = 10784\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|trainer.py:2394] 2022-06-03 21:48:50,592 >>   Batch size = 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/169 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 2/169 [00:00<00:08, 18.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 4/169 [00:00<00:13, 12.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▎         | 6/169 [00:00<00:15, 10.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▍         | 8/169 [00:00<00:15, 10.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 10/169 [00:00<00:14, 10.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 12/169 [00:01<00:15, 10.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 14/169 [00:01<00:15, 10.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 16/169 [00:01<00:18,  8.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 17/169 [00:01<00:18,  8.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 18/169 [00:01<00:17,  8.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 19/169 [00:02<00:18,  7.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 20/169 [00:02<00:20,  7.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 21/169 [00:02<00:19,  7.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▎        | 23/169 [00:02<00:16,  8.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▍        | 25/169 [00:02<00:17,  8.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 26/169 [00:03<00:21,  6.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 28/169 [00:03<00:17,  8.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 29/169 [00:03<00:16,  8.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 31/169 [00:03<00:14,  9.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 32/169 [00:03<00:14,  9.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 34/169 [00:03<00:13, 10.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██▏       | 36/169 [00:03<00:12, 11.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 38/169 [00:04<00:12, 10.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▎       | 40/169 [00:04<00:12, 10.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▍       | 42/169 [00:04<00:12, 10.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 44/169 [00:04<00:12, 10.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 46/169 [00:04<00:11, 10.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 48/169 [00:05<00:14,  8.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 49/169 [00:05<00:13,  8.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 51/169 [00:05<00:12,  9.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 52/169 [00:05<00:14,  7.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 54/169 [00:06<00:15,  7.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 55/169 [00:06<00:15,  7.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▎      | 57/169 [00:06<00:12,  8.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▍      | 59/169 [00:06<00:11,  9.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 61/169 [00:06<00:10,  9.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 63/169 [00:07<00:15,  7.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 65/169 [00:07<00:13,  7.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 66/169 [00:07<00:12,  8.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 68/169 [00:07<00:11,  8.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 69/169 [00:07<00:11,  8.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 71/169 [00:07<00:09,  9.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 73/169 [00:08<00:09, 10.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 75/169 [00:08<00:08, 11.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 77/169 [00:08<00:07, 11.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 79/169 [00:08<00:07, 11.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 81/169 [00:08<00:07, 11.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 83/169 [00:08<00:07, 12.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 85/169 [00:09<00:07, 11.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████▏    | 87/169 [00:09<00:07, 11.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 89/169 [00:09<00:07, 11.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 91/169 [00:09<00:08,  8.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 93/169 [00:09<00:07,  9.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 95/169 [00:10<00:08,  8.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 96/169 [00:10<00:10,  7.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 98/169 [00:10<00:08,  8.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▊    | 99/169 [00:10<00:08,  8.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|█████▉    | 101/169 [00:10<00:07,  9.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 103/169 [00:11<00:06, 10.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 105/169 [00:11<00:06, 10.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 107/169 [00:11<00:07,  8.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 108/169 [00:11<00:08,  6.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 110/169 [00:12<00:07,  7.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▋   | 112/169 [00:12<00:06,  8.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 114/169 [00:12<00:05,  9.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▊   | 116/169 [00:12<00:05,  9.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|██████▉   | 118/169 [00:12<00:05,  9.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 119/169 [00:12<00:05,  9.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 121/169 [00:13<00:04, 10.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 123/169 [00:13<00:04, 10.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 125/169 [00:13<00:03, 11.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 127/169 [00:13<00:03, 11.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▋  | 129/169 [00:13<00:03, 10.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 131/169 [00:13<00:03, 10.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▊  | 133/169 [00:14<00:03, 10.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|███████▉  | 135/169 [00:14<00:03,  9.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 137/169 [00:14<00:03,  9.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 138/169 [00:14<00:03,  9.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 139/169 [00:14<00:03,  9.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 141/169 [00:15<00:03,  7.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▍ | 143/169 [00:15<00:03,  8.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 145/169 [00:15<00:02,  9.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 147/169 [00:15<00:02, 10.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 149/169 [00:16<00:02,  7.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 151/169 [00:16<00:02,  8.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 153/169 [00:16<00:01,  9.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 155/169 [00:16<00:01,  9.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 157/169 [00:16<00:01, 10.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 159/169 [00:16<00:00, 10.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 161/169 [00:17<00:00, 10.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▋| 163/169 [00:17<00:00,  8.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 164/169 [00:17<00:00,  6.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 165/169 [00:17<00:00,  7.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 166/169 [00:18<00:00,  7.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 168/169 [00:18<00:00,  8.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  0%|          | 35/10570 [00:00<00:30, 340.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  0%|          | 33/10570 [00:00<00:32, 328.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  1%|          | 73/10570 [00:00<00:28, 362.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  0%|          | 35/10570 [00:00<00:30, 341.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  0%|          | 34/10570 [00:00<00:31, 338.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  0%|          | 34/10570 [00:00<00:31, 336.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  1%|          | 71/10570 [00:00<00:29, 356.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  1%|          | 112/10570 [00:00<00:27, 373.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  1%|          | 73/10570 [00:00<00:28, 362.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  0%|          | 35/10570 [00:00<00:30, 344.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  0%|          | 34/10570 [00:00<00:31, 338.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  0%|          | 34/10570 [00:00<00:31, 332.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  1%|          | 72/10570 [00:00<00:29, 359.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  1%|          | 72/10570 [00:00<00:29, 357.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  1%|          | 110/10570 [00:00<00:28, 370.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  1%|▏         | 151/10570 [00:00<00:27, 379.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  1%|          | 112/10570 [00:00<00:27, 374.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  1%|          | 74/10570 [00:00<00:28, 366.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  0%|          | 34/10570 [00:00<00:31, 338.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  1%|          | 72/10570 [00:00<00:29, 360.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  1%|          | 71/10570 [00:00<00:29, 353.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  1%|          | 111/10570 [00:00<00:28, 370.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  1%|          | 111/10570 [00:00<00:28, 368.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  1%|▏         | 149/10570 [00:00<00:27, 378.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  0%|          | 34/10570 [00:00<00:31, 338.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  2%|▏         | 189/10570 [00:00<00:27, 379.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  1%|▏         | 151/10570 [00:00<00:27, 379.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  0%|          | 34/10570 [00:00<00:31, 333.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  1%|          | 114/10570 [00:00<00:27, 379.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  1%|          | 72/10570 [00:00<00:29, 359.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  1%|          | 111/10570 [00:00<00:28, 371.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  1%|          | 109/10570 [00:00<00:28, 364.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  1%|▏         | 150/10570 [00:00<00:27, 376.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  1%|▏         | 150/10570 [00:00<00:27, 374.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  2%|▏         | 187/10570 [00:00<00:27, 377.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  1%|          | 71/10570 [00:00<00:29, 353.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  2%|▏         | 227/10570 [00:00<00:28, 357.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  2%|▏         | 189/10570 [00:00<00:27, 377.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  1%|          | 71/10570 [00:00<00:29, 353.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  1%|▏         | 153/10570 [00:00<00:27, 381.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  1%|          | 111/10570 [00:00<00:28, 372.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  1%|▏         | 150/10570 [00:00<00:27, 377.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  1%|▏         | 148/10570 [00:00<00:28, 371.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  0%|          | 34/10570 [00:00<00:31, 336.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  2%|▏         | 188/10570 [00:00<00:27, 375.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  2%|▏         | 188/10570 [00:00<00:27, 373.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  1%|          | 110/10570 [00:00<00:28, 367.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  2%|▏         | 225/10570 [00:00<00:28, 357.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:49:24 - INFO - utils_qa - Post-processing 10570 example predictions split into 10784 features.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  1%|          | 109/10570 [00:00<00:28, 364.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  2%|▏         | 227/10570 [00:00<00:29, 354.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  2%|▏         | 192/10570 [00:00<00:27, 381.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  1%|▏         | 150/10570 [00:00<00:27, 378.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  2%|▏         | 188/10570 [00:00<00:27, 376.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  2%|▏         | 186/10570 [00:00<00:27, 370.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  1%|          | 71/10570 [00:00<00:29, 355.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  2%|▏         | 263/10570 [00:00<00:34, 301.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  2%|▏         | 226/10570 [00:00<00:29, 353.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  2%|▏         | 226/10570 [00:00<00:29, 353.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  1%|▏         | 149/10570 [00:00<00:28, 371.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 33/10570 [00:00<00:32, 325.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  1%|▏         | 148/10570 [00:00<00:28, 370.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  2%|▏         | 188/10570 [00:00<00:27, 377.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  1%|          | 110/10570 [00:00<00:28, 368.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  2%|▏         | 261/10570 [00:00<00:33, 305.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  2%|▏         | 231/10570 [00:00<00:28, 357.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  2%|▏         | 226/10570 [00:00<00:29, 356.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  2%|▏         | 224/10570 [00:00<00:29, 347.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  3%|▎         | 295/10570 [00:00<00:35, 289.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  2%|▏         | 187/10570 [00:00<00:27, 374.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  2%|▏         | 263/10570 [00:00<00:34, 297.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  0%|          | 33/10570 [00:00<00:32, 329.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 70/10570 [00:00<00:30, 346.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  2%|▏         | 186/10570 [00:00<00:28, 370.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  2%|▏         | 262/10570 [00:00<00:34, 294.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  0%|          | 0/10570 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  2%|▏         | 262/10570 [00:00<00:34, 299.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  1%|▏         | 149/10570 [00:00<00:27, 376.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  2%|▏         | 226/10570 [00:00<00:28, 357.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  0%|          | 34/10570 [00:00<00:31, 337.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  3%|▎         | 293/10570 [00:00<00:35, 286.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  3%|▎         | 333/10570 [00:01<00:32, 312.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  1%|          | 70/10570 [00:00<00:30, 348.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  2%|▏         | 225/10570 [00:00<00:29, 355.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 109/10570 [00:00<00:28, 362.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  2%|▏         | 262/10570 [00:00<00:34, 302.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  3%|▎         | 295/10570 [00:00<00:35, 285.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  2%|▏         | 260/10570 [00:00<00:34, 296.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  3%|▎         | 268/10570 [00:00<00:35, 291.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  2%|▏         | 224/10570 [00:00<00:29, 351.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  0%|          | 33/10570 [00:00<00:32, 326.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  2%|▏         | 187/10570 [00:00<00:27, 376.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  3%|▎         | 294/10570 [00:00<00:36, 281.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  1%|          | 72/10570 [00:00<00:29, 357.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  3%|▎         | 294/10570 [00:00<00:36, 284.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  3%|▎         | 331/10570 [00:01<00:32, 310.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  4%|▎         | 373/10570 [00:01<00:30, 335.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  1%|          | 108/10570 [00:00<00:29, 359.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  2%|▏         | 262/10570 [00:00<00:33, 303.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|▏         | 148/10570 [00:00<00:27, 372.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  3%|▎         | 333/10570 [00:01<00:33, 308.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  3%|▎         | 300/10570 [00:00<00:34, 296.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  3%|▎         | 294/10570 [00:00<00:35, 287.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  1%|          | 69/10570 [00:00<00:30, 345.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  2%|▏         | 261/10570 [00:00<00:34, 302.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  2%|▏         | 225/10570 [00:00<00:29, 355.44it/s][1,mpirank:6,algo-1]<stderr>:#015  3%|▎         | 331/10570 [00:01<00:33, 304.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  3%|▎         | 291/10570 [00:00<00:38, 270.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  3%|▎         | 331/10570 [00:01<00:33, 307.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  1%|          | 111/10570 [00:00<00:28, 368.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  4%|▎         | 370/10570 [00:01<00:30, 332.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  4%|▍         | 408/10570 [00:01<00:29, 338.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  2%|▏         | 260/10570 [00:00<00:34, 301.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  1%|▏         | 147/10570 [00:00<00:28, 367.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 186/10570 [00:00<00:27, 372.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  4%|▎         | 372/10570 [00:01<00:30, 331.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  3%|▎         | 339/10570 [00:01<00:31, 320.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  3%|▎         | 294/10570 [00:00<00:35, 287.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  3%|▎         | 332/10570 [00:01<00:32, 310.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  1%|          | 107/10570 [00:00<00:29, 358.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  4%|▎         | 370/10570 [00:01<00:31, 327.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  3%|▎         | 326/10570 [00:01<00:35, 290.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  4%|▎         | 370/10570 [00:01<00:31, 328.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  1%|▏         | 150/10570 [00:00<00:27, 374.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  4%|▍         | 406/10570 [00:01<00:30, 337.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  4%|▍         | 444/10570 [00:01<00:29, 344.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  3%|▎         | 293/10570 [00:00<00:36, 282.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  2%|▏         | 184/10570 [00:00<00:28, 360.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  4%|▍         | 407/10570 [00:01<00:30, 336.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  2%|▏         | 261/10570 [00:00<00:34, 301.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  3%|▎         | 292/10570 [00:00<00:36, 279.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 224/10570 [00:00<00:29, 354.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  3%|▎         | 332/10570 [00:01<00:32, 310.92it/s][1,mpirank:3,algo-1]<stderr>:#015  4%|▎         | 379/10570 [00:01<00:29, 340.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  4%|▎         | 371/10570 [00:01<00:30, 332.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  1%|▏         | 146/10570 [00:00<00:28, 368.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  4%|▍         | 406/10570 [00:01<00:30, 334.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  3%|▎         | 364/10570 [00:01<00:32, 312.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  4%|▍         | 406/10570 [00:01<00:30, 334.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  2%|▏         | 188/10570 [00:00<00:27, 373.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  4%|▍         | 443/10570 [00:01<00:29, 345.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  5%|▍         | 481/10570 [00:01<00:28, 350.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  3%|▎         | 331/10570 [00:01<00:33, 306.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  4%|▍         | 443/10570 [00:01<00:29, 342.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  2%|▏         | 221/10570 [00:00<00:29, 346.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  3%|▎         | 329/10570 [00:01<00:33, 301.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  4%|▎         | 371/10570 [00:01<00:30, 332.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  4%|▍         | 415/10570 [00:01<00:29, 340.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  4%|▍         | 407/10570 [00:01<00:30, 337.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  2%|▏         | 183/10570 [00:00<00:28, 367.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  3%|▎         | 293/10570 [00:00<00:36, 281.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  4%|▍         | 442/10570 [00:01<00:29, 341.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  4%|▍         | 400/10570 [00:01<00:31, 324.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  4%|▍         | 442/10570 [00:01<00:29, 341.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|▏         | 260/10570 [00:00<00:34, 302.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  5%|▍         | 520/10570 [00:01<00:27, 362.09it/s][1,mpirank:9,algo-2]<stderr>:#015  5%|▍         | 480/10570 [00:01<00:28, 350.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  2%|▏         | 226/10570 [00:00<00:29, 352.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  4%|▎         | 370/10570 [00:01<00:31, 328.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  5%|▍         | 479/10570 [00:01<00:29, 347.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  3%|▎         | 368/10570 [00:01<00:31, 324.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  4%|▍         | 407/10570 [00:01<00:30, 337.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  4%|▍         | 453/10570 [00:01<00:28, 351.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  4%|▍         | 444/10570 [00:01<00:29, 344.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  3%|▎         | 330/10570 [00:01<00:33, 305.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  2%|▏         | 220/10570 [00:00<00:29, 352.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  2%|▏         | 256/10570 [00:00<00:33, 307.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  5%|▍         | 478/10570 [00:01<00:29, 344.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  4%|▍         | 435/10570 [00:01<00:30, 330.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  5%|▍         | 478/10570 [00:01<00:29, 346.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  5%|▍         | 519/10570 [00:01<00:27, 361.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  5%|▌         | 557/10570 [00:01<00:27, 363.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  4%|▍         | 406/10570 [00:01<00:30, 335.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  5%|▍         | 518/10570 [00:01<00:28, 357.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 292/10570 [00:00<00:36, 279.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  4%|▍         | 403/10570 [00:01<00:30, 329.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  4%|▍         | 444/10570 [00:01<00:29, 345.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  5%|▍         | 491/10570 [00:01<00:28, 358.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  5%|▍         | 481/10570 [00:01<00:28, 350.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  2%|▏         | 262/10570 [00:00<00:34, 298.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  3%|▎         | 369/10570 [00:01<00:31, 327.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  4%|▍         | 470/10570 [00:01<00:30, 336.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  5%|▍         | 517/10570 [00:01<00:28, 355.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  5%|▍         | 517/10570 [00:01<00:28, 357.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  5%|▌         | 556/10570 [00:01<00:27, 362.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  6%|▌         | 594/10570 [00:01<00:27, 360.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  4%|▍         | 442/10570 [00:01<00:29, 342.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  2%|▏         | 256/10570 [00:00<00:33, 309.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  3%|▎         | 288/10570 [00:00<00:37, 271.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 328/10570 [00:01<00:34, 300.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  5%|▌         | 555/10570 [00:01<00:28, 357.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  4%|▍         | 438/10570 [00:01<00:30, 334.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  5%|▍         | 481/10570 [00:01<00:28, 351.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  5%|▌         | 531/10570 [00:01<00:27, 368.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  5%|▍         | 520/10570 [00:01<00:27, 360.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  4%|▍         | 405/10570 [00:01<00:30, 334.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  5%|▍         | 507/10570 [00:01<00:29, 345.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  5%|▌         | 553/10570 [00:01<00:28, 356.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  5%|▌         | 554/10570 [00:01<00:27, 359.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  3%|▎         | 294/10570 [00:00<00:36, 282.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  6%|▌         | 593/10570 [00:01<00:27, 361.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  6%|▌         | 633/10570 [00:01<00:26, 368.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  5%|▍         | 479/10570 [00:01<00:29, 347.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|▎         | 366/10570 [00:01<00:31, 321.31it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  3%|▎         | 324/10570 [00:01<00:34, 292.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  6%|▌         | 591/10570 [00:01<00:28, 356.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  4%|▍         | 473/10570 [00:01<00:29, 337.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  5%|▍         | 520/10570 [00:01<00:27, 361.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  5%|▌         | 557/10570 [00:01<00:27, 361.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  5%|▌         | 569/10570 [00:01<00:27, 364.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  3%|▎         | 288/10570 [00:00<00:37, 272.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  4%|▍         | 441/10570 [00:01<00:29, 341.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  5%|▌         | 544/10570 [00:01<00:28, 352.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  6%|▌         | 589/10570 [00:01<00:28, 355.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  6%|▌         | 591/10570 [00:01<00:27, 356.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  3%|▎         | 332/10570 [00:01<00:33, 306.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  6%|▋         | 670/10570 [00:01<00:26, 368.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  6%|▌         | 632/10570 [00:01<00:27, 367.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  5%|▍         | 517/10570 [00:01<00:28, 355.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 402/10570 [00:01<00:30, 330.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  3%|▎         | 362/10570 [00:01<00:32, 315.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  6%|▌         | 629/10570 [00:01<00:27, 361.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  5%|▍         | 511/10570 [00:01<00:28, 347.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  5%|▌         | 557/10570 [00:01<00:27, 362.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  6%|▌         | 594/10570 [00:01<00:27, 359.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  6%|▌         | 606/10570 [00:01<00:27, 363.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  3%|▎         | 324/10570 [00:01<00:34, 295.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  5%|▍         | 477/10570 [00:01<00:29, 345.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  6%|▌         | 626/10570 [00:01<00:27, 359.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  5%|▌         | 580/10570 [00:01<00:28, 351.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  6%|▌         | 629/10570 [00:01<00:27, 361.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  4%|▎         | 371/10570 [00:01<00:31, 328.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  7%|▋         | 707/10570 [00:02<00:26, 366.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  6%|▋         | 669/10570 [00:01<00:27, 366.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  5%|▌         | 554/10570 [00:01<00:27, 358.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 437/10570 [00:01<00:30, 336.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  4%|▍         | 398/10570 [00:01<00:31, 326.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  6%|▋         | 666/10570 [00:01<00:27, 362.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  5%|▌         | 548/10570 [00:01<00:28, 354.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  6%|▌         | 594/10570 [00:01<00:27, 360.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  6%|▌         | 643/10570 [00:01<00:27, 365.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  6%|▌         | 633/10570 [00:01<00:27, 366.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  3%|▎         | 362/10570 [00:01<00:32, 317.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  5%|▍         | 516/10570 [00:01<00:28, 356.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  6%|▋         | 663/10570 [00:01<00:27, 359.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  6%|▌         | 616/10570 [00:01<00:28, 349.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  4%|▍         | 406/10570 [00:01<00:30, 333.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  6%|▋         | 666/10570 [00:01<00:27, 361.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  7%|▋         | 706/10570 [00:02<00:27, 365.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  7%|▋         | 744/10570 [00:02<00:27, 358.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  6%|▌         | 591/10570 [00:01<00:27, 357.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|▍         | 473/10570 [00:01<00:29, 341.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  4%|▍         | 432/10570 [00:01<00:30, 329.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  7%|▋         | 703/10570 [00:02<00:27, 364.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  6%|▌         | 584/10570 [00:01<00:28, 353.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  6%|▌         | 633/10570 [00:01<00:27, 367.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  6%|▋         | 682/10570 [00:01<00:26, 372.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  6%|▋         | 670/10570 [00:01<00:27, 366.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  4%|▍         | 398/10570 [00:01<00:30, 328.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  5%|▌         | 553/10570 [00:01<00:27, 358.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  7%|▋         | 700/10570 [00:02<00:27, 362.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  6%|▌         | 652/10570 [00:01<00:28, 351.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  4%|▍         | 442/10570 [00:01<00:29, 340.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  7%|▋         | 703/10570 [00:02<00:27, 361.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  6%|▌         | 629/10570 [00:01<00:27, 362.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  7%|▋         | 780/10570 [00:02<00:27, 350.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  7%|▋         | 743/10570 [00:02<00:28, 350.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▍         | 511/10570 [00:01<00:28, 352.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  4%|▍         | 467/10570 [00:01<00:30, 333.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  7%|▋         | 740/10570 [00:02<00:27, 356.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  6%|▌         | 620/10570 [00:01<00:28, 352.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  6%|▋         | 670/10570 [00:01<00:26, 366.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  7%|▋         | 707/10570 [00:02<00:27, 364.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  4%|▍         | 433/10570 [00:01<00:30, 333.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  7%|▋         | 720/10570 [00:02<00:26, 365.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  6%|▌         | 590/10570 [00:01<00:27, 356.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  7%|▋         | 689/10570 [00:02<00:27, 355.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  7%|▋         | 737/10570 [00:02<00:27, 355.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  5%|▍         | 478/10570 [00:01<00:29, 345.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  7%|▋         | 740/10570 [00:02<00:27, 354.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  6%|▋         | 666/10570 [00:01<00:27, 362.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  7%|▋         | 779/10570 [00:02<00:28, 345.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|▌         | 549/10570 [00:01<00:27, 358.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  5%|▍         | 504/10570 [00:01<00:29, 342.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  8%|▊         | 816/10570 [00:02<00:29, 334.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  6%|▌         | 656/10570 [00:01<00:28, 353.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  7%|▋         | 776/10570 [00:02<00:28, 349.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  7%|▋         | 707/10570 [00:02<00:26, 365.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  4%|▍         | 468/10570 [00:01<00:29, 336.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  7%|▋         | 744/10570 [00:02<00:27, 356.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  6%|▌         | 627/10570 [00:01<00:27, 360.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  7%|▋         | 757/10570 [00:02<00:27, 357.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  7%|▋         | 725/10570 [00:02<00:28, 348.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  5%|▍         | 517/10570 [00:01<00:28, 356.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  7%|▋         | 773/10570 [00:02<00:28, 348.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  7%|▋         | 776/10570 [00:02<00:28, 346.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  7%|▋         | 703/10570 [00:02<00:27, 363.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  5%|▌         | 542/10570 [00:01<00:28, 351.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  8%|▊         | 850/10570 [00:02<00:28, 335.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 586/10570 [00:01<00:28, 355.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  8%|▊         | 814/10570 [00:02<00:29, 331.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  7%|▋         | 693/10570 [00:02<00:27, 358.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  7%|▋         | 744/10570 [00:02<00:27, 357.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  5%|▍         | 506/10570 [00:01<00:29, 346.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  8%|▊         | 812/10570 [00:02<00:29, 333.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  6%|▋         | 664/10570 [00:01<00:27, 361.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  7%|▋         | 780/10570 [00:02<00:28, 349.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  8%|▊         | 793/10570 [00:02<00:28, 348.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  5%|▌         | 554/10570 [00:01<00:27, 358.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  7%|▋         | 760/10570 [00:02<00:28, 342.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  8%|▊         | 808/10570 [00:02<00:29, 330.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  7%|▋         | 740/10570 [00:02<00:27, 354.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  8%|▊         | 811/10570 [00:02<00:29, 330.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  8%|▊         | 887/10570 [00:02<00:28, 343.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 622/10570 [00:01<00:27, 355.85it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  5%|▌         | 578/10570 [00:01<00:28, 349.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  8%|▊         | 848/10570 [00:02<00:29, 331.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  7%|▋         | 729/10570 [00:02<00:28, 350.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  5%|▌         | 544/10570 [00:01<00:28, 354.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  8%|▊         | 846/10570 [00:02<00:29, 332.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  7%|▋         | 780/10570 [00:02<00:27, 350.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  7%|▋         | 701/10570 [00:02<00:27, 363.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  8%|▊         | 815/10570 [00:02<00:29, 333.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  6%|▌         | 591/10570 [00:01<00:27, 356.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  8%|▊         | 828/10570 [00:02<00:29, 334.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  8%|▊         | 795/10570 [00:02<00:29, 332.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  8%|▊         | 842/10570 [00:02<00:29, 328.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  8%|▊         | 845/10570 [00:02<00:29, 330.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  7%|▋         | 776/10570 [00:02<00:28, 347.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|▌         | 659/10570 [00:01<00:27, 358.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  6%|▌         | 614/10570 [00:01<00:28, 349.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  9%|▊         | 922/10570 [00:02<00:28, 337.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  8%|▊         | 885/10570 [00:02<00:28, 340.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  7%|▋         | 765/10570 [00:02<00:28, 345.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  5%|▌         | 580/10570 [00:01<00:28, 352.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  8%|▊         | 883/10570 [00:02<00:28, 341.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  7%|▋         | 738/10570 [00:02<00:27, 355.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  8%|▊         | 816/10570 [00:02<00:29, 333.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  8%|▊         | 849/10570 [00:02<00:29, 333.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  8%|▊         | 864/10570 [00:02<00:28, 340.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  6%|▌         | 629/10570 [00:01<00:27, 361.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  8%|▊         | 878/10570 [00:02<00:28, 335.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  8%|▊         | 829/10570 [00:02<00:30, 321.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  8%|▊         | 881/10570 [00:02<00:28, 337.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 697/10570 [00:02<00:27, 362.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  6%|▌         | 650/10570 [00:01<00:28, 350.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  9%|▉         | 958/10570 [00:02<00:28, 342.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  9%|▊         | 920/10570 [00:02<00:28, 335.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  8%|▊         | 811/10570 [00:02<00:29, 329.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  6%|▌         | 616/10570 [00:01<00:28, 351.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  8%|▊         | 800/10570 [00:02<00:29, 331.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  9%|▊         | 918/10570 [00:02<00:28, 336.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  7%|▋         | 774/10570 [00:02<00:28, 347.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  8%|▊         | 850/10570 [00:02<00:29, 334.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  8%|▊         | 886/10570 [00:02<00:28, 341.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  6%|▋         | 666/10570 [00:01<00:27, 361.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  9%|▊         | 901/10570 [00:02<00:27, 346.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  9%|▊         | 912/10570 [00:02<00:28, 334.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  8%|▊         | 864/10570 [00:02<00:29, 326.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  9%|▊         | 915/10570 [00:02<00:28, 334.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  7%|▋         | 688/10570 [00:02<00:27, 356.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 734/10570 [00:02<00:27, 353.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015  9%|▉         | 993/10570 [00:02<00:28, 337.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  9%|▉         | 956/10570 [00:02<00:28, 340.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  8%|▊         | 845/10570 [00:02<00:29, 329.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  6%|▌         | 652/10570 [00:01<00:28, 354.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  9%|▉         | 953/10570 [00:02<00:28, 339.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  8%|▊         | 834/10570 [00:02<00:30, 322.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  8%|▊         | 887/10570 [00:02<00:28, 342.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  7%|▋         | 703/10570 [00:02<00:27, 363.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  9%|▊         | 921/10570 [00:02<00:28, 335.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  9%|▉         | 936/10570 [00:02<00:28, 341.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  8%|▊         | 809/10570 [00:02<00:29, 329.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  9%|▉         | 946/10570 [00:02<00:28, 335.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  9%|▊         | 899/10570 [00:02<00:29, 333.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  9%|▉         | 950/10570 [00:02<00:28, 336.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  7%|▋         | 724/10570 [00:02<00:28, 348.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  8%|▊         | 881/10570 [00:02<00:28, 338.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|▋         | 770/10570 [00:02<00:28, 347.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015  9%|▉         | 991/10570 [00:02<00:28, 336.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 10%|▉         | 1027/10570 [00:02<00:29, 325.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  7%|▋         | 690/10570 [00:02<00:27, 359.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015  9%|▉         | 988/10570 [00:02<00:28, 335.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  8%|▊         | 869/10570 [00:02<00:29, 329.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  9%|▊         | 922/10570 [00:02<00:28, 336.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  9%|▉         | 956/10570 [00:02<00:28, 339.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015  9%|▉         | 971/10570 [00:02<00:28, 340.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  7%|▋         | 740/10570 [00:02<00:27, 354.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  8%|▊         | 843/10570 [00:02<00:29, 328.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015  9%|▉         | 980/10570 [00:02<00:28, 333.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  9%|▉         | 933/10570 [00:02<00:29, 328.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015  9%|▉         | 984/10570 [00:02<00:28, 333.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  9%|▊         | 915/10570 [00:02<00:28, 335.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  7%|▋         | 759/10570 [00:02<00:28, 341.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 10%|▉         | 1025/10570 [00:02<00:29, 324.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 805/10570 [00:02<00:29, 331.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 10%|█         | 1060/10570 [00:03<00:30, 312.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  7%|▋         | 727/10570 [00:02<00:28, 349.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  9%|▊         | 904/10570 [00:02<00:28, 334.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 10%|▉         | 1022/10570 [00:02<00:29, 325.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  9%|▉         | 958/10570 [00:02<00:28, 341.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015  9%|▉         | 991/10570 [00:02<00:28, 335.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  8%|▊         | 879/10570 [00:02<00:28, 336.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 10%|▉         | 1006/10570 [00:02<00:28, 337.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  7%|▋         | 776/10570 [00:02<00:28, 346.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  9%|▉         | 967/10570 [00:02<00:29, 329.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 10%|▉         | 1014/10570 [00:02<00:29, 325.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 10%|▉         | 1018/10570 [00:02<00:29, 325.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  9%|▉         | 950/10570 [00:02<00:28, 336.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  8%|▊         | 794/10570 [00:02<00:29, 331.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 839/10570 [00:02<00:29, 326.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 10%|█         | 1058/10570 [00:03<00:30, 312.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 10%|█         | 1092/10570 [00:03<00:31, 305.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  9%|▉         | 938/10570 [00:02<00:29, 330.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  7%|▋         | 763/10570 [00:02<00:28, 343.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015  9%|▉         | 993/10570 [00:02<00:28, 336.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 10%|▉         | 1055/10570 [00:03<00:30, 312.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  9%|▊         | 913/10570 [00:02<00:28, 333.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 10%|▉         | 1025/10570 [00:02<00:29, 323.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 10%|▉         | 1040/10570 [00:03<00:29, 320.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015  9%|▉         | 1000/10570 [00:03<00:29, 328.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  8%|▊         | 811/10570 [00:02<00:29, 328.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 10%|▉         | 1047/10570 [00:03<00:30, 311.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015  9%|▉         | 984/10570 [00:02<00:28, 334.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 10%|▉         | 1051/10570 [00:03<00:30, 312.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|▊         | 874/10570 [00:02<00:29, 331.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  8%|▊         | 828/10570 [00:02<00:30, 317.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015  9%|▉         | 972/10570 [00:02<00:29, 329.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 10%|█         | 1090/10570 [00:03<00:31, 304.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 11%|█         | 1123/10570 [00:03<00:32, 292.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  8%|▊         | 798/10570 [00:02<00:29, 330.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  9%|▉         | 947/10570 [00:02<00:28, 334.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 10%|█         | 1087/10570 [00:03<00:31, 304.81it/s][1,mpirank:13,algo-2]<stderr>:#015 10%|▉         | 1027/10570 [00:02<00:29, 323.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 10%|█         | 1058/10570 [00:03<00:30, 311.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  8%|▊         | 845/10570 [00:02<00:29, 327.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 10%|█         | 1073/10570 [00:03<00:30, 311.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 10%|▉         | 1033/10570 [00:03<00:30, 312.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 10%|█         | 1079/10570 [00:03<00:31, 303.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 10%|█         | 1083/10570 [00:03<00:31, 304.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 10%|▉         | 1018/10570 [00:03<00:29, 320.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▊         | 908/10570 [00:02<00:29, 332.68it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  8%|▊         | 862/10570 [00:02<00:30, 323.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 10%|▉         | 1005/10570 [00:02<00:29, 326.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 11%|█         | 1154/10570 [00:03<00:31, 294.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015  9%|▉         | 981/10570 [00:02<00:28, 334.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  8%|▊         | 832/10570 [00:02<00:30, 318.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 11%|█         | 1121/10570 [00:03<00:34, 276.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 10%|█         | 1060/10570 [00:03<00:30, 311.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 11%|█         | 1118/10570 [00:03<00:32, 292.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  8%|▊         | 881/10570 [00:02<00:28, 335.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 10%|█         | 1090/10570 [00:03<00:31, 303.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 10%|█         | 1105/10570 [00:03<00:31, 298.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 10%|█         | 1065/10570 [00:03<00:31, 301.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 11%|█         | 1110/10570 [00:03<00:32, 291.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 942/10570 [00:02<00:29, 331.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  8%|▊         | 897/10570 [00:02<00:29, 329.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 11%|█         | 1114/10570 [00:03<00:32, 292.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 10%|▉         | 1051/10570 [00:03<00:30, 307.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 11%|█         | 1185/10570 [00:03<00:31, 296.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 10%|▉         | 1038/10570 [00:03<00:30, 310.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  8%|▊         | 867/10570 [00:02<00:29, 325.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 10%|▉         | 1015/10570 [00:02<00:29, 323.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 11%|█         | 1151/10570 [00:03<00:33, 281.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 11%|█         | 1148/10570 [00:03<00:32, 293.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  9%|▊         | 915/10570 [00:02<00:29, 331.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 10%|█         | 1092/10570 [00:03<00:31, 304.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 11%|█         | 1135/10570 [00:03<00:31, 298.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 11%|█         | 1121/10570 [00:03<00:32, 291.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 11%|█         | 1140/10570 [00:03<00:32, 292.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 10%|█         | 1096/10570 [00:03<00:32, 288.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|▉         | 976/10570 [00:02<00:28, 331.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 11%|█         | 1144/10570 [00:03<00:32, 292.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  9%|▉         | 931/10570 [00:02<00:29, 323.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 10%|█         | 1082/10570 [00:03<00:31, 301.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 11%|█▏        | 1215/10570 [00:03<00:31, 295.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  9%|▊         | 902/10570 [00:02<00:29, 330.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 10%|█         | 1070/10570 [00:03<00:31, 302.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 11%|█         | 1181/10570 [00:03<00:32, 286.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 11%|█         | 1179/10570 [00:03<00:31, 296.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 10%|▉         | 1048/10570 [00:03<00:30, 309.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  9%|▉         | 949/10570 [00:02<00:28, 333.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 11%|█         | 1166/10570 [00:03<00:31, 299.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 11%|█         | 1123/10570 [00:03<00:32, 290.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 11%|█         | 1151/10570 [00:03<00:32, 292.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 11%|█         | 1171/10570 [00:03<00:31, 294.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 11%|█         | 1126/10570 [00:03<00:32, 288.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 11%|█         | 1174/10570 [00:03<00:31, 294.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|▉         | 1010/10570 [00:02<00:29, 324.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  9%|▉         | 965/10570 [00:02<00:29, 325.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 12%|█▏        | 1245/10570 [00:03<00:31, 296.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 11%|█         | 1113/10570 [00:03<00:32, 288.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  9%|▉         | 936/10570 [00:02<00:29, 326.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 11%|█▏        | 1211/10570 [00:03<00:32, 288.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 11%|█▏        | 1209/10570 [00:03<00:31, 295.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 10%|█         | 1101/10570 [00:03<00:32, 289.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015  9%|▉         | 983/10570 [00:02<00:28, 331.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 10%|█         | 1080/10570 [00:03<00:31, 300.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 11%|█         | 1153/10570 [00:03<00:32, 292.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 11%|█▏        | 1197/10570 [00:03<00:31, 299.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 11%|█         | 1181/10570 [00:03<00:31, 294.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 11%|█▏        | 1201/10570 [00:03<00:31, 294.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 11%|█         | 1156/10570 [00:03<00:32, 289.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 11%|█▏        | 1204/10570 [00:03<00:31, 294.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  9%|▉         | 998/10570 [00:03<00:29, 324.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|▉         | 1043/10570 [00:03<00:30, 308.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 12%|█▏        | 1275/10570 [00:03<00:31, 292.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 11%|█         | 1143/10570 [00:03<00:32, 289.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  9%|▉         | 969/10570 [00:02<00:29, 326.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 12%|█▏        | 1241/10570 [00:03<00:32, 291.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 12%|█▏        | 1239/10570 [00:03<00:31, 296.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 11%|█         | 1131/10570 [00:03<00:32, 288.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 10%|▉         | 1017/10570 [00:03<00:29, 320.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 11%|█         | 1183/10570 [00:03<00:31, 294.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 11%|█▏        | 1211/10570 [00:03<00:31, 294.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 12%|█▏        | 1228/10570 [00:03<00:31, 298.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 11%|█         | 1111/10570 [00:03<00:33, 286.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 12%|█▏        | 1231/10570 [00:03<00:31, 294.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 11%|█         | 1186/10570 [00:03<00:32, 289.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 12%|█▏        | 1234/10570 [00:03<00:31, 295.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 12%|█▏        | 1305/10570 [00:03<00:31, 294.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 10%|▉         | 1031/10570 [00:03<00:30, 308.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 11%|█         | 1173/10570 [00:03<00:32, 290.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 1074/10570 [00:03<00:31, 299.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015  9%|▉         | 1002/10570 [00:03<00:29, 324.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 12%|█▏        | 1271/10570 [00:03<00:32, 289.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 12%|█▏        | 1269/10570 [00:03<00:31, 292.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 11%|█         | 1161/10570 [00:03<00:32, 289.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 11%|█▏        | 1213/10570 [00:03<00:31, 293.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 12%|█▏        | 1241/10570 [00:03<00:31, 295.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 12%|█▏        | 1258/10570 [00:03<00:31, 296.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 11%|█         | 1140/10570 [00:03<00:32, 286.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 10%|▉         | 1050/10570 [00:03<00:31, 305.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 12%|█▏        | 1261/10570 [00:03<00:32, 290.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 12%|█▏        | 1216/10570 [00:03<00:32, 288.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 12%|█▏        | 1264/10570 [00:03<00:31, 292.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 13%|█▎        | 1335/10570 [00:04<00:31, 294.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 11%|█▏        | 1203/10570 [00:03<00:32, 291.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 10%|█         | 1063/10570 [00:03<00:31, 298.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 12%|█▏        | 1301/10570 [00:03<00:31, 291.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|█         | 1105/10570 [00:03<00:33, 284.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 12%|█▏        | 1299/10570 [00:03<00:31, 293.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 11%|█▏        | 1191/10570 [00:03<00:32, 291.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 10%|▉         | 1035/10570 [00:03<00:31, 306.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 12%|█▏        | 1243/10570 [00:03<00:31, 295.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 12%|█▏        | 1271/10570 [00:03<00:31, 292.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 12%|█▏        | 1289/10570 [00:03<00:31, 296.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 11%|█         | 1170/10570 [00:03<00:32, 288.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 12%|█▏        | 1291/10570 [00:03<00:31, 291.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 10%|█         | 1081/10570 [00:03<00:31, 297.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 12%|█▏        | 1246/10570 [00:03<00:32, 290.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 12%|█▏        | 1294/10570 [00:03<00:31, 293.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 13%|█▎        | 1365/10570 [00:04<00:31, 294.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 12%|█▏        | 1233/10570 [00:03<00:31, 292.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 13%|█▎        | 1331/10570 [00:04<00:31, 292.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 10%|█         | 1093/10570 [00:03<00:32, 291.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 1134/10570 [00:03<00:33, 284.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 13%|█▎        | 1329/10570 [00:04<00:31, 293.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 12%|█▏        | 1221/10570 [00:03<00:32, 290.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 10%|█         | 1066/10570 [00:03<00:32, 295.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 12%|█▏        | 1273/10570 [00:03<00:31, 292.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 12%|█▏        | 1301/10570 [00:03<00:31, 294.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 12%|█▏        | 1319/10570 [00:03<00:31, 297.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 11%|█▏        | 1200/10570 [00:03<00:32, 289.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 12%|█▏        | 1321/10570 [00:04<00:31, 292.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 13%|█▎        | 1324/10570 [00:04<00:31, 294.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 12%|█▏        | 1276/10570 [00:03<00:32, 287.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 11%|█         | 1111/10570 [00:03<00:33, 283.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 13%|█▎        | 1395/10570 [00:04<00:31, 291.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 12%|█▏        | 1263/10570 [00:03<00:32, 289.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 13%|█▎        | 1361/10570 [00:04<00:31, 292.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█         | 1163/10570 [00:03<00:32, 285.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 13%|█▎        | 1359/10570 [00:04<00:31, 293.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 12%|█▏        | 1251/10570 [00:03<00:32, 290.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 11%|█         | 1123/10570 [00:03<00:33, 279.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 12%|█▏        | 1303/10570 [00:03<00:31, 294.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 13%|█▎        | 1331/10570 [00:04<00:31, 294.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 13%|█▎        | 1349/10570 [00:04<00:30, 297.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 12%|█▏        | 1229/10570 [00:03<00:32, 288.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 10%|█         | 1096/10570 [00:03<00:33, 280.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 13%|█▎        | 1351/10570 [00:04<00:31, 292.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 13%|█▎        | 1354/10570 [00:04<00:31, 293.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 12%|█▏        | 1305/10570 [00:04<00:32, 287.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 11%|█         | 1140/10570 [00:03<00:33, 283.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 13%|█▎        | 1425/10570 [00:04<00:31, 291.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 12%|█▏        | 1292/10570 [00:03<00:32, 288.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 13%|█▎        | 1391/10570 [00:04<00:31, 290.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|█▏        | 1193/10570 [00:03<00:32, 287.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 13%|█▎        | 1389/10570 [00:04<00:31, 291.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 11%|█         | 1152/10570 [00:03<00:33, 281.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 12%|█▏        | 1281/10570 [00:03<00:32, 288.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 13%|█▎        | 1333/10570 [00:04<00:31, 294.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 13%|█▎        | 1361/10570 [00:04<00:31, 293.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 13%|█▎        | 1379/10570 [00:04<00:31, 296.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 12%|█▏        | 1258/10570 [00:03<00:32, 286.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 11%|█         | 1125/10570 [00:03<00:33, 279.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 13%|█▎        | 1381/10570 [00:04<00:31, 291.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 13%|█▎        | 1334/10570 [00:04<00:32, 287.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 13%|█▎        | 1384/10570 [00:04<00:31, 292.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 11%|█         | 1169/10570 [00:03<00:33, 284.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 14%|█▍        | 1455/10570 [00:04<00:31, 292.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 13%|█▎        | 1322/10570 [00:04<00:31, 290.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 1222/10570 [00:03<00:32, 286.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 13%|█▎        | 1421/10570 [00:04<00:31, 289.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 13%|█▎        | 1419/10570 [00:04<00:31, 291.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 11%|█         | 1181/10570 [00:03<00:33, 283.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 12%|█▏        | 1310/10570 [00:04<00:32, 285.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 13%|█▎        | 1363/10570 [00:04<00:31, 294.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 13%|█▎        | 1391/10570 [00:04<00:31, 291.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 13%|█▎        | 1409/10570 [00:04<00:31, 294.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 12%|█▏        | 1288/10570 [00:03<00:32, 288.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 11%|█         | 1154/10570 [00:03<00:33, 280.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 13%|█▎        | 1363/10570 [00:04<00:31, 288.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 13%|█▎        | 1411/10570 [00:04<00:31, 289.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 13%|█▎        | 1414/10570 [00:04<00:31, 291.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 11%|█▏        | 1198/10570 [00:03<00:32, 284.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 14%|█▍        | 1485/10570 [00:04<00:31, 292.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 13%|█▎        | 1352/10570 [00:04<00:31, 290.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 1251/10570 [00:03<00:32, 286.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 14%|█▎        | 1451/10570 [00:04<00:31, 290.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 14%|█▎        | 1449/10570 [00:04<00:31, 291.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 11%|█▏        | 1210/10570 [00:03<00:33, 283.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 13%|█▎        | 1340/10570 [00:04<00:32, 287.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 13%|█▎        | 1393/10570 [00:04<00:31, 291.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 14%|█▎        | 1439/10570 [00:04<00:30, 295.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 13%|█▎        | 1421/10570 [00:04<00:31, 290.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 12%|█▏        | 1318/10570 [00:04<00:32, 289.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 11%|█         | 1183/10570 [00:03<00:33, 281.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 14%|█▎        | 1440/10570 [00:04<00:31, 288.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 13%|█▎        | 1392/10570 [00:04<00:32, 285.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 14%|█▎        | 1444/10570 [00:04<00:31, 290.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 12%|█▏        | 1227/10570 [00:03<00:32, 284.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 14%|█▍        | 1515/10570 [00:04<00:30, 294.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 13%|█▎        | 1382/10570 [00:04<00:31, 288.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 1280/10570 [00:03<00:32, 284.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 14%|█▍        | 1481/10570 [00:04<00:31, 291.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 14%|█▍        | 1479/10570 [00:04<00:31, 291.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 12%|█▏        | 1239/10570 [00:03<00:32, 283.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 13%|█▎        | 1369/10570 [00:04<00:31, 287.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 13%|█▎        | 1423/10570 [00:04<00:31, 291.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 14%|█▍        | 1469/10570 [00:04<00:30, 295.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 14%|█▎        | 1451/10570 [00:04<00:31, 291.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 13%|█▎        | 1347/10570 [00:04<00:31, 289.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 11%|█▏        | 1212/10570 [00:03<00:33, 281.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 14%|█▍        | 1470/10570 [00:04<00:31, 289.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 13%|█▎        | 1421/10570 [00:04<00:32, 285.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 14%|█▍        | 1474/10570 [00:04<00:31, 291.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 12%|█▏        | 1256/10570 [00:03<00:32, 283.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 15%|█▍        | 1545/10570 [00:04<00:30, 295.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 13%|█▎        | 1411/10570 [00:04<00:31, 287.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|█▏        | 1309/10570 [00:04<00:32, 285.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 14%|█▍        | 1511/10570 [00:04<00:30, 292.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 14%|█▍        | 1509/10570 [00:04<00:30, 293.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 12%|█▏        | 1268/10570 [00:03<00:33, 281.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 13%|█▎        | 1398/10570 [00:04<00:32, 285.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 14%|█▎        | 1453/10570 [00:04<00:31, 292.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 14%|█▍        | 1500/10570 [00:04<00:30, 298.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 14%|█▍        | 1481/10570 [00:04<00:31, 291.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 13%|█▎        | 1376/10570 [00:04<00:31, 288.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 12%|█▏        | 1241/10570 [00:03<00:33, 282.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 14%|█▍        | 1500/10570 [00:04<00:31, 292.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 14%|█▎        | 1450/10570 [00:04<00:31, 285.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 14%|█▍        | 1504/10570 [00:04<00:30, 292.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 12%|█▏        | 1285/10570 [00:03<00:32, 284.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 15%|█▍        | 1575/10570 [00:04<00:30, 296.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 14%|█▎        | 1441/10570 [00:04<00:31, 288.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 1338/10570 [00:04<00:32, 285.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 15%|█▍        | 1541/10570 [00:04<00:30, 292.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 15%|█▍        | 1539/10570 [00:04<00:30, 291.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 12%|█▏        | 1297/10570 [00:04<00:32, 282.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 14%|█▎        | 1427/10570 [00:04<00:32, 285.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 14%|█▍        | 1483/10570 [00:04<00:31, 292.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 14%|█▍        | 1530/10570 [00:04<00:30, 297.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 14%|█▍        | 1511/10570 [00:04<00:30, 292.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 13%|█▎        | 1405/10570 [00:04<00:32, 285.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 12%|█▏        | 1270/10570 [00:03<00:33, 279.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 14%|█▍        | 1479/10570 [00:04<00:31, 286.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 14%|█▍        | 1530/10570 [00:04<00:30, 292.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 15%|█▍        | 1534/10570 [00:04<00:30, 293.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 12%|█▏        | 1314/10570 [00:04<00:32, 284.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 15%|█▌        | 1605/10570 [00:04<00:30, 296.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 14%|█▍        | 1471/10570 [00:04<00:31, 290.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 1367/10570 [00:04<00:32, 284.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 15%|█▍        | 1571/10570 [00:04<00:30, 294.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 15%|█▍        | 1569/10570 [00:04<00:30, 293.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 14%|█▍        | 1457/10570 [00:04<00:31, 287.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 13%|█▎        | 1326/10570 [00:04<00:33, 273.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 14%|█▍        | 1513/10570 [00:04<00:30, 293.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 15%|█▍        | 1561/10570 [00:04<00:30, 299.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 15%|█▍        | 1541/10570 [00:04<00:30, 293.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 14%|█▎        | 1434/10570 [00:04<00:31, 285.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 15%|█▍        | 1560/10570 [00:04<00:30, 294.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 12%|█▏        | 1299/10570 [00:04<00:33, 280.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 14%|█▍        | 1509/10570 [00:04<00:31, 288.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 15%|█▍        | 1564/10570 [00:04<00:30, 293.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 13%|█▎        | 1343/10570 [00:04<00:32, 284.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 15%|█▌        | 1635/10570 [00:05<00:30, 296.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 14%|█▍        | 1501/10570 [00:04<00:31, 291.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 1396/10570 [00:04<00:32, 282.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 15%|█▌        | 1599/10570 [00:04<00:30, 294.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 15%|█▌        | 1601/10570 [00:05<00:31, 287.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 14%|█▍        | 1486/10570 [00:04<00:31, 287.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 15%|█▍        | 1543/10570 [00:04<00:30, 294.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 13%|█▎        | 1355/10570 [00:04<00:33, 275.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 15%|█▌        | 1592/10570 [00:04<00:29, 299.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 15%|█▍        | 1571/10570 [00:04<00:30, 294.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 14%|█▍        | 1463/10570 [00:04<00:31, 286.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 15%|█▌        | 1590/10570 [00:04<00:30, 294.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 15%|█▍        | 1538/10570 [00:04<00:31, 288.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 13%|█▎        | 1328/10570 [00:04<00:32, 280.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 15%|█▌        | 1594/10570 [00:04<00:30, 294.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 13%|█▎        | 1372/10570 [00:04<00:32, 283.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 16%|█▌        | 1666/10570 [00:05<00:29, 298.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 14%|█▍        | 1531/10570 [00:04<00:31, 290.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 15%|█▌        | 1629/10570 [00:05<00:30, 294.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|█▎        | 1425/10570 [00:04<00:33, 276.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 15%|█▌        | 1630/10570 [00:05<00:31, 282.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 14%|█▍        | 1516/10570 [00:04<00:31, 289.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 15%|█▍        | 1573/10570 [00:04<00:30, 295.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 13%|█▎        | 1383/10570 [00:04<00:33, 276.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 15%|█▌        | 1622/10570 [00:04<00:30, 296.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 14%|█▍        | 1492/10570 [00:04<00:31, 287.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 15%|█▌        | 1601/10570 [00:04<00:30, 290.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 15%|█▌        | 1620/10570 [00:05<00:30, 295.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 15%|█▍        | 1567/10570 [00:04<00:31, 287.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 13%|█▎        | 1357/10570 [00:04<00:32, 279.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 15%|█▌        | 1624/10570 [00:05<00:30, 294.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 13%|█▎        | 1401/10570 [00:04<00:32, 280.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 16%|█▌        | 1696/10570 [00:05<00:29, 298.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 15%|█▍        | 1561/10570 [00:04<00:30, 290.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 16%|█▌        | 1659/10570 [00:05<00:30, 296.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 1454/10570 [00:04<00:32, 279.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 16%|█▌        | 1660/10570 [00:05<00:30, 287.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 15%|█▍        | 1545/10570 [00:04<00:31, 289.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 13%|█▎        | 1411/10570 [00:04<00:33, 276.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 15%|█▌        | 1603/10570 [00:04<00:30, 293.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 16%|█▌        | 1653/10570 [00:05<00:29, 297.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 14%|█▍        | 1522/10570 [00:04<00:31, 288.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 15%|█▌        | 1631/10570 [00:05<00:31, 286.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 16%|█▌        | 1650/10570 [00:05<00:30, 295.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 15%|█▌        | 1596/10570 [00:05<00:31, 284.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 13%|█▎        | 1385/10570 [00:04<00:33, 278.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 16%|█▌        | 1654/10570 [00:05<00:30, 294.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 16%|█▋        | 1726/10570 [00:05<00:29, 297.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 14%|█▎        | 1430/10570 [00:04<00:32, 281.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 15%|█▌        | 1591/10570 [00:04<00:30, 291.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 16%|█▌        | 1690/10570 [00:05<00:29, 297.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 1483/10570 [00:04<00:32, 280.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 16%|█▌        | 1691/10570 [00:05<00:30, 291.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 15%|█▍        | 1575/10570 [00:04<00:30, 290.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 14%|█▎        | 1439/10570 [00:04<00:32, 277.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 15%|█▌        | 1633/10570 [00:05<00:30, 293.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 16%|█▌        | 1684/10570 [00:05<00:29, 301.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 15%|█▍        | 1551/10570 [00:04<00:31, 288.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 16%|█▌        | 1662/10570 [00:05<00:30, 290.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 16%|█▌        | 1680/10570 [00:05<00:30, 295.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 15%|█▌        | 1626/10570 [00:05<00:31, 286.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 13%|█▎        | 1413/10570 [00:04<00:33, 276.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 16%|█▌        | 1685/10570 [00:05<00:29, 297.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 17%|█▋        | 1756/10570 [00:05<00:29, 296.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 14%|█▍        | 1459/10570 [00:04<00:32, 281.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 15%|█▌        | 1621/10570 [00:05<00:30, 290.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 16%|█▋        | 1720/10570 [00:05<00:29, 297.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|█▍        | 1512/10570 [00:04<00:32, 282.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 16%|█▋        | 1721/10570 [00:05<00:30, 292.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 15%|█▌        | 1605/10570 [00:05<00:30, 291.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 14%|█▍        | 1468/10570 [00:04<00:32, 278.90it/s][1,mpirank:13,algo-2]<stderr>:#015 16%|█▌        | 1663/10570 [00:05<00:30, 295.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 16%|█▌        | 1715/10570 [00:05<00:29, 299.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 15%|█▍        | 1580/10570 [00:04<00:31, 284.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 16%|█▌        | 1693/10570 [00:05<00:30, 293.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 16%|█▌        | 1710/10570 [00:05<00:30, 294.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 16%|█▌        | 1656/10570 [00:05<00:30, 288.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 14%|█▎        | 1441/10570 [00:04<00:33, 276.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 16%|█▌        | 1715/10570 [00:05<00:29, 296.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 17%|█▋        | 1786/10570 [00:05<00:29, 296.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 14%|█▍        | 1488/10570 [00:04<00:32, 281.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 16%|█▌        | 1651/10570 [00:05<00:30, 292.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 17%|█▋        | 1750/10570 [00:05<00:29, 295.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▍        | 1541/10570 [00:04<00:31, 283.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 15%|█▌        | 1635/10570 [00:05<00:30, 290.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 17%|█▋        | 1751/10570 [00:05<00:30, 289.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 14%|█▍        | 1497/10570 [00:04<00:32, 281.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 16%|█▌        | 1694/10570 [00:05<00:29, 296.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 17%|█▋        | 1745/10570 [00:05<00:29, 298.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 15%|█▌        | 1610/10570 [00:05<00:31, 288.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 16%|█▋        | 1723/10570 [00:05<00:30, 293.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 16%|█▋        | 1740/10570 [00:05<00:30, 293.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 14%|█▍        | 1469/10570 [00:04<00:32, 277.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 16%|█▌        | 1686/10570 [00:05<00:30, 291.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 17%|█▋        | 1745/10570 [00:05<00:29, 294.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 17%|█▋        | 1816/10570 [00:05<00:29, 296.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 14%|█▍        | 1517/10570 [00:04<00:31, 283.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 16%|█▌        | 1682/10570 [00:05<00:30, 295.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 17%|█▋        | 1780/10570 [00:05<00:29, 294.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▍        | 1570/10570 [00:04<00:31, 285.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 16%|█▌        | 1665/10570 [00:05<00:30, 293.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 17%|█▋        | 1781/10570 [00:05<00:30, 290.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 14%|█▍        | 1526/10570 [00:04<00:32, 281.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 16%|█▋        | 1724/10570 [00:05<00:29, 296.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 17%|█▋        | 1775/10570 [00:05<00:29, 297.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 16%|█▌        | 1639/10570 [00:05<00:31, 287.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 17%|█▋        | 1753/10570 [00:05<00:30, 292.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 17%|█▋        | 1770/10570 [00:05<00:29, 293.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 14%|█▍        | 1498/10570 [00:04<00:32, 278.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 16%|█▌        | 1716/10570 [00:05<00:30, 290.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 17%|█▋        | 1775/10570 [00:05<00:29, 293.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 17%|█▋        | 1846/10570 [00:05<00:29, 294.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 15%|█▍        | 1546/10570 [00:04<00:31, 284.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 16%|█▌        | 1712/10570 [00:05<00:30, 293.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 17%|█▋        | 1810/10570 [00:05<00:29, 295.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 1599/10570 [00:05<00:31, 285.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 16%|█▌        | 1695/10570 [00:05<00:30, 293.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 17%|█▋        | 1811/10570 [00:05<00:29, 292.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 15%|█▍        | 1555/10570 [00:05<00:31, 282.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 17%|█▋        | 1754/10570 [00:05<00:30, 292.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 17%|█▋        | 1806/10570 [00:05<00:29, 298.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 16%|█▌        | 1669/10570 [00:05<00:30, 290.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 17%|█▋        | 1783/10570 [00:05<00:29, 293.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 17%|█▋        | 1800/10570 [00:05<00:29, 294.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 14%|█▍        | 1526/10570 [00:04<00:32, 278.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 17%|█▋        | 1805/10570 [00:05<00:29, 294.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 17%|█▋        | 1746/10570 [00:05<00:30, 288.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 18%|█▊        | 1876/10570 [00:05<00:29, 295.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 15%|█▍        | 1575/10570 [00:04<00:31, 284.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 16%|█▋        | 1742/10570 [00:05<00:30, 292.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|█▌        | 1628/10570 [00:05<00:31, 285.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 17%|█▋        | 1840/10570 [00:05<00:29, 293.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 16%|█▋        | 1725/10570 [00:05<00:30, 292.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 17%|█▋        | 1841/10570 [00:05<00:29, 291.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 15%|█▍        | 1584/10570 [00:05<00:31, 282.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 17%|█▋        | 1784/10570 [00:05<00:30, 292.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 17%|█▋        | 1836/10570 [00:05<00:29, 296.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 16%|█▌        | 1699/10570 [00:05<00:30, 291.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 17%|█▋        | 1813/10570 [00:05<00:29, 294.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 17%|█▋        | 1830/10570 [00:05<00:29, 292.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 15%|█▍        | 1555/10570 [00:05<00:32, 279.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 17%|█▋        | 1775/10570 [00:05<00:30, 288.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 17%|█▋        | 1835/10570 [00:05<00:29, 293.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 18%|█▊        | 1907/10570 [00:05<00:29, 298.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 15%|█▌        | 1604/10570 [00:05<00:32, 279.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 17%|█▋        | 1772/10570 [00:05<00:30, 290.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 18%|█▊        | 1870/10570 [00:05<00:29, 294.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 1657/10570 [00:05<00:31, 279.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 18%|█▊        | 1871/10570 [00:05<00:29, 293.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 17%|█▋        | 1755/10570 [00:05<00:30, 290.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 17%|█▋        | 1814/10570 [00:05<00:29, 293.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 18%|█▊        | 1867/10570 [00:05<00:29, 298.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 15%|█▌        | 1613/10570 [00:05<00:32, 273.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 16%|█▋        | 1729/10570 [00:05<00:30, 291.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 17%|█▋        | 1843/10570 [00:05<00:29, 292.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 18%|█▊        | 1860/10570 [00:05<00:29, 291.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 15%|█▍        | 1583/10570 [00:05<00:32, 278.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 17%|█▋        | 1805/10570 [00:05<00:30, 289.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 18%|█▊        | 1865/10570 [00:05<00:29, 293.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 18%|█▊        | 1937/10570 [00:06<00:29, 292.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 15%|█▌        | 1633/10570 [00:05<00:31, 280.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 17%|█▋        | 1802/10570 [00:05<00:30, 290.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 18%|█▊        | 1901/10570 [00:05<00:29, 296.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 1687/10570 [00:05<00:31, 284.17it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 18%|█▊        | 1902/10570 [00:06<00:29, 295.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 17%|█▋        | 1785/10570 [00:05<00:30, 288.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 17%|█▋        | 1844/10570 [00:05<00:29, 292.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 18%|█▊        | 1898/10570 [00:05<00:28, 300.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 16%|█▌        | 1641/10570 [00:05<00:32, 274.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 17%|█▋        | 1759/10570 [00:05<00:30, 287.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 18%|█▊        | 1873/10570 [00:05<00:29, 293.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 15%|█▌        | 1612/10570 [00:05<00:31, 280.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 18%|█▊        | 1891/10570 [00:06<00:29, 294.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 17%|█▋        | 1834/10570 [00:05<00:30, 289.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 18%|█▊        | 1896/10570 [00:05<00:29, 296.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 16%|█▌        | 1663/10570 [00:05<00:31, 284.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 19%|█▊        | 1967/10570 [00:06<00:29, 289.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 17%|█▋        | 1832/10570 [00:05<00:30, 289.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 18%|█▊        | 1931/10570 [00:06<00:29, 297.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|█▌        | 1716/10570 [00:05<00:31, 283.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 18%|█▊        | 1932/10570 [00:06<00:29, 295.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 17%|█▋        | 1815/10570 [00:05<00:30, 288.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 18%|█▊        | 1874/10570 [00:05<00:29, 293.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 18%|█▊        | 1929/10570 [00:06<00:28, 301.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 16%|█▌        | 1671/10570 [00:05<00:31, 279.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 17%|█▋        | 1789/10570 [00:05<00:30, 288.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 18%|█▊        | 1904/10570 [00:06<00:29, 296.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 18%|█▊        | 1922/10570 [00:06<00:29, 296.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 16%|█▌        | 1641/10570 [00:05<00:31, 279.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 18%|█▊        | 1864/10570 [00:06<00:29, 290.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 18%|█▊        | 1927/10570 [00:06<00:29, 297.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 19%|█▉        | 1997/10570 [00:06<00:29, 290.92it/s][1,mpirank:11,algo-2]<stderr>:#015 16%|█▌        | 1693/10570 [00:05<00:31, 285.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 18%|█▊        | 1861/10570 [00:05<00:30, 289.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 19%|█▊        | 1961/10570 [00:06<00:28, 297.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 1745/10570 [00:05<00:31, 283.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 19%|█▊        | 1962/10570 [00:06<00:29, 296.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 17%|█▋        | 1844/10570 [00:05<00:30, 288.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 18%|█▊        | 1905/10570 [00:05<00:29, 296.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 19%|█▊        | 1960/10570 [00:06<00:28, 301.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 16%|█▌        | 1700/10570 [00:05<00:31, 281.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 17%|█▋        | 1818/10570 [00:05<00:30, 287.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 18%|█▊        | 1934/10570 [00:06<00:29, 295.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 18%|█▊        | 1952/10570 [00:06<00:29, 296.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 16%|█▌        | 1670/10570 [00:05<00:31, 281.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 18%|█▊        | 1894/10570 [00:06<00:29, 292.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 19%|█▊        | 1957/10570 [00:06<00:29, 296.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 16%|█▋        | 1722/10570 [00:05<00:30, 286.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 19%|█▉        | 2027/10570 [00:06<00:29, 291.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 18%|█▊        | 1891/10570 [00:06<00:29, 291.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 19%|█▉        | 1991/10570 [00:06<00:28, 297.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 1774/10570 [00:05<00:30, 284.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 19%|█▉        | 1992/10570 [00:06<00:28, 296.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 18%|█▊        | 1874/10570 [00:06<00:30, 289.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 18%|█▊        | 1935/10570 [00:06<00:29, 296.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 19%|█▉        | 1991/10570 [00:06<00:28, 301.39it/s][1,mpirank:8,algo-2]<stderr>:#015 16%|█▋        | 1729/10570 [00:05<00:31, 282.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 17%|█▋        | 1847/10570 [00:05<00:30, 287.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 19%|█▊        | 1964/10570 [00:06<00:29, 293.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 19%|█▉        | 1982/10570 [00:06<00:28, 297.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 18%|█▊        | 1924/10570 [00:06<00:29, 294.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 16%|█▌        | 1699/10570 [00:05<00:31, 281.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 19%|█▉        | 1987/10570 [00:06<00:28, 296.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 19%|█▉        | 2057/10570 [00:06<00:28, 293.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 17%|█▋        | 1751/10570 [00:05<00:31, 282.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 18%|█▊        | 1921/10570 [00:06<00:29, 293.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 19%|█▉        | 2021/10570 [00:06<00:28, 294.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 1803/10570 [00:05<00:30, 283.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 19%|█▉        | 2022/10570 [00:06<00:29, 294.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 18%|█▊        | 1904/10570 [00:06<00:29, 292.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 19%|█▊        | 1965/10570 [00:06<00:29, 294.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 19%|█▉        | 2022/10570 [00:06<00:28, 299.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 18%|█▊        | 1876/10570 [00:06<00:30, 287.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 17%|█▋        | 1758/10570 [00:05<00:32, 273.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 19%|█▉        | 1994/10570 [00:06<00:29, 293.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 19%|█▉        | 2012/10570 [00:06<00:29, 294.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 18%|█▊        | 1954/10570 [00:06<00:29, 293.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 16%|█▋        | 1728/10570 [00:05<00:31, 281.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 19%|█▉        | 2017/10570 [00:06<00:29, 294.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 20%|█▉        | 2087/10570 [00:06<00:28, 295.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 17%|█▋        | 1780/10570 [00:05<00:31, 282.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 18%|█▊        | 1951/10570 [00:06<00:29, 290.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 19%|█▉        | 2051/10570 [00:06<00:28, 296.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|█▋        | 1832/10570 [00:05<00:30, 283.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 19%|█▉        | 2052/10570 [00:06<00:28, 295.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 18%|█▊        | 1934/10570 [00:06<00:29, 292.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 19%|█▉        | 1995/10570 [00:06<00:29, 294.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 19%|█▉        | 2053/10570 [00:06<00:28, 300.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 18%|█▊        | 1906/10570 [00:06<00:29, 290.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 17%|█▋        | 1787/10570 [00:05<00:31, 275.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 19%|█▉        | 2024/10570 [00:06<00:29, 292.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 19%|█▉        | 2042/10570 [00:06<00:28, 294.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 19%|█▉        | 1984/10570 [00:06<00:29, 294.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 17%|█▋        | 1757/10570 [00:05<00:31, 279.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 19%|█▉        | 2047/10570 [00:06<00:28, 294.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 20%|██        | 2117/10570 [00:06<00:28, 295.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 17%|█▋        | 1809/10570 [00:05<00:30, 283.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 19%|█▊        | 1981/10570 [00:06<00:29, 292.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 20%|█▉        | 2081/10570 [00:06<00:28, 296.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 1861/10570 [00:06<00:30, 284.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 20%|█▉        | 2082/10570 [00:06<00:28, 295.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 19%|█▊        | 1964/10570 [00:06<00:29, 293.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 19%|█▉        | 2025/10570 [00:06<00:29, 293.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 20%|█▉        | 2084/10570 [00:06<00:28, 300.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 18%|█▊        | 1936/10570 [00:06<00:29, 290.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 19%|█▉        | 2054/10570 [00:06<00:28, 294.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 17%|█▋        | 1816/10570 [00:05<00:31, 277.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 20%|█▉        | 2072/10570 [00:06<00:28, 295.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 19%|█▉        | 2014/10570 [00:06<00:29, 291.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 20%|█▉        | 2077/10570 [00:06<00:28, 295.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 17%|█▋        | 1786/10570 [00:05<00:31, 279.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 17%|█▋        | 1838/10570 [00:05<00:30, 282.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 20%|█▉        | 2111/10570 [00:06<00:28, 296.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 19%|█▉        | 2011/10570 [00:06<00:29, 289.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 20%|██        | 2147/10570 [00:06<00:31, 267.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 1891/10570 [00:06<00:30, 286.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 20%|█▉        | 2112/10570 [00:06<00:28, 295.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 19%|█▉        | 1994/10570 [00:06<00:29, 292.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 19%|█▉        | 2055/10570 [00:06<00:28, 294.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 20%|██        | 2115/10570 [00:06<00:28, 300.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 19%|█▊        | 1966/10570 [00:06<00:29, 291.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 17%|█▋        | 1844/10570 [00:06<00:31, 278.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 20%|█▉        | 2084/10570 [00:06<00:28, 295.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 20%|█▉        | 2102/10570 [00:06<00:28, 296.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 19%|█▉        | 2044/10570 [00:06<00:29, 291.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 20%|█▉        | 2107/10570 [00:06<00:28, 296.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 17%|█▋        | 1814/10570 [00:05<00:31, 279.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 18%|█▊        | 1867/10570 [00:06<00:30, 283.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 19%|█▉        | 2041/10570 [00:06<00:29, 289.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 21%|██        | 2176/10570 [00:06<00:30, 272.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 1921/10570 [00:06<00:30, 287.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 20%|█▉        | 2085/10570 [00:06<00:28, 294.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 20%|██        | 2141/10570 [00:06<00:31, 269.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 19%|█▉        | 2024/10570 [00:06<00:30, 284.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 19%|█▉        | 1996/10570 [00:06<00:29, 291.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 20%|██        | 2114/10570 [00:06<00:28, 295.92it/s][1,mpirank:8,algo-2]<stderr>:#015 18%|█▊        | 1873/10570 [00:06<00:30, 280.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 20%|██        | 2142/10570 [00:06<00:31, 267.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 20%|█▉        | 2074/10570 [00:06<00:29, 292.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 17%|█▋        | 1842/10570 [00:06<00:31, 278.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 20%|██        | 2146/10570 [00:06<00:30, 273.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 18%|█▊        | 1897/10570 [00:06<00:30, 286.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 20%|██        | 2132/10570 [00:06<00:31, 268.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 20%|██        | 2137/10570 [00:06<00:31, 268.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 20%|█▉        | 2071/10570 [00:06<00:29, 291.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 21%|██        | 2206/10570 [00:07<00:30, 278.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|█▊        | 1950/10570 [00:06<00:30, 280.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 20%|██        | 2115/10570 [00:06<00:28, 295.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 21%|██        | 2169/10570 [00:06<00:30, 271.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 19%|█▉        | 2054/10570 [00:06<00:29, 287.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 19%|█▉        | 2026/10570 [00:06<00:29, 290.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 21%|██        | 2171/10570 [00:06<00:30, 272.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 18%|█▊        | 1902/10570 [00:06<00:31, 277.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 20%|█▉        | 2104/10570 [00:06<00:28, 293.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 18%|█▊        | 1870/10570 [00:06<00:31, 278.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 21%|██        | 2175/10570 [00:06<00:30, 274.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 20%|██        | 2144/10570 [00:06<00:31, 267.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 18%|█▊        | 1927/10570 [00:06<00:30, 287.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 20%|██        | 2162/10570 [00:06<00:30, 274.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 20%|██        | 2166/10570 [00:06<00:30, 273.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 20%|█▉        | 2101/10570 [00:06<00:29, 291.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 21%|██        | 2236/10570 [00:07<00:29, 284.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▊        | 1980/10570 [00:06<00:30, 283.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 20%|█▉        | 2084/10570 [00:06<00:29, 289.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 21%|██        | 2198/10570 [00:07<00:30, 273.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 19%|█▉        | 2056/10570 [00:06<00:29, 290.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 21%|██        | 2201/10570 [00:07<00:30, 277.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 18%|█▊        | 1931/10570 [00:06<00:30, 280.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 18%|█▊        | 1899/10570 [00:06<00:30, 280.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 20%|██        | 2145/10570 [00:06<00:31, 268.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 21%|██        | 2205/10570 [00:06<00:29, 280.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 21%|██        | 2173/10570 [00:06<00:30, 272.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 19%|█▊        | 1956/10570 [00:06<00:30, 286.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 21%|██        | 2191/10570 [00:07<00:30, 276.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 20%|██        | 2134/10570 [00:06<00:31, 266.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 21%|██        | 2194/10570 [00:07<00:30, 273.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 21%|██▏       | 2265/10570 [00:07<00:29, 284.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 2009/10570 [00:06<00:30, 282.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 21%|██        | 2228/10570 [00:07<00:29, 281.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 20%|██        | 2114/10570 [00:06<00:29, 290.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 20%|██        | 2131/10570 [00:06<00:32, 262.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 20%|█▉        | 2086/10570 [00:06<00:29, 291.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 21%|██        | 2231/10570 [00:07<00:29, 283.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 19%|█▊        | 1960/10570 [00:06<00:30, 281.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 18%|█▊        | 1928/10570 [00:06<00:30, 282.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 21%|██        | 2174/10570 [00:06<00:30, 273.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 21%|██        | 2236/10570 [00:07<00:29, 287.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 21%|██        | 2203/10570 [00:07<00:30, 277.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 19%|█▉        | 1985/10570 [00:06<00:29, 286.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 21%|██        | 2221/10570 [00:07<00:29, 282.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 20%|██        | 2163/10570 [00:07<00:30, 271.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 21%|██        | 2224/10570 [00:07<00:29, 279.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 22%|██▏       | 2295/10570 [00:07<00:28, 288.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|█▉        | 2038/10570 [00:06<00:30, 283.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 21%|██▏       | 2257/10570 [00:07<00:29, 282.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 20%|██        | 2161/10570 [00:06<00:31, 270.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 20%|██        | 2116/10570 [00:06<00:29, 290.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 21%|██▏       | 2260/10570 [00:07<00:29, 284.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 19%|█▉        | 1989/10570 [00:06<00:30, 282.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 19%|█▊        | 1957/10570 [00:06<00:30, 281.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 21%|██        | 2204/10570 [00:07<00:30, 278.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 20%|██        | 2144/10570 [00:06<00:31, 263.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 21%|██▏       | 2265/10570 [00:07<00:28, 287.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 21%|██        | 2233/10570 [00:07<00:29, 283.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 19%|█▉        | 2014/10570 [00:06<00:30, 284.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 21%|██▏       | 2251/10570 [00:07<00:29, 284.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 21%|██        | 2191/10570 [00:07<00:30, 272.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 21%|██▏       | 2254/10570 [00:07<00:29, 282.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 22%|██▏       | 2325/10570 [00:07<00:28, 289.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|█▉        | 2067/10570 [00:06<00:29, 284.90it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 22%|██▏       | 2287/10570 [00:07<00:28, 286.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 21%|██        | 2190/10570 [00:07<00:30, 273.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 22%|██▏       | 2290/10570 [00:07<00:28, 287.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 19%|█▉        | 2018/10570 [00:06<00:30, 280.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 19%|█▉        | 1986/10570 [00:06<00:30, 281.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 21%|██        | 2234/10570 [00:07<00:29, 284.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 21%|██        | 2173/10570 [00:07<00:31, 268.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 22%|██▏       | 2296/10570 [00:07<00:28, 291.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 21%|██▏       | 2262/10570 [00:07<00:29, 283.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 20%|██        | 2146/10570 [00:06<00:32, 262.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 19%|█▉        | 2043/10570 [00:06<00:30, 282.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 22%|██▏       | 2280/10570 [00:07<00:29, 285.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 21%|██        | 2221/10570 [00:07<00:29, 278.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 22%|██▏       | 2284/10570 [00:07<00:29, 285.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 22%|██▏       | 2355/10570 [00:07<00:28, 290.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|█▉        | 2096/10570 [00:06<00:30, 278.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 22%|██▏       | 2317/10570 [00:07<00:28, 288.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 22%|██▏       | 2319/10570 [00:07<00:28, 288.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 21%|██        | 2220/10570 [00:07<00:29, 278.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 19%|█▉        | 2047/10570 [00:06<00:30, 281.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 21%|██▏       | 2263/10570 [00:07<00:29, 284.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 19%|█▉        | 2015/10570 [00:06<00:30, 279.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 21%|██        | 2202/10570 [00:07<00:30, 272.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 22%|██▏       | 2326/10570 [00:07<00:28, 293.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 22%|██▏       | 2292/10570 [00:07<00:28, 287.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 21%|██        | 2174/10570 [00:07<00:31, 267.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 20%|█▉        | 2072/10570 [00:06<00:29, 284.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 22%|██▏       | 2310/10570 [00:07<00:28, 287.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 21%|██▏       | 2250/10570 [00:07<00:29, 282.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 22%|██▏       | 2314/10570 [00:07<00:28, 286.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 23%|██▎       | 2385/10570 [00:07<00:28, 288.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 22%|██▏       | 2347/10570 [00:07<00:28, 289.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 22%|██▏       | 2349/10570 [00:07<00:28, 289.91it/s][1,mpirank:4,algo-1]<stderr>:#015 21%|██▏       | 2249/10570 [00:07<00:29, 280.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 20%|█▉        | 2076/10570 [00:06<00:29, 283.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 2124/10570 [00:06<00:32, 260.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 22%|██▏       | 2293/10570 [00:07<00:28, 288.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 19%|█▉        | 2043/10570 [00:06<00:30, 279.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 21%|██        | 2232/10570 [00:07<00:29, 279.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 22%|██▏       | 2356/10570 [00:07<00:27, 294.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 22%|██▏       | 2321/10570 [00:07<00:28, 288.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 21%|██        | 2203/10570 [00:07<00:30, 272.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 20%|█▉        | 2101/10570 [00:06<00:29, 285.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 22%|██▏       | 2340/10570 [00:07<00:28, 288.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 22%|██▏       | 2279/10570 [00:07<00:29, 282.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 22%|██▏       | 2344/10570 [00:07<00:28, 288.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 23%|██▎       | 2414/10570 [00:07<00:28, 289.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 22%|██▏       | 2377/10570 [00:07<00:28, 289.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 22%|██▏       | 2278/10570 [00:07<00:29, 282.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 23%|██▎       | 2379/10570 [00:07<00:28, 289.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 20%|█▉        | 2105/10570 [00:06<00:29, 284.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 22%|██▏       | 2323/10570 [00:07<00:28, 289.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|██        | 2151/10570 [00:07<00:32, 257.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 20%|█▉        | 2072/10570 [00:06<00:30, 280.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 21%|██▏       | 2261/10570 [00:07<00:29, 279.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 23%|██▎       | 2386/10570 [00:07<00:28, 292.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 22%|██▏       | 2351/10570 [00:07<00:28, 289.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 21%|██        | 2233/10570 [00:07<00:29, 279.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 22%|██▏       | 2370/10570 [00:07<00:28, 289.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 22%|██▏       | 2309/10570 [00:07<00:28, 285.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 22%|██▏       | 2374/10570 [00:07<00:28, 289.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 23%|██▎       | 2444/10570 [00:07<00:28, 289.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 20%|██        | 2130/10570 [00:06<00:32, 258.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 23%|██▎       | 2407/10570 [00:07<00:28, 287.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 22%|██▏       | 2308/10570 [00:07<00:29, 284.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 23%|██▎       | 2408/10570 [00:07<00:28, 287.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 22%|██▏       | 2353/10570 [00:07<00:28, 291.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 2179/10570 [00:07<00:31, 262.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 20%|█▉        | 2101/10570 [00:06<00:30, 281.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 22%|██▏       | 2291/10570 [00:07<00:29, 283.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 23%|██▎       | 2416/10570 [00:07<00:27, 292.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 23%|██▎       | 2381/10570 [00:07<00:28, 288.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 21%|██▏       | 2262/10570 [00:07<00:29, 279.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 20%|██        | 2134/10570 [00:07<00:32, 257.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 22%|██▏       | 2339/10570 [00:07<00:28, 286.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 23%|██▎       | 2400/10570 [00:07<00:28, 287.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 23%|██▎       | 2403/10570 [00:07<00:28, 287.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 23%|██▎       | 2473/10570 [00:07<00:28, 288.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 20%|██        | 2159/10570 [00:07<00:31, 265.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 23%|██▎       | 2436/10570 [00:07<00:28, 288.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 23%|██▎       | 2437/10570 [00:07<00:28, 288.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 22%|██▏       | 2337/10570 [00:07<00:29, 282.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 2208/10570 [00:07<00:31, 268.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 23%|██▎       | 2383/10570 [00:07<00:28, 289.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 22%|██▏       | 2320/10570 [00:07<00:29, 284.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 23%|██▎       | 2446/10570 [00:07<00:27, 291.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 23%|██▎       | 2410/10570 [00:07<00:28, 287.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 22%|██▏       | 2292/10570 [00:07<00:29, 283.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 20%|██        | 2162/10570 [00:07<00:31, 263.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 22%|██▏       | 2368/10570 [00:07<00:28, 287.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 23%|██▎       | 2429/10570 [00:07<00:28, 286.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 23%|██▎       | 2432/10570 [00:07<00:28, 286.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 20%|██        | 2130/10570 [00:07<00:33, 254.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 21%|██        | 2187/10570 [00:07<00:31, 268.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 24%|██▎       | 2502/10570 [00:08<00:29, 275.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 23%|██▎       | 2465/10570 [00:07<00:28, 287.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 23%|██▎       | 2466/10570 [00:08<00:28, 286.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 22%|██▏       | 2366/10570 [00:07<00:28, 283.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██        | 2237/10570 [00:07<00:30, 274.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 23%|██▎       | 2412/10570 [00:07<00:28, 289.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 22%|██▏       | 2350/10570 [00:07<00:28, 286.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 23%|██▎       | 2440/10570 [00:07<00:28, 288.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 23%|██▎       | 2476/10570 [00:07<00:27, 290.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 22%|██▏       | 2321/10570 [00:07<00:29, 284.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 21%|██        | 2190/10570 [00:07<00:31, 266.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 23%|██▎       | 2397/10570 [00:07<00:28, 285.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 23%|██▎       | 2458/10570 [00:07<00:28, 286.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 23%|██▎       | 2461/10570 [00:07<00:28, 286.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 20%|██        | 2158/10570 [00:07<00:32, 260.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 21%|██        | 2216/10570 [00:07<00:30, 273.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 24%|██▍       | 2531/10570 [00:08<00:28, 277.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 24%|██▎       | 2495/10570 [00:08<00:28, 285.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 24%|██▎       | 2495/10570 [00:08<00:28, 284.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 23%|██▎       | 2395/10570 [00:07<00:29, 281.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|██▏       | 2265/10570 [00:07<00:30, 275.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 23%|██▎       | 2442/10570 [00:07<00:28, 289.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 23%|██▎       | 2379/10570 [00:07<00:28, 285.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 23%|██▎       | 2469/10570 [00:07<00:28, 287.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 22%|██▏       | 2350/10570 [00:07<00:28, 285.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 21%|██        | 2219/10570 [00:07<00:30, 271.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 23%|██▎       | 2426/10570 [00:07<00:28, 285.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 24%|██▎       | 2487/10570 [00:08<00:28, 286.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 24%|██▎       | 2490/10570 [00:08<00:28, 287.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 21%|██        | 2186/10570 [00:07<00:31, 263.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 24%|██▎       | 2506/10570 [00:08<00:29, 276.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 21%|██        | 2245/10570 [00:07<00:30, 275.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 24%|██▍       | 2559/10570 [00:08<00:28, 277.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 23%|██▎       | 2424/10570 [00:07<00:28, 282.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 24%|██▍       | 2524/10570 [00:08<00:29, 273.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 2294/10570 [00:07<00:29, 279.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 23%|██▎       | 2471/10570 [00:07<00:28, 288.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 24%|██▍       | 2524/10570 [00:08<00:29, 272.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 23%|██▎       | 2408/10570 [00:07<00:28, 283.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 21%|██▏       | 2247/10570 [00:07<00:30, 273.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 23%|██▎       | 2379/10570 [00:07<00:28, 283.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 23%|██▎       | 2455/10570 [00:08<00:28, 283.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 24%|██▎       | 2498/10570 [00:08<00:29, 276.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 24%|██▍       | 2535/10570 [00:08<00:28, 279.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 21%|██        | 2215/10570 [00:07<00:31, 269.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 24%|██▍       | 2516/10570 [00:08<00:29, 271.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 24%|██▍       | 2519/10570 [00:08<00:29, 271.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 22%|██▏       | 2274/10570 [00:07<00:29, 277.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 24%|██▍       | 2588/10570 [00:08<00:28, 279.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 23%|██▎       | 2453/10570 [00:08<00:28, 282.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 24%|██▍       | 2552/10570 [00:08<00:29, 274.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 2323/10570 [00:07<00:29, 280.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 24%|██▍       | 2552/10570 [00:08<00:29, 274.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 23%|██▎       | 2437/10570 [00:08<00:28, 284.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 24%|██▎       | 2500/10570 [00:08<00:29, 274.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 22%|██▏       | 2275/10570 [00:07<00:30, 275.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 23%|██▎       | 2408/10570 [00:07<00:28, 282.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 24%|██▍       | 2526/10570 [00:08<00:29, 275.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 24%|██▎       | 2485/10570 [00:08<00:28, 285.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 21%|██        | 2243/10570 [00:07<00:30, 270.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 24%|██▍       | 2564/10570 [00:08<00:28, 278.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 24%|██▍       | 2544/10570 [00:08<00:29, 272.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 24%|██▍       | 2547/10570 [00:08<00:29, 273.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 22%|██▏       | 2303/10570 [00:07<00:29, 279.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 25%|██▍       | 2618/10570 [00:08<00:28, 283.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 23%|██▎       | 2482/10570 [00:08<00:28, 282.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 24%|██▍       | 2581/10570 [00:08<00:28, 276.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|██▏       | 2352/10570 [00:07<00:29, 281.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 24%|██▍       | 2580/10570 [00:08<00:28, 275.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 24%|██▍       | 2529/10570 [00:08<00:28, 277.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 23%|██▎       | 2466/10570 [00:08<00:28, 282.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 22%|██▏       | 2304/10570 [00:07<00:29, 277.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 23%|██▎       | 2437/10570 [00:07<00:29, 276.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 24%|██▍       | 2554/10570 [00:08<00:29, 275.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 21%|██▏       | 2271/10570 [00:07<00:30, 272.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 25%|██▍       | 2594/10570 [00:08<00:28, 282.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 24%|██▍       | 2572/10570 [00:08<00:29, 272.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 24%|██▍       | 2575/10570 [00:08<00:29, 275.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 24%|██▍       | 2514/10570 [00:08<00:29, 270.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 22%|██▏       | 2332/10570 [00:07<00:29, 280.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 25%|██▌       | 2648/10570 [00:08<00:27, 288.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 2381/10570 [00:07<00:29, 280.12it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 25%|██▍       | 2609/10570 [00:08<00:29, 271.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 25%|██▍       | 2610/10570 [00:08<00:28, 280.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 24%|██▍       | 2511/10570 [00:08<00:30, 268.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 24%|██▍       | 2557/10570 [00:08<00:29, 275.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 24%|██▎       | 2495/10570 [00:08<00:28, 280.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 22%|██▏       | 2332/10570 [00:07<00:30, 267.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 24%|██▍       | 2583/10570 [00:08<00:28, 277.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 22%|██▏       | 2300/10570 [00:07<00:30, 274.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 25%|██▍       | 2600/10570 [00:08<00:29, 273.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 25%|██▍       | 2603/10570 [00:08<00:28, 275.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 25%|██▍       | 2623/10570 [00:08<00:28, 274.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 24%|██▍       | 2542/10570 [00:08<00:29, 271.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 23%|██▎       | 2465/10570 [00:08<00:31, 257.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 22%|██▏       | 2361/10570 [00:07<00:29, 282.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 25%|██▌       | 2678/10570 [00:08<00:27, 290.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 25%|██▍       | 2639/10570 [00:08<00:28, 277.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 2410/10570 [00:08<00:29, 279.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 24%|██▍       | 2539/10570 [00:08<00:29, 271.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 25%|██▍       | 2639/10570 [00:08<00:29, 272.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 24%|██▍       | 2586/10570 [00:08<00:28, 276.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 22%|██▏       | 2361/10570 [00:07<00:30, 272.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 25%|██▍       | 2613/10570 [00:08<00:28, 282.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 24%|██▍       | 2524/10570 [00:08<00:29, 268.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 22%|██▏       | 2328/10570 [00:07<00:29, 276.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 25%|██▍       | 2628/10570 [00:08<00:29, 273.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 25%|██▍       | 2633/10570 [00:08<00:28, 280.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 25%|██▌       | 2654/10570 [00:08<00:28, 282.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 24%|██▍       | 2570/10570 [00:08<00:29, 271.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 24%|██▎       | 2494/10570 [00:08<00:30, 264.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 26%|██▌       | 2708/10570 [00:08<00:27, 290.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 23%|██▎       | 2390/10570 [00:07<00:29, 279.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 25%|██▌       | 2669/10570 [00:08<00:27, 283.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 2439/10570 [00:08<00:29, 280.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 24%|██▍       | 2567/10570 [00:08<00:29, 271.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 25%|██▌       | 2669/10570 [00:08<00:28, 279.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 25%|██▍       | 2616/10570 [00:08<00:28, 282.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 25%|██▍       | 2642/10570 [00:08<00:28, 283.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 23%|██▎       | 2389/10570 [00:08<00:30, 271.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 22%|██▏       | 2356/10570 [00:07<00:29, 277.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 24%|██▍       | 2552/10570 [00:08<00:29, 269.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 25%|██▌       | 2658/10570 [00:08<00:28, 280.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 25%|██▌       | 2663/10570 [00:08<00:27, 284.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 25%|██▌       | 2684/10570 [00:08<00:27, 285.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 26%|██▌       | 2738/10570 [00:08<00:26, 292.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 23%|██▎       | 2418/10570 [00:08<00:29, 278.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 25%|██▍       | 2599/10570 [00:08<00:29, 266.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 24%|██▍       | 2521/10570 [00:08<00:31, 251.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 26%|██▌       | 2698/10570 [00:08<00:27, 285.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 25%|██▍       | 2596/10570 [00:08<00:28, 276.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 26%|██▌       | 2698/10570 [00:08<00:27, 282.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|██▎       | 2468/10570 [00:08<00:29, 272.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 25%|██▌       | 2646/10570 [00:08<00:27, 285.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 25%|██▌       | 2672/10570 [00:08<00:27, 287.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 23%|██▎       | 2417/10570 [00:08<00:29, 273.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 24%|██▍       | 2580/10570 [00:08<00:29, 271.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 23%|██▎       | 2384/10570 [00:08<00:29, 274.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 25%|██▌       | 2687/10570 [00:08<00:28, 281.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 25%|██▌       | 2693/10570 [00:08<00:27, 286.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 26%|██▌       | 2714/10570 [00:08<00:27, 289.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 23%|██▎       | 2446/10570 [00:08<00:29, 278.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 26%|██▌       | 2769/10570 [00:08<00:26, 295.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 25%|██▍       | 2628/10570 [00:08<00:29, 273.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 24%|██▍       | 2548/10570 [00:08<00:31, 255.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 26%|██▌       | 2728/10570 [00:08<00:27, 289.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 25%|██▍       | 2624/10570 [00:08<00:28, 277.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 26%|██▌       | 2728/10570 [00:08<00:27, 286.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▎       | 2496/10570 [00:08<00:29, 269.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 25%|██▌       | 2676/10570 [00:08<00:27, 287.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 26%|██▌       | 2701/10570 [00:08<00:27, 287.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 23%|██▎       | 2445/10570 [00:08<00:29, 274.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 25%|██▍       | 2608/10570 [00:08<00:29, 273.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 23%|██▎       | 2412/10570 [00:08<00:29, 274.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 26%|██▌       | 2744/10570 [00:08<00:26, 292.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 26%|██▌       | 2717/10570 [00:08<00:27, 284.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 26%|██▌       | 2723/10570 [00:08<00:27, 289.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 23%|██▎       | 2474/10570 [00:08<00:29, 278.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 26%|██▋       | 2800/10570 [00:09<00:26, 296.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 25%|██▌       | 2658/10570 [00:08<00:28, 279.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 24%|██▍       | 2575/10570 [00:08<00:30, 258.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 26%|██▌       | 2758/10570 [00:09<00:26, 291.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 25%|██▌       | 2654/10570 [00:08<00:27, 283.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 26%|██▌       | 2758/10570 [00:09<00:26, 289.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 26%|██▌       | 2706/10570 [00:08<00:27, 288.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 2523/10570 [00:08<00:30, 262.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 26%|██▌       | 2731/10570 [00:08<00:26, 290.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 23%|██▎       | 2473/10570 [00:08<00:29, 274.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 25%|██▍       | 2638/10570 [00:08<00:28, 278.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 23%|██▎       | 2440/10570 [00:08<00:29, 273.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 26%|██▋       | 2775/10570 [00:08<00:26, 296.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 26%|██▌       | 2747/10570 [00:09<00:27, 287.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 26%|██▌       | 2753/10570 [00:09<00:26, 291.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 25%|██▌       | 2687/10570 [00:08<00:27, 282.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 27%|██▋       | 2830/10570 [00:09<00:26, 296.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 24%|██▎       | 2502/10570 [00:08<00:30, 264.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 25%|██▍       | 2603/10570 [00:08<00:30, 263.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 26%|██▋       | 2789/10570 [00:09<00:26, 294.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 25%|██▌       | 2683/10570 [00:08<00:27, 283.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 26%|██▋       | 2789/10570 [00:09<00:26, 293.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 26%|██▌       | 2736/10570 [00:08<00:26, 291.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 2550/10570 [00:08<00:30, 264.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 26%|██▌       | 2761/10570 [00:09<00:26, 292.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 25%|██▌       | 2668/10570 [00:08<00:27, 283.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 23%|██▎       | 2468/10570 [00:08<00:29, 272.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 27%|██▋       | 2806/10570 [00:09<00:25, 299.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 24%|██▎       | 2501/10570 [00:08<00:30, 261.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 26%|██▋       | 2778/10570 [00:09<00:26, 292.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 26%|██▋       | 2784/10570 [00:09<00:26, 293.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 26%|██▌       | 2716/10570 [00:09<00:27, 284.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 27%|██▋       | 2860/10570 [00:09<00:26, 294.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 25%|██▍       | 2632/10570 [00:08<00:29, 269.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 24%|██▍       | 2530/10570 [00:08<00:30, 267.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 27%|██▋       | 2819/10570 [00:09<00:26, 295.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 27%|██▋       | 2819/10570 [00:09<00:26, 294.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 26%|██▌       | 2712/10570 [00:08<00:27, 280.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 26%|██▌       | 2767/10570 [00:09<00:26, 295.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|██▍       | 2578/10570 [00:08<00:29, 266.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 26%|██▋       | 2791/10570 [00:09<00:26, 294.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 26%|██▌       | 2697/10570 [00:08<00:27, 283.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 24%|██▎       | 2496/10570 [00:08<00:30, 267.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 24%|██▍       | 2529/10570 [00:08<00:30, 265.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 27%|██▋       | 2814/10570 [00:09<00:26, 295.25it/s][1,mpirank:6,algo-1]<stderr>:#015 27%|██▋       | 2809/10570 [00:09<00:26, 295.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 27%|██▋       | 2837/10570 [00:09<00:25, 298.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 26%|██▌       | 2746/10570 [00:09<00:27, 286.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 27%|██▋       | 2890/10570 [00:09<00:26, 292.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 24%|██▍       | 2557/10570 [00:08<00:29, 267.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 25%|██▌       | 2661/10570 [00:08<00:28, 274.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 27%|██▋       | 2849/10570 [00:09<00:26, 294.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 27%|██▋       | 2849/10570 [00:09<00:26, 292.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 26%|██▌       | 2742/10570 [00:09<00:27, 283.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 26%|██▋       | 2798/10570 [00:09<00:26, 297.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▍       | 2605/10570 [00:08<00:30, 264.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 27%|██▋       | 2821/10570 [00:09<00:26, 295.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 26%|██▌       | 2727/10570 [00:09<00:27, 286.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 24%|██▍       | 2556/10570 [00:08<00:30, 265.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 27%|██▋       | 2867/10570 [00:09<00:25, 296.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 27%|██▋       | 2844/10570 [00:09<00:26, 293.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 27%|██▋       | 2839/10570 [00:09<00:26, 293.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 24%|██▍       | 2523/10570 [00:08<00:30, 259.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 26%|██▋       | 2777/10570 [00:09<00:26, 291.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 28%|██▊       | 2920/10570 [00:09<00:26, 291.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 24%|██▍       | 2585/10570 [00:08<00:29, 269.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 25%|██▌       | 2690/10570 [00:08<00:28, 276.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 27%|██▋       | 2879/10570 [00:09<00:26, 289.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 26%|██▌       | 2773/10570 [00:09<00:26, 289.29it/s][1,mpirank:9,algo-2]<stderr>:#015 27%|██▋       | 2879/10570 [00:09<00:26, 291.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 27%|██▋       | 2828/10570 [00:09<00:26, 296.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▍       | 2634/10570 [00:08<00:29, 271.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 27%|██▋       | 2851/10570 [00:09<00:26, 292.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 26%|██▌       | 2757/10570 [00:09<00:27, 288.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 24%|██▍       | 2584/10570 [00:08<00:29, 267.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 27%|██▋       | 2897/10570 [00:09<00:26, 295.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 27%|██▋       | 2874/10570 [00:09<00:26, 292.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 27%|██▋       | 2869/10570 [00:09<00:26, 292.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 24%|██▍       | 2550/10570 [00:08<00:30, 260.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 27%|██▋       | 2807/10570 [00:09<00:26, 292.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 28%|██▊       | 2950/10570 [00:09<00:26, 291.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 25%|██▍       | 2614/10570 [00:08<00:29, 273.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 26%|██▌       | 2719/10570 [00:09<00:28, 278.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 28%|██▊       | 2908/10570 [00:09<00:26, 289.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 27%|██▋       | 2803/10570 [00:09<00:26, 291.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 28%|██▊       | 2909/10570 [00:09<00:26, 290.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 27%|██▋       | 2858/10570 [00:09<00:26, 294.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▌       | 2663/10570 [00:08<00:28, 276.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 27%|██▋       | 2881/10570 [00:09<00:26, 291.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 26%|██▋       | 2787/10570 [00:09<00:26, 291.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 25%|██▍       | 2613/10570 [00:08<00:29, 272.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 28%|██▊       | 2927/10570 [00:09<00:25, 295.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 27%|██▋       | 2899/10570 [00:09<00:26, 289.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 24%|██▍       | 2577/10570 [00:08<00:30, 262.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 27%|██▋       | 2904/10570 [00:09<00:26, 289.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 27%|██▋       | 2837/10570 [00:09<00:26, 290.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 28%|██▊       | 2980/10570 [00:09<00:26, 290.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 25%|██▌       | 2643/10570 [00:08<00:28, 277.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 26%|██▌       | 2748/10570 [00:09<00:27, 280.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 28%|██▊       | 2937/10570 [00:09<00:26, 286.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 27%|██▋       | 2833/10570 [00:09<00:26, 290.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 28%|██▊       | 2939/10570 [00:09<00:26, 290.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|██▌       | 2692/10570 [00:09<00:28, 278.63it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 27%|██▋       | 2888/10570 [00:09<00:26, 292.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 28%|██▊       | 2911/10570 [00:09<00:26, 290.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 27%|██▋       | 2817/10570 [00:09<00:26, 292.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 25%|██▍       | 2642/10570 [00:08<00:28, 275.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 28%|██▊       | 2957/10570 [00:09<00:25, 294.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 25%|██▍       | 2605/10570 [00:08<00:29, 266.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 28%|██▊       | 2929/10570 [00:09<00:26, 289.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 28%|██▊       | 2934/10570 [00:09<00:26, 289.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 27%|██▋       | 2867/10570 [00:09<00:26, 289.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 28%|██▊       | 3010/10570 [00:09<00:25, 291.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 25%|██▌       | 2672/10570 [00:08<00:28, 280.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 26%|██▋       | 2778/10570 [00:09<00:27, 284.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 28%|██▊       | 2966/10570 [00:09<00:26, 287.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 27%|██▋       | 2863/10570 [00:09<00:26, 289.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 28%|██▊       | 2969/10570 [00:09<00:26, 290.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 2721/10570 [00:09<00:27, 280.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 28%|██▊       | 2918/10570 [00:09<00:26, 289.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 28%|██▊       | 2941/10570 [00:09<00:26, 291.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 27%|██▋       | 2847/10570 [00:09<00:26, 291.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 25%|██▌       | 2671/10570 [00:09<00:28, 278.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 28%|██▊       | 2987/10570 [00:09<00:25, 294.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 25%|██▍       | 2634/10570 [00:08<00:29, 271.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 28%|██▊       | 2958/10570 [00:09<00:26, 289.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 28%|██▊       | 2963/10570 [00:09<00:26, 287.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 27%|██▋       | 2896/10570 [00:09<00:26, 287.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 29%|██▉       | 3040/10570 [00:09<00:25, 292.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 26%|██▌       | 2701/10570 [00:09<00:28, 280.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 27%|██▋       | 2808/10570 [00:09<00:27, 286.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 28%|██▊       | 2996/10570 [00:09<00:26, 288.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 27%|██▋       | 2892/10570 [00:09<00:26, 287.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 28%|██▊       | 2999/10570 [00:09<00:26, 290.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▌       | 2750/10570 [00:09<00:27, 282.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 28%|██▊       | 2948/10570 [00:09<00:26, 290.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 28%|██▊       | 2971/10570 [00:09<00:26, 290.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 27%|██▋       | 2877/10570 [00:09<00:26, 288.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 26%|██▌       | 2699/10570 [00:09<00:28, 278.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 29%|██▊       | 3017/10570 [00:09<00:25, 294.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 25%|██▌       | 2662/10570 [00:09<00:28, 274.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 28%|██▊       | 2987/10570 [00:09<00:26, 289.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 28%|██▊       | 2992/10570 [00:09<00:26, 287.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 28%|██▊       | 2925/10570 [00:09<00:26, 287.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 29%|██▉       | 3070/10570 [00:10<00:25, 290.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 26%|██▌       | 2731/10570 [00:09<00:27, 282.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 27%|██▋       | 2837/10570 [00:09<00:27, 284.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 29%|██▊       | 3026/10570 [00:09<00:26, 289.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 28%|██▊       | 2921/10570 [00:09<00:26, 288.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 29%|██▊       | 3029/10570 [00:09<00:25, 290.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|██▋       | 2780/10570 [00:09<00:27, 286.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 28%|██▊       | 2978/10570 [00:09<00:26, 289.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 28%|██▊       | 3001/10570 [00:09<00:26, 289.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 27%|██▋       | 2906/10570 [00:09<00:26, 286.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 26%|██▌       | 2728/10570 [00:09<00:28, 279.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 29%|██▉       | 3047/10570 [00:09<00:25, 294.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 25%|██▌       | 2690/10570 [00:09<00:28, 274.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 29%|██▊       | 3021/10570 [00:09<00:26, 288.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 29%|██▊       | 3017/10570 [00:09<00:26, 289.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 28%|██▊       | 2954/10570 [00:09<00:26, 287.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 29%|██▉       | 3100/10570 [00:10<00:25, 291.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 26%|██▌       | 2760/10570 [00:09<00:27, 284.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 27%|██▋       | 2866/10570 [00:09<00:27, 282.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 29%|██▉       | 3055/10570 [00:10<00:25, 289.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 28%|██▊       | 2950/10570 [00:09<00:26, 288.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 29%|██▉       | 3059/10570 [00:10<00:25, 289.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 2810/10570 [00:09<00:26, 287.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 28%|██▊       | 3008/10570 [00:09<00:26, 290.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 29%|██▊       | 3031/10570 [00:09<00:25, 290.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 28%|██▊       | 2935/10570 [00:09<00:26, 286.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 26%|██▌       | 2757/10570 [00:09<00:27, 281.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 26%|██▌       | 2718/10570 [00:09<00:28, 275.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 29%|██▉       | 3077/10570 [00:09<00:25, 292.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 29%|██▉       | 3050/10570 [00:10<00:26, 287.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 29%|██▉       | 3046/10570 [00:10<00:26, 289.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 28%|██▊       | 2983/10570 [00:09<00:26, 286.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 26%|██▋       | 2789/10570 [00:09<00:27, 286.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 30%|██▉       | 3130/10570 [00:10<00:26, 285.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 27%|██▋       | 2895/10570 [00:09<00:27, 280.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 29%|██▉       | 3084/10570 [00:10<00:25, 288.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 28%|██▊       | 2979/10570 [00:09<00:26, 287.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 29%|██▉       | 3088/10570 [00:10<00:25, 288.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 2839/10570 [00:09<00:27, 285.85it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 29%|██▊       | 3038/10570 [00:09<00:25, 290.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 28%|██▊       | 2964/10570 [00:09<00:26, 286.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 29%|██▉       | 3061/10570 [00:10<00:25, 289.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 26%|██▋       | 2786/10570 [00:09<00:27, 283.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 26%|██▌       | 2746/10570 [00:09<00:28, 276.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 29%|██▉       | 3107/10570 [00:10<00:25, 293.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 29%|██▉       | 3079/10570 [00:10<00:26, 287.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 29%|██▉       | 3075/10570 [00:10<00:26, 285.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 29%|██▊       | 3013/10570 [00:10<00:26, 287.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 27%|██▋       | 2818/10570 [00:09<00:27, 286.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 30%|██▉       | 3159/10570 [00:10<00:25, 285.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 28%|██▊       | 2924/10570 [00:09<00:27, 280.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 29%|██▉       | 3113/10570 [00:10<00:25, 288.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 28%|██▊       | 3009/10570 [00:09<00:26, 288.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 29%|██▉       | 3117/10570 [00:10<00:25, 289.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 2868/10570 [00:09<00:27, 284.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 29%|██▉       | 3068/10570 [00:10<00:25, 289.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 29%|██▉       | 3090/10570 [00:10<00:25, 288.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 28%|██▊       | 2993/10570 [00:09<00:26, 286.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 27%|██▋       | 2815/10570 [00:09<00:27, 284.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 26%|██▋       | 2775/10570 [00:09<00:27, 280.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 30%|██▉       | 3137/10570 [00:10<00:25, 293.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 29%|██▉       | 3109/10570 [00:10<00:25, 287.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 29%|██▉       | 3105/10570 [00:10<00:25, 287.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 29%|██▉       | 3042/10570 [00:10<00:26, 288.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 27%|██▋       | 2847/10570 [00:09<00:27, 284.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 30%|███       | 3188/10570 [00:10<00:25, 284.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 28%|██▊       | 2953/10570 [00:09<00:27, 280.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 30%|██▉       | 3142/10570 [00:10<00:25, 288.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 29%|██▊       | 3038/10570 [00:10<00:26, 287.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 30%|██▉       | 3146/10570 [00:10<00:25, 286.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|██▋       | 2897/10570 [00:09<00:27, 281.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 29%|██▉       | 3098/10570 [00:10<00:25, 290.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 30%|██▉       | 3119/10570 [00:10<00:25, 288.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 29%|██▊       | 3022/10570 [00:10<00:26, 286.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 27%|██▋       | 2804/10570 [00:09<00:27, 282.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 27%|██▋       | 2844/10570 [00:09<00:27, 278.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 30%|██▉       | 3167/10570 [00:10<00:25, 292.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 30%|██▉       | 3138/10570 [00:10<00:25, 288.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 30%|██▉       | 3134/10570 [00:10<00:25, 288.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 29%|██▉       | 3071/10570 [00:10<00:26, 285.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 27%|██▋       | 2876/10570 [00:09<00:27, 283.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 30%|███       | 3217/10570 [00:10<00:26, 278.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 28%|██▊       | 2982/10570 [00:09<00:27, 278.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 30%|███       | 3171/10570 [00:10<00:25, 288.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 29%|██▉       | 3067/10570 [00:10<00:26, 286.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 30%|███       | 3175/10570 [00:10<00:25, 285.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 2926/10570 [00:09<00:27, 281.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 30%|██▉       | 3128/10570 [00:10<00:25, 290.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 30%|██▉       | 3148/10570 [00:10<00:25, 287.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 29%|██▉       | 3051/10570 [00:10<00:26, 286.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 27%|██▋       | 2833/10570 [00:09<00:27, 280.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 27%|██▋       | 2872/10570 [00:09<00:27, 278.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 30%|██▉       | 3167/10570 [00:10<00:25, 286.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 30%|███       | 3197/10570 [00:10<00:25, 291.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 30%|██▉       | 3163/10570 [00:10<00:25, 286.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 29%|██▉       | 3101/10570 [00:10<00:26, 286.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 27%|██▋       | 2905/10570 [00:09<00:27, 282.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 28%|██▊       | 3011/10570 [00:10<00:26, 280.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 30%|███       | 3200/10570 [00:10<00:25, 287.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 31%|███       | 3245/10570 [00:10<00:27, 270.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 29%|██▉       | 3097/10570 [00:10<00:25, 287.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 30%|███       | 3204/10570 [00:10<00:25, 285.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 2955/10570 [00:09<00:27, 280.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 30%|███       | 3177/10570 [00:10<00:25, 287.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 29%|██▉       | 3080/10570 [00:10<00:26, 284.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 30%|██▉       | 3158/10570 [00:10<00:25, 288.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 27%|██▋       | 2900/10570 [00:09<00:27, 277.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 27%|██▋       | 2862/10570 [00:09<00:27, 278.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 30%|███       | 3196/10570 [00:10<00:25, 285.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 30%|███       | 3192/10570 [00:10<00:25, 285.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 31%|███       | 3227/10570 [00:10<00:25, 290.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 30%|██▉       | 3130/10570 [00:10<00:25, 286.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 28%|██▊       | 2934/10570 [00:09<00:27, 282.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 31%|███       | 3229/10570 [00:10<00:25, 286.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 31%|███       | 3273/10570 [00:10<00:27, 266.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 29%|██▉       | 3040/10570 [00:10<00:28, 263.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 30%|██▉       | 3126/10570 [00:10<00:26, 286.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 31%|███       | 3233/10570 [00:10<00:25, 285.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 30%|███       | 3206/10570 [00:10<00:25, 287.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 29%|██▉       | 3109/10570 [00:10<00:26, 284.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 30%|███       | 3187/10570 [00:10<00:25, 286.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|██▊       | 2984/10570 [00:10<00:28, 270.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 28%|██▊       | 2929/10570 [00:09<00:27, 278.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 31%|███       | 3225/10570 [00:10<00:25, 285.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 27%|██▋       | 2890/10570 [00:09<00:27, 276.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 30%|███       | 3221/10570 [00:10<00:25, 283.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 31%|███       | 3257/10570 [00:10<00:25, 290.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 30%|██▉       | 3159/10570 [00:10<00:26, 284.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 28%|██▊       | 2963/10570 [00:09<00:27, 281.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 31%|███       | 3258/10570 [00:10<00:25, 286.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 31%|███       | 3300/10570 [00:10<00:27, 262.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 29%|██▉       | 3068/10570 [00:10<00:28, 265.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 30%|██▉       | 3155/10570 [00:10<00:26, 284.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 31%|███       | 3262/10570 [00:10<00:25, 285.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 31%|███       | 3235/10570 [00:10<00:25, 286.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 30%|██▉       | 3138/10570 [00:10<00:26, 285.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 30%|███       | 3217/10570 [00:10<00:25, 288.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▊       | 3013/10570 [00:10<00:27, 274.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 28%|██▊       | 2957/10570 [00:10<00:27, 277.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 31%|███       | 3254/10570 [00:10<00:25, 285.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 28%|██▊       | 2918/10570 [00:09<00:27, 276.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 31%|███       | 3287/10570 [00:10<00:25, 290.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 30%|███       | 3188/10570 [00:10<00:26, 282.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 31%|███       | 3250/10570 [00:10<00:26, 271.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 28%|██▊       | 2992/10570 [00:10<00:26, 281.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 31%|███       | 3287/10570 [00:10<00:25, 280.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 31%|███▏      | 3327/10570 [00:10<00:27, 261.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 29%|██▉       | 3097/10570 [00:10<00:27, 270.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 30%|███       | 3184/10570 [00:10<00:26, 283.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 31%|███       | 3291/10570 [00:10<00:25, 286.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 31%|███       | 3264/10570 [00:10<00:25, 286.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 30%|██▉       | 3167/10570 [00:10<00:26, 283.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 31%|███       | 3246/10570 [00:10<00:25, 286.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 3042/10570 [00:10<00:27, 276.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 28%|██▊       | 2985/10570 [00:10<00:27, 277.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 28%|██▊       | 2946/10570 [00:10<00:27, 277.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 31%|███▏      | 3317/10570 [00:10<00:24, 292.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 31%|███       | 3283/10570 [00:10<00:25, 280.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 30%|███       | 3217/10570 [00:10<00:25, 283.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 29%|██▊       | 3021/10570 [00:10<00:26, 281.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 31%|███       | 3278/10570 [00:10<00:27, 266.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 31%|███▏      | 3317/10570 [00:10<00:25, 284.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 32%|███▏      | 3354/10570 [00:11<00:27, 261.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 30%|██▉       | 3125/10570 [00:10<00:27, 272.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 30%|███       | 3213/10570 [00:10<00:25, 283.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 31%|███▏      | 3321/10570 [00:10<00:25, 287.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 31%|███       | 3293/10570 [00:10<00:25, 286.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 30%|███       | 3196/10570 [00:10<00:26, 282.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 31%|███       | 3276/10570 [00:10<00:25, 287.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 3070/10570 [00:10<00:27, 276.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 29%|██▊       | 3014/10570 [00:10<00:27, 278.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 28%|██▊       | 2974/10570 [00:10<00:27, 276.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 32%|███▏      | 3347/10570 [00:10<00:24, 293.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 31%|███▏      | 3313/10570 [00:10<00:25, 283.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 31%|███       | 3246/10570 [00:10<00:25, 282.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 29%|██▉       | 3050/10570 [00:10<00:26, 281.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 31%|███▏      | 3305/10570 [00:10<00:27, 262.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 32%|███▏      | 3347/10570 [00:11<00:25, 287.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 30%|██▉       | 3153/10570 [00:10<00:27, 272.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 31%|███       | 3242/10570 [00:10<00:25, 282.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 32%|███▏      | 3351/10570 [00:11<00:24, 289.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 32%|███▏      | 3381/10570 [00:11<00:29, 247.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 31%|███▏      | 3323/10570 [00:10<00:25, 288.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 31%|███▏      | 3305/10570 [00:10<00:25, 287.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 31%|███       | 3225/10570 [00:10<00:25, 282.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|██▉       | 3099/10570 [00:10<00:26, 278.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 28%|██▊       | 3002/10570 [00:10<00:27, 275.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 29%|██▉       | 3043/10570 [00:10<00:26, 278.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 32%|███▏      | 3377/10570 [00:10<00:24, 293.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 32%|███▏      | 3342/10570 [00:11<00:25, 284.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 31%|███       | 3275/10570 [00:10<00:25, 283.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 29%|██▉       | 3079/10570 [00:10<00:26, 279.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 32%|███▏      | 3332/10570 [00:11<00:27, 261.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 32%|███▏      | 3376/10570 [00:11<00:25, 287.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 30%|███       | 3181/10570 [00:10<00:27, 272.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 31%|███       | 3271/10570 [00:10<00:25, 283.83it/s][1,mpirank:9,algo-2]<stderr>:#015 32%|███▏      | 3380/10570 [00:11<00:24, 288.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 32%|███▏      | 3407/10570 [00:11<00:28, 249.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 32%|███▏      | 3352/10570 [00:11<00:25, 283.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 32%|███▏      | 3335/10570 [00:10<00:24, 289.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 31%|███       | 3254/10570 [00:10<00:25, 281.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|██▉       | 3128/10570 [00:10<00:26, 279.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 29%|██▊       | 3030/10570 [00:10<00:27, 276.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 29%|██▉       | 3071/10570 [00:10<00:27, 275.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 32%|███▏      | 3407/10570 [00:11<00:24, 293.70it/s][1,mpirank:5,algo-1]<stderr>:#015 32%|███▏      | 3371/10570 [00:11<00:25, 285.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 31%|███▏      | 3304/10570 [00:11<00:25, 284.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 29%|██▉       | 3108/10570 [00:10<00:26, 279.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 32%|███▏      | 3359/10570 [00:11<00:27, 259.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 32%|███▏      | 3406/10570 [00:11<00:24, 288.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 32%|███▏      | 3409/10570 [00:11<00:24, 289.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 30%|███       | 3210/10570 [00:10<00:26, 275.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 31%|███       | 3300/10570 [00:10<00:25, 284.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 32%|███▏      | 3433/10570 [00:11<00:28, 248.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 32%|███▏      | 3381/10570 [00:11<00:25, 284.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 32%|███▏      | 3365/10570 [00:11<00:24, 289.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 31%|███       | 3283/10570 [00:10<00:25, 282.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|██▉       | 3156/10570 [00:10<00:26, 277.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 29%|██▉       | 3058/10570 [00:10<00:27, 275.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 29%|██▉       | 3100/10570 [00:10<00:26, 278.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 32%|███▏      | 3400/10570 [00:11<00:25, 286.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 33%|███▎      | 3437/10570 [00:11<00:24, 293.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 32%|███▏      | 3333/10570 [00:11<00:25, 285.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 30%|██▉       | 3137/10570 [00:10<00:26, 280.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 32%|███▏      | 3385/10570 [00:11<00:27, 258.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 32%|███▏      | 3435/10570 [00:11<00:24, 288.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 31%|███       | 3238/10570 [00:10<00:26, 275.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 33%|███▎      | 3438/10570 [00:11<00:24, 285.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 32%|███▏      | 3330/10570 [00:11<00:25, 286.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 32%|███▏      | 3410/10570 [00:11<00:25, 286.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 33%|███▎      | 3460/10570 [00:11<00:28, 252.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 31%|███▏      | 3312/10570 [00:11<00:25, 283.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 32%|███▏      | 3395/10570 [00:11<00:24, 290.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 3184/10570 [00:10<00:26, 277.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 29%|██▉       | 3086/10570 [00:10<00:27, 275.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 30%|██▉       | 3128/10570 [00:10<00:26, 278.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 32%|███▏      | 3429/10570 [00:11<00:24, 286.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 33%|███▎      | 3467/10570 [00:11<00:24, 293.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 32%|███▏      | 3362/10570 [00:11<00:25, 286.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 30%|██▉       | 3166/10570 [00:10<00:26, 278.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 32%|███▏      | 3411/10570 [00:11<00:27, 257.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 33%|███▎      | 3465/10570 [00:11<00:24, 289.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 31%|███       | 3266/10570 [00:11<00:26, 275.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 33%|███▎      | 3467/10570 [00:11<00:24, 287.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 32%|███▏      | 3359/10570 [00:11<00:25, 287.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 33%|███▎      | 3439/10570 [00:11<00:24, 286.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 33%|███▎      | 3486/10570 [00:11<00:28, 252.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 32%|███▏      | 3341/10570 [00:11<00:25, 284.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 32%|███▏      | 3425/10570 [00:11<00:24, 289.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|███       | 3213/10570 [00:10<00:26, 278.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 29%|██▉       | 3114/10570 [00:10<00:27, 275.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 30%|██▉       | 3156/10570 [00:10<00:26, 277.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 33%|███▎      | 3459/10570 [00:11<00:24, 288.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 33%|███▎      | 3497/10570 [00:11<00:24, 292.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 32%|███▏      | 3391/10570 [00:11<00:25, 286.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 30%|███       | 3194/10570 [00:10<00:26, 277.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 33%|███▎      | 3437/10570 [00:11<00:27, 255.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 33%|███▎      | 3494/10570 [00:11<00:24, 288.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 31%|███       | 3294/10570 [00:11<00:26, 275.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 33%|███▎      | 3496/10570 [00:11<00:24, 286.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 32%|███▏      | 3388/10570 [00:11<00:25, 286.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 33%|███▎      | 3468/10570 [00:11<00:24, 286.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 33%|███▎      | 3512/10570 [00:11<00:27, 253.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 32%|███▏      | 3370/10570 [00:11<00:25, 284.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 33%|███▎      | 3455/10570 [00:11<00:24, 290.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 3241/10570 [00:11<00:26, 278.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 30%|██▉       | 3142/10570 [00:10<00:27, 274.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 30%|███       | 3184/10570 [00:10<00:26, 275.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 33%|███▎      | 3488/10570 [00:11<00:24, 286.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 33%|███▎      | 3527/10570 [00:11<00:23, 293.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 32%|███▏      | 3420/10570 [00:11<00:25, 285.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 30%|███       | 3222/10570 [00:10<00:26, 277.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 33%|███▎      | 3463/10570 [00:11<00:27, 255.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 33%|███▎      | 3523/10570 [00:11<00:25, 280.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 31%|███▏      | 3322/10570 [00:11<00:26, 276.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 33%|███▎      | 3525/10570 [00:11<00:24, 287.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 32%|███▏      | 3417/10570 [00:11<00:25, 285.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 33%|███▎      | 3497/10570 [00:11<00:24, 286.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 33%|███▎      | 3539/10570 [00:11<00:27, 255.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 32%|███▏      | 3399/10570 [00:11<00:25, 285.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 3270/10570 [00:11<00:26, 278.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 33%|███▎      | 3485/10570 [00:11<00:24, 288.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 30%|██▉       | 3170/10570 [00:10<00:26, 274.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 30%|███       | 3212/10570 [00:11<00:26, 275.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 33%|███▎      | 3517/10570 [00:11<00:24, 285.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 34%|███▎      | 3557/10570 [00:11<00:23, 294.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 33%|███▎      | 3449/10570 [00:11<00:24, 286.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 31%|███       | 3250/10570 [00:10<00:26, 273.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 33%|███▎      | 3489/10570 [00:11<00:27, 253.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 34%|███▎      | 3553/10570 [00:11<00:24, 283.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 32%|███▏      | 3350/10570 [00:11<00:25, 277.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 33%|███▎      | 3446/10570 [00:11<00:24, 285.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 34%|███▎      | 3554/10570 [00:11<00:24, 281.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 33%|███▎      | 3527/10570 [00:11<00:24, 287.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 34%|███▎      | 3565/10570 [00:11<00:27, 254.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 32%|███▏      | 3428/10570 [00:11<00:25, 284.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███       | 3298/10570 [00:11<00:26, 278.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 33%|███▎      | 3515/10570 [00:11<00:24, 289.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 30%|███       | 3198/10570 [00:10<00:27, 273.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 31%|███       | 3240/10570 [00:11<00:26, 275.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 34%|███▎      | 3547/10570 [00:11<00:24, 288.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 34%|███▍      | 3587/10570 [00:11<00:23, 291.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 33%|███▎      | 3478/10570 [00:11<00:24, 285.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 31%|███       | 3279/10570 [00:11<00:26, 275.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 33%|███▎      | 3515/10570 [00:11<00:27, 253.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 34%|███▍      | 3582/10570 [00:11<00:24, 282.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 32%|███▏      | 3378/10570 [00:11<00:25, 278.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 33%|███▎      | 3475/10570 [00:11<00:24, 285.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 34%|███▍      | 3583/10570 [00:11<00:24, 281.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 34%|███▎      | 3556/10570 [00:11<00:24, 287.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 33%|███▎      | 3457/10570 [00:11<00:24, 285.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 34%|███▍      | 3591/10570 [00:12<00:27, 254.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|███▏      | 3327/10570 [00:11<00:25, 280.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 34%|███▎      | 3544/10570 [00:11<00:24, 288.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 31%|███       | 3268/10570 [00:11<00:26, 276.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 31%|███       | 3226/10570 [00:11<00:26, 272.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 34%|███▍      | 3576/10570 [00:11<00:24, 285.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 34%|███▍      | 3617/10570 [00:11<00:23, 291.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 33%|███▎      | 3507/10570 [00:11<00:24, 285.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 31%|███▏      | 3308/10570 [00:11<00:26, 277.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 34%|███▎      | 3542/10570 [00:11<00:27, 256.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 34%|███▍      | 3611/10570 [00:11<00:24, 284.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 32%|███▏      | 3406/10570 [00:11<00:25, 278.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 33%|███▎      | 3504/10570 [00:11<00:24, 285.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 34%|███▍      | 3612/10570 [00:12<00:24, 283.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 34%|███▍      | 3585/10570 [00:11<00:24, 285.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 34%|███▍      | 3617/10570 [00:12<00:27, 254.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 33%|███▎      | 3486/10570 [00:11<00:24, 284.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 34%|███▍      | 3573/10570 [00:11<00:24, 287.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 31%|███       | 3296/10570 [00:11<00:26, 276.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 3356/10570 [00:11<00:26, 273.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 31%|███       | 3254/10570 [00:11<00:26, 272.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 34%|███▍      | 3605/10570 [00:11<00:24, 285.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 35%|███▍      | 3647/10570 [00:11<00:23, 293.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 33%|███▎      | 3536/10570 [00:11<00:24, 286.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 32%|███▏      | 3337/10570 [00:11<00:25, 278.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 34%|███▍      | 3571/10570 [00:12<00:26, 264.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 32%|███▏      | 3434/10570 [00:11<00:25, 278.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 34%|███▍      | 3641/10570 [00:12<00:24, 286.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 33%|███▎      | 3534/10570 [00:11<00:24, 287.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 34%|███▍      | 3642/10570 [00:12<00:24, 285.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 34%|███▍      | 3614/10570 [00:11<00:24, 286.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 33%|███▎      | 3515/10570 [00:11<00:24, 284.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 34%|███▍      | 3644/10570 [00:12<00:27, 256.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 34%|███▍      | 3602/10570 [00:11<00:24, 286.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 31%|███▏      | 3325/10570 [00:11<00:26, 277.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 31%|███       | 3282/10570 [00:11<00:26, 272.56it/s][1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 3385/10570 [00:11<00:26, 275.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 34%|███▍      | 3635/10570 [00:12<00:24, 287.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 35%|███▍      | 3677/10570 [00:12<00:23, 293.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 34%|███▎      | 3565/10570 [00:12<00:24, 283.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 32%|███▏      | 3366/10570 [00:11<00:25, 279.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 34%|███▍      | 3600/10570 [00:12<00:25, 269.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 33%|███▎      | 3463/10570 [00:11<00:25, 279.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 35%|███▍      | 3671/10570 [00:12<00:23, 288.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 34%|███▎      | 3563/10570 [00:11<00:24, 286.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 35%|███▍      | 3671/10570 [00:12<00:24, 286.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 34%|███▍      | 3644/10570 [00:12<00:24, 287.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 34%|███▎      | 3544/10570 [00:11<00:24, 285.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 35%|███▍      | 3670/10570 [00:12<00:27, 251.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 34%|███▍      | 3632/10570 [00:11<00:24, 288.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|███▏      | 3413/10570 [00:11<00:25, 276.38it/s]#033[A[1,mpirank:8,algo-2]<stderr>:#015 32%|███▏      | 3354/10570 [00:11<00:25, 278.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 31%|███▏      | 3310/10570 [00:11<00:26, 273.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 35%|███▍      | 3664/10570 [00:12<00:24, 286.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 35%|███▌      | 3707/10570 [00:12<00:23, 293.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 34%|███▍      | 3594/10570 [00:12<00:24, 282.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 34%|███▍      | 3630/10570 [00:12<00:25, 276.47it/s][1,mpirank:11,algo-2]<stderr>:#015 32%|███▏      | 3395/10570 [00:11<00:25, 279.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 33%|███▎      | 3491/10570 [00:11<00:25, 278.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 35%|███▌      | 3700/10570 [00:12<00:23, 287.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 34%|███▍      | 3592/10570 [00:12<00:24, 285.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 35%|███▌      | 3700/10570 [00:12<00:23, 287.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 35%|███▍      | 3673/10570 [00:12<00:23, 287.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 34%|███▍      | 3573/10570 [00:12<00:24, 283.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 35%|███▍      | 3696/10570 [00:12<00:27, 252.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 35%|███▍      | 3662/10570 [00:12<00:23, 289.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 32%|███▏      | 3382/10570 [00:11<00:25, 278.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 32%|███▏      | 3338/10570 [00:11<00:26, 274.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 3442/10570 [00:11<00:25, 277.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 35%|███▍      | 3693/10570 [00:12<00:23, 287.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 35%|███▌      | 3737/10570 [00:12<00:23, 293.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 34%|███▍      | 3623/10570 [00:12<00:24, 283.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 32%|███▏      | 3423/10570 [00:11<00:25, 278.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 35%|███▍      | 3660/10570 [00:12<00:24, 280.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 33%|███▎      | 3519/10570 [00:11<00:25, 278.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 35%|███▌      | 3730/10570 [00:12<00:23, 288.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 35%|███▌      | 3729/10570 [00:12<00:23, 287.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 34%|███▍      | 3621/10570 [00:12<00:24, 285.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 35%|███▌      | 3702/10570 [00:12<00:23, 287.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 34%|███▍      | 3602/10570 [00:12<00:24, 282.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 35%|███▌      | 3724/10570 [00:12<00:26, 260.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 35%|███▍      | 3692/10570 [00:12<00:23, 290.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 32%|███▏      | 3410/10570 [00:11<00:25, 278.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 32%|███▏      | 3366/10570 [00:11<00:26, 275.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 35%|███▌      | 3722/10570 [00:12<00:23, 287.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 3471/10570 [00:11<00:25, 278.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 36%|███▌      | 3767/10570 [00:12<00:23, 294.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 35%|███▍      | 3653/10570 [00:12<00:24, 285.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 33%|███▎      | 3452/10570 [00:11<00:25, 280.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 35%|███▍      | 3690/10570 [00:12<00:24, 283.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 34%|███▎      | 3547/10570 [00:12<00:25, 274.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 36%|███▌      | 3760/10570 [00:12<00:23, 289.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 36%|███▌      | 3758/10570 [00:12<00:23, 288.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 35%|███▍      | 3650/10570 [00:12<00:24, 286.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 35%|███▌      | 3732/10570 [00:12<00:23, 288.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 34%|███▍      | 3631/10570 [00:12<00:24, 283.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 36%|███▌      | 3753/10570 [00:12<00:25, 268.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 33%|███▎      | 3438/10570 [00:11<00:25, 278.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 35%|███▌      | 3722/10570 [00:12<00:23, 290.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 32%|███▏      | 3394/10570 [00:11<00:26, 275.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 35%|███▌      | 3751/10570 [00:12<00:23, 287.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 3499/10570 [00:11<00:25, 279.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 36%|███▌      | 3797/10570 [00:12<00:23, 292.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 35%|███▍      | 3682/10570 [00:12<00:24, 286.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 33%|███▎      | 3481/10570 [00:11<00:25, 279.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 35%|███▌      | 3719/10570 [00:12<00:24, 284.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 34%|███▍      | 3575/10570 [00:12<00:25, 274.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 36%|███▌      | 3790/10570 [00:12<00:23, 290.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 36%|███▌      | 3788/10570 [00:12<00:23, 289.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 35%|███▍      | 3679/10570 [00:12<00:24, 283.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 36%|███▌      | 3761/10570 [00:12<00:23, 288.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 35%|███▍      | 3660/10570 [00:12<00:24, 284.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 36%|███▌      | 3784/10570 [00:12<00:24, 278.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 33%|███▎      | 3466/10570 [00:11<00:25, 278.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 36%|███▌      | 3781/10570 [00:12<00:23, 290.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 32%|███▏      | 3422/10570 [00:11<00:26, 274.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 35%|███▌      | 3752/10570 [00:12<00:23, 289.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|███▎      | 3528/10570 [00:12<00:25, 280.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 36%|███▌      | 3827/10570 [00:12<00:23, 293.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 35%|███▌      | 3711/10570 [00:12<00:24, 285.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 33%|███▎      | 3509/10570 [00:11<00:25, 279.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 35%|███▌      | 3748/10570 [00:12<00:23, 284.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 34%|███▍      | 3603/10570 [00:12<00:25, 274.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 36%|███▌      | 3820/10570 [00:12<00:23, 288.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 36%|███▌      | 3817/10570 [00:12<00:23, 288.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 35%|███▌      | 3708/10570 [00:12<00:24, 284.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 36%|███▌      | 3791/10570 [00:12<00:23, 289.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 35%|███▍      | 3689/10570 [00:12<00:24, 285.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 36%|███▌      | 3812/10570 [00:12<00:24, 278.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 33%|███▎      | 3494/10570 [00:12<00:25, 277.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 36%|███▌      | 3782/10570 [00:12<00:23, 292.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 33%|███▎      | 3450/10570 [00:11<00:25, 274.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▎      | 3557/10570 [00:12<00:24, 280.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 36%|███▌      | 3811/10570 [00:12<00:23, 288.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 36%|███▋      | 3857/10570 [00:12<00:22, 294.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 35%|███▌      | 3740/10570 [00:12<00:23, 285.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 33%|███▎      | 3538/10570 [00:12<00:25, 281.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 36%|███▌      | 3778/10570 [00:12<00:23, 288.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 34%|███▍      | 3632/10570 [00:12<00:25, 276.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 36%|███▋      | 3850/10570 [00:12<00:23, 290.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 36%|███▋      | 3847/10570 [00:12<00:23, 289.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 35%|███▌      | 3737/10570 [00:12<00:24, 284.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 36%|███▌      | 3820/10570 [00:12<00:23, 287.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 35%|███▌      | 3718/10570 [00:12<00:24, 284.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 36%|███▋      | 3842/10570 [00:12<00:23, 282.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 33%|███▎      | 3523/10570 [00:12<00:25, 277.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 33%|███▎      | 3478/10570 [00:12<00:25, 272.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 36%|███▌      | 3812/10570 [00:12<00:23, 289.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 36%|███▋      | 3841/10570 [00:12<00:23, 289.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 3586/10570 [00:12<00:25, 275.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 36%|███▌      | 3770/10570 [00:12<00:23, 286.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 37%|███▋      | 3887/10570 [00:12<00:23, 288.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 36%|███▌      | 3807/10570 [00:12<00:23, 287.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 34%|███▎      | 3567/10570 [00:12<00:25, 278.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 35%|███▍      | 3660/10570 [00:12<00:24, 277.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 37%|███▋      | 3876/10570 [00:12<00:23, 289.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 36%|███▌      | 3767/10570 [00:12<00:23, 286.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 37%|███▋      | 3880/10570 [00:12<00:23, 282.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 36%|███▋      | 3850/10570 [00:12<00:23, 288.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 35%|███▌      | 3747/10570 [00:12<00:24, 283.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 37%|███▋      | 3872/10570 [00:13<00:23, 285.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 34%|███▎      | 3551/10570 [00:12<00:25, 273.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 33%|███▎      | 3506/10570 [00:12<00:25, 272.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 36%|███▋      | 3842/10570 [00:12<00:23, 290.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 37%|███▋      | 3870/10570 [00:12<00:23, 286.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 3614/10570 [00:12<00:25, 276.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 36%|███▌      | 3799/10570 [00:12<00:23, 285.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 37%|███▋      | 3917/10570 [00:12<00:22, 290.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 36%|███▋      | 3837/10570 [00:12<00:23, 288.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 34%|███▍      | 3595/10570 [00:12<00:25, 277.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 35%|███▍      | 3689/10570 [00:12<00:24, 279.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 37%|███▋      | 3905/10570 [00:13<00:23, 288.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 36%|███▌      | 3796/10570 [00:12<00:23, 286.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 37%|███▋      | 3909/10570 [00:13<00:23, 284.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 37%|███▋      | 3879/10570 [00:12<00:23, 288.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 36%|███▌      | 3776/10570 [00:12<00:23, 285.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 37%|███▋      | 3902/10570 [00:13<00:23, 287.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 33%|███▎      | 3535/10570 [00:12<00:25, 274.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 37%|███▋      | 3899/10570 [00:12<00:23, 286.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 37%|███▋      | 3872/10570 [00:12<00:23, 289.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|███▍      | 3643/10570 [00:12<00:24, 278.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 36%|███▌      | 3828/10570 [00:12<00:23, 286.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 37%|███▋      | 3947/10570 [00:12<00:22, 291.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 34%|███▍      | 3579/10570 [00:12<00:26, 261.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 34%|███▍      | 3623/10570 [00:12<00:24, 278.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 37%|███▋      | 3866/10570 [00:13<00:23, 285.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 35%|███▌      | 3717/10570 [00:12<00:24, 277.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 37%|███▋      | 3934/10570 [00:13<00:22, 288.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 36%|███▌      | 3826/10570 [00:12<00:23, 287.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 37%|███▋      | 3939/10570 [00:13<00:23, 286.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 37%|███▋      | 3908/10570 [00:12<00:23, 288.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 36%|███▌      | 3805/10570 [00:12<00:23, 284.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 37%|███▋      | 3931/10570 [00:13<00:23, 286.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 37%|███▋      | 3928/10570 [00:13<00:23, 287.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 37%|███▋      | 3901/10570 [00:12<00:23, 288.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 34%|███▎      | 3563/10570 [00:12<00:25, 273.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▍      | 3672/10570 [00:12<00:24, 279.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 36%|███▋      | 3857/10570 [00:13<00:23, 287.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 38%|███▊      | 3977/10570 [00:13<00:22, 293.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 34%|███▍      | 3606/10570 [00:12<00:27, 250.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 35%|███▍      | 3651/10570 [00:12<00:24, 278.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 37%|███▋      | 3895/10570 [00:13<00:23, 286.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 35%|███▌      | 3745/10570 [00:12<00:24, 277.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 37%|███▋      | 3963/10570 [00:13<00:22, 288.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 36%|███▋      | 3855/10570 [00:12<00:23, 287.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 38%|███▊      | 3969/10570 [00:13<00:22, 288.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 37%|███▋      | 3937/10570 [00:13<00:23, 288.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 36%|███▋      | 3834/10570 [00:12<00:23, 285.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 37%|███▋      | 3961/10570 [00:13<00:22, 287.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 37%|███▋      | 3957/10570 [00:13<00:22, 287.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 37%|███▋      | 3931/10570 [00:13<00:22, 289.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 34%|███▍      | 3591/10570 [00:12<00:25, 272.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 37%|███▋      | 3886/10570 [00:13<00:23, 287.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▌      | 3701/10570 [00:12<00:24, 280.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 38%|███▊      | 4007/10570 [00:13<00:22, 292.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 34%|███▍      | 3632/10570 [00:12<00:27, 248.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 35%|███▍      | 3680/10570 [00:12<00:24, 279.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 37%|███▋      | 3925/10570 [00:13<00:23, 287.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 36%|███▌      | 3774/10570 [00:12<00:24, 279.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 38%|███▊      | 3993/10570 [00:13<00:22, 289.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 37%|███▋      | 3885/10570 [00:13<00:23, 288.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 38%|███▊      | 3998/10570 [00:13<00:22, 288.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 38%|███▊      | 3966/10570 [00:13<00:22, 288.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 37%|███▋      | 3863/10570 [00:13<00:23, 282.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 38%|███▊      | 3991/10570 [00:13<00:22, 289.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 38%|███▊      | 3987/10570 [00:13<00:22, 289.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 37%|███▋      | 3961/10570 [00:13<00:22, 289.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 37%|███▋      | 3915/10570 [00:13<00:23, 287.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 34%|███▍      | 3619/10570 [00:12<00:25, 272.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|███▌      | 3730/10570 [00:12<00:24, 280.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 38%|███▊      | 4037/10570 [00:13<00:22, 293.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 35%|███▍      | 3657/10570 [00:12<00:27, 248.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 35%|███▌      | 3708/10570 [00:12<00:24, 278.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 37%|███▋      | 3955/10570 [00:13<00:22, 288.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 36%|███▌      | 3802/10570 [00:12<00:24, 278.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 38%|███▊      | 4022/10570 [00:13<00:22, 288.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 37%|███▋      | 3914/10570 [00:13<00:23, 288.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 38%|███▊      | 4028/10570 [00:13<00:22, 289.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 38%|███▊      | 3995/10570 [00:13<00:22, 286.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 37%|███▋      | 3892/10570 [00:13<00:23, 282.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 38%|███▊      | 4020/10570 [00:13<00:22, 289.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 38%|███▊      | 4016/10570 [00:13<00:22, 288.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 37%|███▋      | 3944/10570 [00:13<00:23, 287.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 38%|███▊      | 3991/10570 [00:13<00:22, 291.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 35%|███▍      | 3647/10570 [00:12<00:25, 273.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 38%|███▊      | 4067/10570 [00:13<00:22, 295.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 3759/10570 [00:12<00:24, 280.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 35%|███▍      | 3682/10570 [00:12<00:27, 247.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 35%|███▌      | 3736/10570 [00:12<00:24, 279.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 38%|███▊      | 3985/10570 [00:13<00:22, 289.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 36%|███▌      | 3830/10570 [00:13<00:24, 278.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 38%|███▊      | 4052/10570 [00:13<00:22, 289.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 37%|███▋      | 3943/10570 [00:13<00:22, 288.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 38%|███▊      | 4058/10570 [00:13<00:22, 290.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 38%|███▊      | 4024/10570 [00:13<00:22, 287.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 37%|███▋      | 3921/10570 [00:13<00:23, 284.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 38%|███▊      | 4050/10570 [00:13<00:22, 291.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 38%|███▊      | 4046/10570 [00:13<00:22, 289.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 38%|███▊      | 3973/10570 [00:13<00:22, 287.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 35%|███▍      | 3675/10570 [00:12<00:25, 274.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 38%|███▊      | 4021/10570 [00:13<00:22, 290.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 3788/10570 [00:12<00:24, 281.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 39%|███▉      | 4097/10570 [00:13<00:22, 292.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 35%|███▌      | 3708/10570 [00:12<00:27, 250.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 36%|███▌      | 3765/10570 [00:12<00:24, 279.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 38%|███▊      | 4014/10570 [00:13<00:22, 288.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 37%|███▋      | 3859/10570 [00:13<00:24, 279.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 39%|███▊      | 4082/10570 [00:13<00:22, 290.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 38%|███▊      | 3973/10570 [00:13<00:22, 289.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 39%|███▊      | 4088/10570 [00:13<00:22, 291.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 38%|███▊      | 4054/10570 [00:13<00:22, 288.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 37%|███▋      | 3950/10570 [00:13<00:23, 284.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 39%|███▊      | 4080/10570 [00:13<00:22, 292.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 39%|███▊      | 4076/10570 [00:13<00:22, 290.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 38%|███▊      | 4002/10570 [00:13<00:22, 287.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 35%|███▌      | 3703/10570 [00:12<00:25, 274.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 38%|███▊      | 4051/10570 [00:13<00:22, 291.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▌      | 3817/10570 [00:13<00:24, 280.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 39%|███▉      | 4127/10570 [00:13<00:22, 287.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 35%|███▌      | 3736/10570 [00:12<00:26, 258.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 36%|███▌      | 3794/10570 [00:12<00:24, 279.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 38%|███▊      | 4044/10570 [00:13<00:22, 289.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 37%|███▋      | 3888/10570 [00:13<00:23, 279.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 38%|███▊      | 4002/10570 [00:13<00:22, 288.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 39%|███▉      | 4112/10570 [00:13<00:22, 284.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 39%|███▊      | 4084/10570 [00:13<00:22, 290.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 39%|███▉      | 4118/10570 [00:13<00:22, 285.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 38%|███▊      | 3979/10570 [00:13<00:23, 285.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 39%|███▉      | 4110/10570 [00:13<00:22, 286.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 38%|███▊      | 4031/10570 [00:13<00:22, 287.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 35%|███▌      | 3731/10570 [00:12<00:24, 274.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 39%|███▉      | 4106/10570 [00:13<00:22, 285.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 39%|███▊      | 4081/10570 [00:13<00:22, 291.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|███▋      | 3846/10570 [00:13<00:23, 281.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 36%|███▌      | 3764/10570 [00:13<00:25, 264.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 36%|███▌      | 3822/10570 [00:13<00:24, 279.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 39%|███▊      | 4074/10570 [00:13<00:22, 290.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 37%|███▋      | 3916/10570 [00:13<00:23, 279.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 38%|███▊      | 4031/10570 [00:13<00:22, 288.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 39%|███▉      | 4141/10570 [00:13<00:22, 279.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 39%|███▉      | 4114/10570 [00:13<00:22, 283.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 39%|███▉      | 4156/10570 [00:13<00:26, 240.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 38%|███▊      | 4008/10570 [00:13<00:23, 284.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 39%|███▉      | 4139/10570 [00:13<00:22, 280.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 38%|███▊      | 4061/10570 [00:13<00:22, 288.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 36%|███▌      | 3759/10570 [00:13<00:24, 274.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 39%|███▉      | 4147/10570 [00:13<00:24, 258.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 39%|███▉      | 4135/10570 [00:13<00:22, 282.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 3875/10570 [00:13<00:23, 281.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 39%|███▉      | 4111/10570 [00:13<00:22, 285.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 36%|███▌      | 3793/10570 [00:13<00:25, 268.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 36%|███▋      | 3851/10570 [00:13<00:24, 279.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 37%|███▋      | 3945/10570 [00:13<00:23, 279.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 39%|███▉      | 4104/10570 [00:13<00:22, 285.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 38%|███▊      | 4061/10570 [00:13<00:22, 289.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 40%|███▉      | 4182/10570 [00:13<00:26, 242.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 38%|███▊      | 4037/10570 [00:13<00:23, 282.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 39%|███▉      | 4143/10570 [00:13<00:23, 274.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 39%|███▊      | 4090/10570 [00:13<00:22, 288.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 36%|███▌      | 3787/10570 [00:13<00:24, 275.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 3904/10570 [00:13<00:23, 281.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 39%|███▉      | 4140/10570 [00:13<00:22, 282.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 36%|███▌      | 3821/10570 [00:13<00:24, 270.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 39%|███▉      | 4170/10570 [00:14<00:27, 233.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 39%|███▉      | 4174/10570 [00:14<00:26, 240.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 37%|███▋      | 3880/10570 [00:13<00:23, 279.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 38%|███▊      | 3974/10570 [00:13<00:23, 280.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 39%|███▊      | 4090/10570 [00:13<00:22, 288.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 39%|███▉      | 4133/10570 [00:13<00:22, 281.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 39%|███▉      | 4164/10570 [00:13<00:27, 235.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 38%|███▊      | 4067/10570 [00:13<00:22, 284.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 39%|███▉      | 4168/10570 [00:14<00:28, 223.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 36%|███▌      | 3815/10570 [00:13<00:24, 274.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 39%|███▉      | 4119/10570 [00:13<00:22, 281.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 3933/10570 [00:13<00:23, 281.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 36%|███▋      | 3850/10570 [00:13<00:24, 273.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 40%|███▉      | 4208/10570 [00:13<00:29, 214.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 37%|███▋      | 3908/10570 [00:13<00:23, 279.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 39%|███▉      | 4171/10570 [00:13<00:27, 233.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 38%|███▊      | 4003/10570 [00:13<00:23, 277.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 39%|███▉      | 4119/10570 [00:13<00:22, 282.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 40%|███▉      | 4195/10570 [00:14<00:29, 219.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 40%|███▉      | 4199/10570 [00:14<00:29, 216.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 39%|███▉      | 4169/10570 [00:13<00:27, 236.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 39%|███▉      | 4096/10570 [00:13<00:22, 283.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 40%|███▉      | 4189/10570 [00:14<00:28, 223.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 36%|███▋      | 3843/10570 [00:13<00:24, 275.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 39%|███▉      | 4162/10570 [00:14<00:27, 235.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|███▋      | 3962/10570 [00:13<00:24, 270.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 37%|███▋      | 3878/10570 [00:13<00:24, 275.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 40%|███▉      | 4193/10570 [00:14<00:31, 204.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 40%|████      | 4237/10570 [00:14<00:27, 233.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 37%|███▋      | 3936/10570 [00:13<00:23, 279.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 39%|███▉      | 4148/10570 [00:14<00:25, 249.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 38%|███▊      | 4031/10570 [00:13<00:23, 277.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 40%|███▉      | 4218/10570 [00:14<00:29, 213.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 40%|███▉      | 4222/10570 [00:14<00:28, 219.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 40%|███▉      | 4196/10570 [00:14<00:29, 217.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 39%|███▉      | 4125/10570 [00:13<00:23, 278.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 39%|███▉      | 4148/10570 [00:13<00:25, 250.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 37%|███▋      | 3871/10570 [00:13<00:24, 269.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 40%|███▉      | 4194/10570 [00:14<00:28, 221.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 40%|███▉      | 4213/10570 [00:14<00:29, 213.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 37%|███▋      | 3906/10570 [00:13<00:24, 276.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 3991/10570 [00:13<00:23, 274.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 38%|███▊      | 3965/10570 [00:13<00:23, 280.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 40%|███▉      | 4187/10570 [00:14<00:28, 227.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 40%|████      | 4262/10570 [00:14<00:27, 226.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 38%|███▊      | 4060/10570 [00:13<00:23, 278.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 40%|███▉      | 4215/10570 [00:14<00:32, 196.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 39%|███▉      | 4174/10570 [00:14<00:26, 237.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 40%|████      | 4247/10570 [00:14<00:27, 232.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 40%|████      | 4251/10570 [00:14<00:26, 237.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 40%|███▉      | 4219/10570 [00:14<00:29, 213.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 37%|███▋      | 3899/10570 [00:13<00:24, 271.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 40%|████      | 4242/10570 [00:14<00:27, 232.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 39%|███▉      | 4174/10570 [00:14<00:26, 237.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 4019/10570 [00:13<00:23, 275.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 37%|███▋      | 3934/10570 [00:13<00:24, 276.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 40%|███▉      | 4218/10570 [00:14<00:29, 215.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 38%|███▊      | 3994/10570 [00:13<00:23, 280.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 39%|███▊      | 4089/10570 [00:13<00:23, 279.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 40%|████      | 4244/10570 [00:14<00:28, 218.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 39%|███▉      | 4153/10570 [00:14<00:27, 232.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 40%|███▉      | 4211/10570 [00:14<00:30, 211.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 40%|████      | 4248/10570 [00:14<00:27, 232.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 37%|███▋      | 3927/10570 [00:13<00:24, 272.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 40%|███▉      | 4199/10570 [00:14<00:29, 214.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 37%|███▋      | 3962/10570 [00:13<00:23, 276.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|███▊      | 4048/10570 [00:13<00:23, 278.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 40%|████      | 4247/10570 [00:14<00:26, 234.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 38%|███▊      | 4023/10570 [00:13<00:23, 279.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 40%|████      | 4271/10570 [00:14<00:32, 193.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 41%|████      | 4286/10570 [00:14<00:34, 179.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 40%|███▉      | 4179/10570 [00:14<00:26, 238.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 39%|███▉      | 4117/10570 [00:14<00:23, 272.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 40%|███▉      | 4199/10570 [00:14<00:29, 213.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 40%|████      | 4276/10570 [00:14<00:33, 189.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 40%|████      | 4240/10570 [00:14<00:27, 230.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 40%|████      | 4267/10570 [00:14<00:30, 206.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 37%|███▋      | 3955/10570 [00:13<00:24, 273.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 40%|████      | 4268/10570 [00:14<00:32, 195.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 40%|███▉      | 4222/10570 [00:14<00:29, 216.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▊      | 4077/10570 [00:14<00:23, 280.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 38%|███▊      | 3991/10570 [00:13<00:23, 277.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 38%|███▊      | 4052/10570 [00:13<00:23, 280.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 40%|████      | 4272/10570 [00:14<00:32, 192.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 40%|███▉      | 4222/10570 [00:14<00:29, 216.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 39%|███▉      | 4145/10570 [00:14<00:25, 255.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 41%|████      | 4306/10570 [00:14<00:36, 170.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 41%|████      | 4292/10570 [00:14<00:35, 175.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 40%|████      | 4264/10570 [00:14<00:29, 215.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 41%|████      | 4297/10570 [00:14<00:35, 178.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 38%|███▊      | 3983/10570 [00:13<00:24, 274.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 40%|███▉      | 4204/10570 [00:14<00:29, 212.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 40%|████      | 4272/10570 [00:14<00:32, 194.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 40%|████      | 4251/10570 [00:14<00:26, 234.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 38%|███▊      | 4019/10570 [00:14<00:23, 276.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 4106/10570 [00:14<00:23, 276.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 41%|████      | 4289/10570 [00:14<00:35, 178.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 39%|███▊      | 4081/10570 [00:13<00:23, 281.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 41%|████      | 4289/10570 [00:14<00:36, 173.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 40%|████      | 4251/10570 [00:14<00:27, 233.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 41%|████      | 4335/10570 [00:14<00:31, 198.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 40%|████      | 4228/10570 [00:14<00:29, 218.02it/s][1,mpirank:12,algo-2]<stderr>:#015 38%|███▊      | 4011/10570 [00:13<00:23, 273.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 41%|████      | 4311/10570 [00:14<00:36, 170.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 41%|████      | 4293/10570 [00:14<00:35, 174.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 41%|████      | 4317/10570 [00:14<00:35, 175.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 39%|███▉      | 4171/10570 [00:14<00:28, 226.61it/s][1,mpirank:8,algo-2]<stderr>:#015 38%|███▊      | 4048/10570 [00:14<00:23, 277.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 4134/10570 [00:14<00:23, 273.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 39%|███▉      | 4110/10570 [00:14<00:23, 275.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 41%|████      | 4293/10570 [00:14<00:35, 175.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 41%|████      | 4309/10570 [00:14<00:36, 170.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 41%|████▏     | 4365/10570 [00:14<00:27, 221.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 41%|████      | 4287/10570 [00:14<00:35, 176.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 41%|████      | 4308/10570 [00:15<00:37, 166.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 38%|███▊      | 4039/10570 [00:14<00:23, 274.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 40%|████      | 4256/10570 [00:14<00:27, 232.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 40%|████      | 4276/10570 [00:14<00:33, 188.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 41%|████      | 4340/10570 [00:14<00:31, 198.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 41%|████      | 4347/10570 [00:14<00:30, 204.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 39%|███▊      | 4076/10570 [00:14<00:23, 277.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 41%|████      | 4312/10570 [00:14<00:36, 171.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 39%|███▉      | 4138/10570 [00:14<00:23, 272.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 40%|████      | 4275/10570 [00:14<00:33, 188.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 40%|███▉      | 4195/10570 [00:14<00:30, 211.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 41%|████      | 4338/10570 [00:14<00:31, 198.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 41%|████      | 4312/10570 [00:14<00:36, 172.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 42%|████▏     | 4395/10570 [00:14<00:25, 240.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 41%|████      | 4337/10570 [00:15<00:31, 194.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|███▉      | 4162/10570 [00:14<00:28, 228.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 38%|███▊      | 4067/10570 [00:14<00:23, 275.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 41%|████▏     | 4369/10570 [00:15<00:28, 220.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 41%|████▏     | 4376/10570 [00:15<00:27, 225.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 41%|████      | 4307/10570 [00:14<00:37, 167.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 39%|███▉      | 4104/10570 [00:14<00:23, 273.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 41%|████      | 4341/10570 [00:14<00:31, 199.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 41%|████      | 4297/10570 [00:14<00:35, 176.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 41%|████▏     | 4367/10570 [00:15<00:28, 220.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 41%|████      | 4341/10570 [00:14<00:31, 200.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 40%|███▉      | 4217/10570 [00:14<00:30, 205.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 42%|████▏     | 4425/10570 [00:14<00:24, 255.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 41%|████▏     | 4366/10570 [00:15<00:28, 218.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 41%|████      | 4296/10570 [00:14<00:35, 175.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 39%|███▊      | 4095/10570 [00:14<00:23, 274.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 40%|████      | 4280/10570 [00:14<00:35, 179.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 42%|████▏     | 4398/10570 [00:15<00:25, 238.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 39%|███▉      | 4166/10570 [00:14<00:28, 227.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 42%|████▏     | 4405/10570 [00:15<00:25, 241.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|███▉      | 4187/10570 [00:14<00:28, 220.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 41%|████      | 4336/10570 [00:15<00:31, 194.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 41%|████▏     | 4370/10570 [00:14<00:27, 222.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 39%|███▉      | 4132/10570 [00:14<00:24, 265.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 41%|████      | 4316/10570 [00:15<00:36, 172.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 42%|████▏     | 4396/10570 [00:15<00:25, 238.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 41%|████▏     | 4370/10570 [00:14<00:27, 223.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 40%|████      | 4245/10570 [00:14<00:28, 224.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 42%|████▏     | 4455/10570 [00:15<00:22, 266.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 42%|████▏     | 4396/10570 [00:15<00:25, 238.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 39%|███▉      | 4123/10570 [00:14<00:23, 270.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 41%|████      | 4315/10570 [00:14<00:36, 172.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 42%|████▏     | 4427/10570 [00:15<00:24, 250.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 42%|████▏     | 4435/10570 [00:15<00:23, 256.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 41%|████▏     | 4365/10570 [00:15<00:28, 217.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 42%|████▏     | 4399/10570 [00:15<00:25, 239.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 40%|███▉      | 4190/10570 [00:14<00:29, 213.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|███▉      | 4210/10570 [00:14<00:30, 205.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 41%|████      | 4345/10570 [00:15<00:31, 200.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 41%|████      | 4301/10570 [00:14<00:37, 166.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 42%|████▏     | 4425/10570 [00:15<00:24, 251.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 42%|████▏     | 4399/10570 [00:15<00:25, 240.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 42%|████▏     | 4485/10570 [00:15<00:22, 275.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 42%|████▏     | 4426/10570 [00:15<00:24, 253.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 39%|███▉      | 4159/10570 [00:14<00:28, 222.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 41%|████      | 4343/10570 [00:15<00:31, 198.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 42%|████▏     | 4456/10570 [00:15<00:23, 261.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 42%|████▏     | 4464/10570 [00:15<00:23, 265.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 42%|████▏     | 4394/10570 [00:15<00:26, 235.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 40%|████      | 4268/10570 [00:14<00:32, 193.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 42%|████▏     | 4427/10570 [00:15<00:24, 248.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 4238/10570 [00:14<00:28, 222.90it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 41%|████▏     | 4374/10570 [00:15<00:27, 221.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 42%|████▏     | 4454/10570 [00:15<00:23, 262.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 41%|████      | 4324/10570 [00:15<00:34, 179.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 42%|████▏     | 4427/10570 [00:15<00:24, 249.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 40%|███▉      | 4213/10570 [00:14<00:31, 204.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 39%|███▉      | 4151/10570 [00:14<00:28, 227.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 43%|████▎     | 4514/10570 [00:15<00:21, 276.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 42%|████▏     | 4456/10570 [00:15<00:23, 263.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 41%|████▏     | 4372/10570 [00:15<00:28, 220.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 40%|███▉      | 4183/10570 [00:14<00:28, 223.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 42%|████▏     | 4486/10570 [00:15<00:22, 270.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 42%|████▏     | 4423/10570 [00:15<00:24, 250.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 43%|████▎     | 4494/10570 [00:15<00:22, 272.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 42%|████▏     | 4456/10570 [00:15<00:23, 259.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 42%|████▏     | 4402/10570 [00:15<00:26, 236.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 41%|████      | 4353/10570 [00:15<00:30, 205.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 42%|████▏     | 4484/10570 [00:15<00:22, 271.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 42%|████▏     | 4457/10570 [00:15<00:23, 261.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 40%|████      | 4241/10570 [00:14<00:28, 223.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|████      | 4262/10570 [00:14<00:29, 215.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 39%|███▉      | 4175/10570 [00:14<00:27, 229.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 43%|████▎     | 4543/10570 [00:15<00:21, 277.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 41%|████      | 4289/10570 [00:15<00:36, 170.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 42%|████▏     | 4486/10570 [00:15<00:22, 272.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 42%|████▏     | 4401/10570 [00:15<00:25, 237.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 43%|████▎     | 4514/10570 [00:15<00:22, 271.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 42%|████▏     | 4453/10570 [00:15<00:23, 261.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 43%|████▎     | 4522/10570 [00:15<00:22, 272.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 42%|████▏     | 4486/10570 [00:15<00:22, 269.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 42%|████▏     | 4431/10570 [00:15<00:24, 251.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 40%|███▉      | 4207/10570 [00:14<00:31, 200.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 41%|████▏     | 4382/10570 [00:15<00:27, 225.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 43%|████▎     | 4513/10570 [00:15<00:22, 274.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 42%|████▏     | 4487/10570 [00:15<00:22, 270.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 43%|████▎     | 4572/10570 [00:15<00:21, 280.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 43%|████▎     | 4515/10570 [00:15<00:21, 275.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 42%|████▏     | 4430/10570 [00:15<00:24, 251.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 40%|████      | 4265/10570 [00:14<00:30, 203.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 43%|████▎     | 4542/10570 [00:15<00:22, 272.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 43%|████▎     | 4550/10570 [00:15<00:21, 274.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 42%|████▏     | 4483/10570 [00:15<00:22, 270.74it/s][1,mpirank:12,algo-2]<stderr>:#015 40%|███▉      | 4199/10570 [00:14<00:31, 204.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 41%|████      | 4308/10570 [00:15<00:38, 161.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 43%|████▎     | 4514/10570 [00:15<00:22, 271.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 42%|████▏     | 4460/10570 [00:15<00:23, 261.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 40%|████      | 4233/10570 [00:14<00:29, 215.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 42%|████▏     | 4410/10570 [00:15<00:25, 239.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 43%|████▎     | 4541/10570 [00:15<00:21, 274.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 43%|████▎     | 4515/10570 [00:15<00:22, 272.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 4285/10570 [00:15<00:37, 169.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 43%|████▎     | 4543/10570 [00:15<00:21, 276.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 44%|████▎     | 4601/10570 [00:15<00:22, 268.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 42%|████▏     | 4459/10570 [00:15<00:23, 261.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 43%|████▎     | 4570/10570 [00:15<00:21, 273.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 43%|████▎     | 4579/10570 [00:15<00:21, 276.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 43%|████▎     | 4511/10570 [00:15<00:22, 272.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 40%|███▉      | 4221/10570 [00:14<00:30, 206.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 41%|████      | 4336/10570 [00:15<00:33, 188.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 43%|████▎     | 4542/10570 [00:15<00:22, 272.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 42%|████▏     | 4490/10570 [00:15<00:22, 270.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 40%|████      | 4258/10570 [00:15<00:28, 222.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 42%|████▏     | 4439/10570 [00:15<00:24, 252.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 43%|████▎     | 4570/10570 [00:15<00:21, 277.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 43%|████▎     | 4543/10570 [00:15<00:22, 273.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 43%|████▎     | 4572/10570 [00:15<00:21, 279.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 41%|████      | 4287/10570 [00:15<00:36, 170.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 44%|████▍     | 4630/10570 [00:15<00:21, 273.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 4304/10570 [00:15<00:38, 163.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 42%|████▏     | 4489/10570 [00:15<00:22, 270.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 40%|████      | 4249/10570 [00:14<00:28, 224.36it/s][1,mpirank:15,algo-2]<stderr>:#015 41%|████▏     | 4364/10570 [00:15<00:29, 210.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 43%|████▎     | 4570/10570 [00:15<00:21, 274.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 44%|████▎     | 4598/10570 [00:15<00:22, 263.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 43%|████▎     | 4539/10570 [00:15<00:22, 264.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 44%|████▎     | 4607/10570 [00:15<00:22, 265.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 43%|████▎     | 4518/10570 [00:15<00:22, 271.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 42%|████▏     | 4468/10570 [00:15<00:23, 262.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 43%|████▎     | 4572/10570 [00:15<00:21, 275.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 44%|████▎     | 4598/10570 [00:15<00:22, 267.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 4330/10570 [00:15<00:33, 184.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 44%|████▎     | 4601/10570 [00:16<00:22, 266.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 43%|████▎     | 4517/10570 [00:15<00:22, 270.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 44%|████▍     | 4658/10570 [00:15<00:22, 263.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 42%|████▏     | 4392/10570 [00:15<00:27, 228.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 44%|████▍     | 4625/10570 [00:15<00:22, 265.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 41%|████      | 4306/10570 [00:15<00:38, 161.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 43%|████▎     | 4568/10570 [00:15<00:22, 270.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 44%|████▍     | 4636/10570 [00:15<00:21, 271.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 44%|████▎     | 4598/10570 [00:15<00:22, 264.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 43%|████▎     | 4546/10570 [00:15<00:22, 271.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 43%|████▎     | 4497/10570 [00:15<00:22, 268.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 44%|████▍     | 4626/10570 [00:15<00:22, 269.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 41%|████      | 4282/10570 [00:15<00:37, 169.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 44%|████▎     | 4600/10570 [00:15<00:22, 263.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████      | 4358/10570 [00:15<00:29, 207.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 44%|████▍     | 4628/10570 [00:16<00:22, 266.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 44%|████▍     | 4687/10570 [00:15<00:21, 270.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 40%|████      | 4273/10570 [00:15<00:34, 181.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 43%|████▎     | 4545/10570 [00:15<00:22, 268.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 42%|████▏     | 4420/10570 [00:15<00:25, 242.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 41%|████      | 4334/10570 [00:15<00:33, 187.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 44%|████▍     | 4652/10570 [00:16<00:23, 257.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 43%|████▎     | 4596/10570 [00:16<00:22, 267.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 44%|████▍     | 4626/10570 [00:15<00:22, 267.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 44%|████▍     | 4664/10570 [00:16<00:22, 260.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 43%|████▎     | 4575/10570 [00:15<00:21, 274.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 43%|████▎     | 4525/10570 [00:15<00:22, 268.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 44%|████▍     | 4629/10570 [00:15<00:21, 270.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 44%|████▍     | 4654/10570 [00:16<00:22, 260.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|████▏     | 4386/10570 [00:15<00:27, 226.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 45%|████▍     | 4717/10570 [00:16<00:21, 277.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 43%|████▎     | 4574/10570 [00:15<00:21, 273.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 41%|████      | 4302/10570 [00:15<00:38, 161.56it/s][1,mpirank:10,algo-2]<stderr>:#015 44%|████▍     | 4655/10570 [00:16<00:22, 258.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 42%|████▏     | 4448/10570 [00:15<00:24, 252.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 41%|████▏     | 4362/10570 [00:15<00:29, 209.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 44%|████▍     | 4681/10570 [00:16<00:22, 264.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 44%|████▎     | 4623/10570 [00:16<00:22, 263.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 44%|████▍     | 4693/10570 [00:16<00:21, 267.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 44%|████▍     | 4653/10570 [00:16<00:22, 258.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 41%|████      | 4293/10570 [00:15<00:37, 165.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 43%|████▎     | 4553/10570 [00:15<00:22, 269.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 44%|████▎     | 4603/10570 [00:16<00:22, 262.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 44%|████▍     | 4682/10570 [00:16<00:22, 265.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 44%|████▍     | 4657/10570 [00:15<00:22, 260.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 4414/10570 [00:15<00:25, 240.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 45%|████▍     | 4746/10570 [00:16<00:20, 280.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 41%|████      | 4323/10570 [00:15<00:36, 171.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 44%|████▍     | 4684/10570 [00:16<00:22, 265.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 42%|████▏     | 4477/10570 [00:15<00:23, 261.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 42%|████▏     | 4390/10570 [00:15<00:27, 227.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 44%|████▎     | 4602/10570 [00:15<00:22, 261.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 45%|████▍     | 4711/10570 [00:16<00:21, 272.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 44%|████▍     | 4681/10570 [00:16<00:22, 263.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 45%|████▍     | 4723/10570 [00:16<00:21, 274.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 44%|████▍     | 4650/10570 [00:16<00:23, 255.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 43%|████▎     | 4581/10570 [00:15<00:22, 271.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 44%|████▍     | 4632/10570 [00:16<00:22, 267.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 41%|████      | 4311/10570 [00:15<00:38, 161.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 45%|████▍     | 4712/10570 [00:16<00:21, 273.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 44%|████▍     | 4686/10570 [00:16<00:22, 267.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 4443/10570 [00:15<00:24, 252.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 41%|████      | 4351/10570 [00:15<00:31, 198.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 45%|████▍     | 4712/10570 [00:16<00:21, 268.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 43%|████▎     | 4505/10570 [00:15<00:22, 266.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 42%|████▏     | 4418/10570 [00:15<00:25, 241.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 45%|████▌     | 4775/10570 [00:16<00:21, 267.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 44%|████▍     | 4631/10570 [00:16<00:22, 268.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 45%|████▍     | 4740/10570 [00:16<00:21, 276.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 45%|████▍     | 4752/10570 [00:16<00:20, 277.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 45%|████▍     | 4711/10570 [00:16<00:21, 272.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 44%|████▍     | 4678/10570 [00:16<00:22, 261.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 44%|████▎     | 4609/10570 [00:16<00:22, 260.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 41%|████      | 4339/10570 [00:15<00:33, 188.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 44%|████▍     | 4659/10570 [00:16<00:22, 257.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 45%|████▍     | 4741/10570 [00:16<00:21, 277.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 45%|████▍     | 4716/10570 [00:16<00:21, 275.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|████▏     | 4472/10570 [00:15<00:23, 262.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 45%|████▍     | 4741/10570 [00:16<00:21, 274.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 41%|████▏     | 4379/10570 [00:15<00:28, 217.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 43%|████▎     | 4533/10570 [00:16<00:22, 265.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 45%|████▌     | 4804/10570 [00:16<00:21, 273.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 42%|████▏     | 4446/10570 [00:15<00:24, 250.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 44%|████▍     | 4658/10570 [00:16<00:22, 257.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 45%|████▍     | 4740/10570 [00:16<00:21, 276.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 45%|████▍     | 4708/10570 [00:16<00:21, 271.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 45%|████▌     | 4768/10570 [00:16<00:21, 264.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 45%|████▌     | 4780/10570 [00:16<00:21, 264.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 44%|████▍     | 4638/10570 [00:16<00:22, 266.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 41%|████▏     | 4367/10570 [00:15<00:29, 210.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 44%|████▍     | 4688/10570 [00:16<00:22, 264.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 45%|████▍     | 4745/10570 [00:16<00:20, 278.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 4501/10570 [00:15<00:22, 267.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 45%|████▌     | 4769/10570 [00:16<00:21, 264.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 42%|████▏     | 4407/10570 [00:15<00:26, 233.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 43%|████▎     | 4560/10570 [00:16<00:22, 264.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 46%|████▌     | 4834/10570 [00:16<00:20, 279.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 42%|████▏     | 4473/10570 [00:15<00:23, 254.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 45%|████▌     | 4769/10570 [00:16<00:22, 263.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 44%|████▍     | 4686/10570 [00:16<00:22, 262.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 45%|████▍     | 4737/10570 [00:16<00:21, 275.48it/s][1,mpirank:9,algo-2]<stderr>:#015 45%|████▌     | 4796/10570 [00:16<00:21, 268.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 45%|████▌     | 4768/10570 [00:16<00:21, 264.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 45%|████▌     | 4809/10570 [00:16<00:21, 271.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 42%|████▏     | 4395/10570 [00:15<00:27, 227.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 45%|████▍     | 4717/10570 [00:16<00:21, 271.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 44%|████▍     | 4665/10570 [00:16<00:23, 255.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 45%|████▌     | 4797/10570 [00:16<00:21, 268.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 4529/10570 [00:16<00:22, 266.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 45%|████▌     | 4773/10570 [00:16<00:21, 265.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 42%|████▏     | 4436/10570 [00:15<00:24, 246.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 43%|████▎     | 4588/10570 [00:16<00:22, 266.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 43%|████▎     | 4501/10570 [00:15<00:23, 260.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 45%|████▌     | 4798/10570 [00:16<00:21, 268.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 45%|████▍     | 4715/10570 [00:16<00:21, 269.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 46%|████▌     | 4826/10570 [00:16<00:20, 275.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 46%|████▌     | 4863/10570 [00:16<00:21, 260.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 45%|████▌     | 4796/10570 [00:16<00:21, 268.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 46%|████▌     | 4839/10570 [00:16<00:20, 276.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 45%|████▌     | 4765/10570 [00:16<00:22, 263.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 42%|████▏     | 4423/10570 [00:15<00:25, 240.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 45%|████▍     | 4745/10570 [00:16<00:21, 273.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 44%|████▍     | 4694/10570 [00:16<00:22, 263.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 46%|████▌     | 4827/10570 [00:16<00:20, 275.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 4557/10570 [00:16<00:22, 269.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 45%|████▌     | 4802/10570 [00:16<00:21, 270.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 42%|████▏     | 4465/10570 [00:16<00:23, 256.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 43%|████▎     | 4528/10570 [00:15<00:23, 260.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 46%|████▌     | 4827/10570 [00:16<00:21, 272.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 44%|████▎     | 4615/10570 [00:16<00:23, 255.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 45%|████▍     | 4744/10570 [00:16<00:21, 274.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 46%|████▋     | 4892/10570 [00:16<00:21, 268.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 46%|████▌     | 4826/10570 [00:16<00:20, 275.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 45%|████▌     | 4793/10570 [00:16<00:21, 266.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 46%|████▌     | 4854/10570 [00:16<00:22, 256.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 42%|████▏     | 4450/10570 [00:15<00:24, 248.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 46%|████▌     | 4867/10570 [00:16<00:22, 257.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 45%|████▍     | 4723/10570 [00:16<00:21, 269.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 45%|████▌     | 4773/10570 [00:16<00:22, 257.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|████▎     | 4585/10570 [00:16<00:22, 270.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 46%|████▌     | 4832/10570 [00:16<00:20, 277.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 42%|████▏     | 4492/10570 [00:16<00:23, 255.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 43%|████▎     | 4556/10570 [00:16<00:22, 264.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 46%|████▌     | 4855/10570 [00:16<00:22, 255.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 44%|████▍     | 4642/10570 [00:16<00:23, 256.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 47%|████▋     | 4922/10570 [00:16<00:20, 275.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 46%|████▌     | 4855/10570 [00:17<00:22, 254.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 45%|████▌     | 4772/10570 [00:16<00:22, 261.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 46%|████▌     | 4823/10570 [00:16<00:20, 274.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 46%|████▌     | 4883/10570 [00:16<00:21, 265.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 42%|████▏     | 4478/10570 [00:16<00:23, 256.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 46%|████▌     | 4854/10570 [00:16<00:22, 256.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 46%|████▋     | 4896/10570 [00:16<00:21, 265.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 45%|████▍     | 4752/10570 [00:16<00:21, 272.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 45%|████▌     | 4802/10570 [00:16<00:21, 264.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 43%|████▎     | 4519/10570 [00:16<00:23, 257.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▎     | 4613/10570 [00:16<00:23, 257.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 43%|████▎     | 4583/10570 [00:16<00:22, 263.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 46%|████▌     | 4885/10570 [00:16<00:21, 265.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 46%|████▌     | 4860/10570 [00:16<00:22, 258.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 44%|████▍     | 4668/10570 [00:16<00:23, 251.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 47%|████▋     | 4951/10570 [00:16<00:20, 279.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 46%|████▌     | 4885/10570 [00:17<00:21, 265.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 45%|████▌     | 4801/10570 [00:16<00:21, 267.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 46%|████▌     | 4851/10570 [00:16<00:21, 271.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 46%|████▋     | 4912/10570 [00:17<00:20, 271.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 43%|████▎     | 4506/10570 [00:16<00:23, 260.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 46%|████▌     | 4883/10570 [00:16<00:21, 265.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 47%|████▋     | 4925/10570 [00:17<00:20, 271.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 46%|████▌     | 4832/10570 [00:16<00:21, 272.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 45%|████▌     | 4780/10570 [00:16<00:22, 259.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 43%|████▎     | 4546/10570 [00:16<00:23, 259.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 4641/10570 [00:16<00:22, 261.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 46%|████▋     | 4914/10570 [00:17<00:20, 272.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 46%|████▋     | 4889/10570 [00:16<00:21, 266.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 44%|████▍     | 4696/10570 [00:16<00:22, 259.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 44%|████▎     | 4610/10570 [00:16<00:23, 253.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 47%|████▋     | 4980/10570 [00:16<00:19, 282.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 46%|████▌     | 4830/10570 [00:16<00:20, 273.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 46%|████▋     | 4915/10570 [00:17<00:20, 272.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 47%|████▋     | 4941/10570 [00:17<00:20, 276.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 43%|████▎     | 4533/10570 [00:16<00:23, 260.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 46%|████▋     | 4912/10570 [00:16<00:20, 271.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 46%|████▌     | 4879/10570 [00:17<00:21, 259.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 47%|████▋     | 4954/10570 [00:17<00:20, 275.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 45%|████▌     | 4809/10570 [00:16<00:21, 266.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 43%|████▎     | 4574/10570 [00:16<00:22, 263.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 47%|████▋     | 4943/10570 [00:17<00:20, 277.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 46%|████▌     | 4860/10570 [00:17<00:22, 253.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 47%|████▋     | 4919/10570 [00:16<00:20, 273.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 4668/10570 [00:16<00:23, 252.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 45%|████▍     | 4724/10570 [00:16<00:22, 263.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 44%|████▍     | 4637/10570 [00:16<00:23, 256.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 47%|████▋     | 5009/10570 [00:17<00:19, 282.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 47%|████▋     | 4945/10570 [00:17<00:20, 278.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 47%|████▋     | 4970/10570 [00:17<00:20, 277.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 46%|████▌     | 4858/10570 [00:16<00:22, 254.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 47%|████▋     | 4941/10570 [00:17<00:20, 276.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 46%|████▋     | 4908/10570 [00:17<00:21, 268.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 47%|████▋     | 4983/10570 [00:17<00:20, 278.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 43%|████▎     | 4560/10570 [00:16<00:23, 258.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 46%|████▌     | 4838/10570 [00:16<00:21, 272.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 47%|████▋     | 4971/10570 [00:17<00:20, 277.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 47%|████▋     | 4948/10570 [00:17<00:20, 278.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 46%|████▋     | 4889/10570 [00:17<00:21, 262.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 44%|████▎     | 4601/10570 [00:16<00:23, 252.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|████▍     | 4697/10570 [00:16<00:22, 261.15it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 45%|████▍     | 4752/10570 [00:16<00:21, 267.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 48%|████▊     | 5039/10570 [00:17<00:19, 284.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 44%|████▍     | 4663/10570 [00:16<00:23, 246.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 47%|████▋     | 4974/10570 [00:17<00:19, 280.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 47%|████▋     | 4999/10570 [00:17<00:20, 278.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 46%|████▌     | 4887/10570 [00:17<00:21, 263.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 47%|████▋     | 4937/10570 [00:17<00:20, 273.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 47%|████▋     | 4970/10570 [00:17<00:20, 277.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 47%|████▋     | 5011/10570 [00:17<00:19, 278.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 43%|████▎     | 4587/10570 [00:16<00:22, 261.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 47%|████▋     | 4999/10570 [00:17<00:20, 278.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 47%|████▋     | 4918/10570 [00:17<00:20, 269.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 47%|████▋     | 4977/10570 [00:17<00:20, 279.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 46%|████▌     | 4866/10570 [00:17<00:22, 253.61it/s][1,mpirank:8,algo-2]<stderr>:#015 44%|████▍     | 4629/10570 [00:16<00:23, 258.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▍     | 4726/10570 [00:16<00:21, 266.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 48%|████▊     | 5069/10570 [00:17<00:19, 287.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 44%|████▍     | 4691/10570 [00:16<00:23, 254.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 47%|████▋     | 5003/10570 [00:17<00:19, 279.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 45%|████▌     | 4779/10570 [00:16<00:22, 253.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 48%|████▊     | 5028/10570 [00:17<00:19, 280.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 47%|████▋     | 4916/10570 [00:17<00:20, 270.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 47%|████▋     | 4998/10570 [00:17<00:20, 278.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 47%|████▋     | 4966/10570 [00:17<00:20, 276.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 48%|████▊     | 5040/10570 [00:17<00:19, 280.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 44%|████▎     | 4614/10570 [00:16<00:23, 249.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 48%|████▊     | 5028/10570 [00:17<00:19, 280.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 47%|████▋     | 4947/10570 [00:17<00:20, 274.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 47%|████▋     | 5006/10570 [00:17<00:19, 279.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 46%|████▋     | 4895/10570 [00:17<00:21, 261.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▍     | 4754/10570 [00:16<00:21, 269.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 48%|████▊     | 5098/10570 [00:17<00:19, 284.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 44%|████▍     | 4655/10570 [00:16<00:23, 247.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 48%|████▊     | 5032/10570 [00:17<00:19, 282.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 45%|████▍     | 4720/10570 [00:16<00:22, 262.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 45%|████▌     | 4807/10570 [00:17<00:22, 260.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 48%|████▊     | 5057/10570 [00:17<00:19, 282.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 47%|████▋     | 4945/10570 [00:17<00:20, 276.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 48%|████▊     | 5027/10570 [00:17<00:19, 279.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 47%|████▋     | 4995/10570 [00:17<00:20, 277.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 48%|████▊     | 5070/10570 [00:17<00:19, 283.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 44%|████▍     | 4641/10570 [00:16<00:23, 253.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 48%|████▊     | 5057/10570 [00:17<00:19, 283.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 47%|████▋     | 4975/10570 [00:17<00:20, 275.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 48%|████▊     | 5035/10570 [00:17<00:19, 279.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 47%|████▋     | 4924/10570 [00:17<00:21, 267.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 49%|████▊     | 5128/10570 [00:17<00:18, 287.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 44%|████▍     | 4682/10570 [00:16<00:23, 253.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 45%|████▍     | 4748/10570 [00:16<00:21, 266.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|████▌     | 4781/10570 [00:17<00:22, 255.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 48%|████▊     | 5062/10570 [00:17<00:19, 285.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 46%|████▌     | 4836/10570 [00:17<00:21, 266.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 48%|████▊     | 5086/10570 [00:17<00:19, 282.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 47%|████▋     | 4973/10570 [00:17<00:20, 276.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 48%|████▊     | 5056/10570 [00:17<00:19, 282.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 48%|████▊     | 5024/10570 [00:17<00:19, 279.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 48%|████▊     | 5099/10570 [00:17<00:19, 280.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 44%|████▍     | 4667/10570 [00:16<00:24, 245.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 48%|████▊     | 5086/10570 [00:17<00:19, 283.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 47%|████▋     | 5003/10570 [00:17<00:20, 275.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 48%|████▊     | 5065/10570 [00:17<00:19, 283.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 47%|████▋     | 4953/10570 [00:17<00:20, 272.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 45%|████▍     | 4710/10570 [00:16<00:22, 260.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 49%|████▉     | 5158/10570 [00:17<00:18, 289.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 4810/10570 [00:17<00:21, 263.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 48%|████▊     | 5091/10570 [00:17<00:19, 284.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 48%|████▊     | 5115/10570 [00:17<00:19, 284.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 45%|████▌     | 4775/10570 [00:16<00:22, 253.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 47%|████▋     | 5001/10570 [00:17<00:20, 274.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 48%|████▊     | 5085/10570 [00:17<00:19, 282.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 48%|████▊     | 5053/10570 [00:17<00:19, 281.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 46%|████▌     | 4863/10570 [00:17<00:23, 247.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 49%|████▊     | 5129/10570 [00:17<00:19, 283.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 44%|████▍     | 4695/10570 [00:16<00:23, 253.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 48%|████▊     | 5115/10570 [00:17<00:19, 282.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 48%|████▊     | 5032/10570 [00:17<00:19, 277.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 48%|████▊     | 5094/10570 [00:17<00:19, 283.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 47%|████▋     | 4982/10570 [00:17<00:20, 274.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 45%|████▍     | 4738/10570 [00:17<00:21, 265.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 49%|████▉     | 5188/10570 [00:17<00:18, 289.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 48%|████▊     | 5121/10570 [00:17<00:18, 287.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 4839/10570 [00:17<00:21, 268.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 49%|████▊     | 5144/10570 [00:17<00:19, 284.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 45%|████▌     | 4803/10570 [00:17<00:22, 259.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 48%|████▊     | 5030/10570 [00:17<00:19, 277.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 48%|████▊     | 5114/10570 [00:17<00:19, 284.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 48%|████▊     | 5082/10570 [00:17<00:19, 282.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 46%|████▋     | 4891/10570 [00:17<00:22, 254.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 49%|████▉     | 5158/10570 [00:17<00:18, 285.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 45%|████▍     | 4723/10570 [00:17<00:22, 259.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 49%|████▊     | 5144/10570 [00:17<00:19, 284.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 48%|████▊     | 5061/10570 [00:17<00:19, 280.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 48%|████▊     | 5123/10570 [00:17<00:19, 282.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 47%|████▋     | 5010/10570 [00:17<00:20, 273.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 49%|████▉     | 5218/10570 [00:17<00:18, 290.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 49%|████▊     | 5150/10570 [00:18<00:18, 286.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 45%|████▌     | 4765/10570 [00:17<00:22, 254.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 49%|████▉     | 5173/10570 [00:17<00:18, 285.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 46%|████▌     | 4832/10570 [00:17<00:21, 265.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 48%|████▊     | 5059/10570 [00:17<00:19, 280.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 49%|████▊     | 5143/10570 [00:17<00:19, 285.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▌     | 4866/10570 [00:17<00:22, 250.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 48%|████▊     | 5111/10570 [00:17<00:19, 282.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 47%|████▋     | 4919/10570 [00:17<00:21, 261.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 49%|████▉     | 5187/10570 [00:17<00:18, 286.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 45%|████▍     | 4750/10570 [00:17<00:22, 262.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 49%|████▉     | 5174/10570 [00:17<00:18, 286.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 48%|████▊     | 5090/10570 [00:17<00:19, 278.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 49%|████▊     | 5152/10570 [00:17<00:19, 284.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 48%|████▊     | 5039/10570 [00:17<00:20, 276.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 50%|████▉     | 5248/10570 [00:17<00:18, 291.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 49%|████▉     | 5180/10570 [00:18<00:18, 287.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 45%|████▌     | 4792/10570 [00:17<00:22, 257.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 49%|████▉     | 5202/10570 [00:18<00:18, 285.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 49%|████▉     | 5172/10570 [00:17<00:18, 286.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 48%|████▊     | 5088/10570 [00:17<00:19, 279.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|████▋     | 4894/10570 [00:17<00:21, 258.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 47%|████▋     | 4947/10570 [00:17<00:21, 266.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 49%|████▊     | 5141/10570 [00:17<00:19, 284.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 49%|████▉     | 5216/10570 [00:18<00:18, 286.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 46%|████▌     | 4859/10570 [00:17<00:23, 247.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 49%|████▉     | 5203/10570 [00:18<00:18, 286.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 48%|████▊     | 5118/10570 [00:17<00:19, 278.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 45%|████▌     | 4777/10570 [00:17<00:23, 249.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 49%|████▉     | 5182/10570 [00:17<00:18, 286.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 48%|████▊     | 5068/10570 [00:17<00:19, 279.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 50%|████▉     | 5278/10570 [00:17<00:18, 292.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 49%|████▉     | 5210/10570 [00:18<00:18, 289.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 46%|████▌     | 4821/10570 [00:17<00:21, 263.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 49%|████▉     | 5231/10570 [00:18<00:18, 287.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 49%|████▉     | 5201/10570 [00:18<00:18, 286.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 48%|████▊     | 5118/10570 [00:17<00:19, 283.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 4923/10570 [00:17<00:21, 264.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 47%|████▋     | 4975/10570 [00:17<00:20, 268.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 50%|████▉     | 5245/10570 [00:18<00:18, 287.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 49%|████▉     | 5170/10570 [00:18<00:19, 284.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 46%|████▌     | 4887/10570 [00:17<00:22, 255.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 49%|████▉     | 5232/10570 [00:18<00:18, 286.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 49%|████▊     | 5147/10570 [00:18<00:19, 280.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 45%|████▌     | 4805/10570 [00:17<00:22, 255.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 49%|████▉     | 5212/10570 [00:17<00:18, 287.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 48%|████▊     | 5097/10570 [00:17<00:19, 278.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 50%|█████     | 5308/10570 [00:18<00:17, 294.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 50%|████▉     | 5240/10570 [00:18<00:18, 289.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 46%|████▌     | 4849/10570 [00:17<00:21, 266.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 50%|████▉     | 5260/10570 [00:18<00:18, 285.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 49%|████▉     | 5230/10570 [00:18<00:18, 287.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 49%|████▊     | 5147/10570 [00:17<00:19, 283.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 4951/10570 [00:17<00:20, 268.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 47%|████▋     | 5002/10570 [00:17<00:20, 268.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 50%|████▉     | 5274/10570 [00:18<00:18, 287.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 49%|████▉     | 5199/10570 [00:18<00:18, 284.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 46%|████▋     | 4915/10570 [00:17<00:21, 262.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 50%|████▉     | 5261/10570 [00:18<00:18, 285.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 49%|████▉     | 5176/10570 [00:18<00:19, 281.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 46%|████▌     | 4833/10570 [00:17<00:21, 262.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 50%|████▉     | 5241/10570 [00:18<00:18, 288.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 48%|████▊     | 5126/10570 [00:17<00:19, 281.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 51%|█████     | 5338/10570 [00:18<00:17, 294.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 50%|████▉     | 5269/10570 [00:18<00:18, 289.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 50%|█████     | 5290/10570 [00:18<00:18, 287.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 50%|████▉     | 5259/10570 [00:18<00:18, 285.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 49%|████▉     | 5176/10570 [00:18<00:18, 284.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 4979/10570 [00:17<00:20, 271.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 48%|████▊     | 5030/10570 [00:17<00:20, 270.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 50%|█████     | 5304/10570 [00:18<00:18, 290.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 49%|████▉     | 5229/10570 [00:18<00:18, 286.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 46%|████▌     | 4876/10570 [00:17<00:23, 247.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 47%|████▋     | 4944/10570 [00:17<00:21, 267.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 50%|█████     | 5291/10570 [00:18<00:18, 288.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 49%|████▉     | 5205/10570 [00:18<00:18, 283.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 50%|████▉     | 5270/10570 [00:18<00:18, 288.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 49%|████▉     | 5155/10570 [00:18<00:19, 282.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 51%|█████     | 5368/10570 [00:18<00:17, 293.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 46%|████▌     | 4860/10570 [00:17<00:23, 244.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 50%|█████     | 5299/10570 [00:18<00:18, 291.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 50%|█████     | 5320/10570 [00:18<00:18, 288.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 50%|█████     | 5289/10570 [00:18<00:18, 287.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 49%|████▉     | 5205/10570 [00:18<00:18, 284.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 48%|████▊     | 5058/10570 [00:18<00:20, 272.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|████▋     | 5007/10570 [00:17<00:20, 271.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 50%|█████     | 5334/10570 [00:18<00:18, 290.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 50%|████▉     | 5258/10570 [00:18<00:18, 285.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 46%|████▋     | 4904/10570 [00:17<00:22, 255.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 47%|████▋     | 4972/10570 [00:17<00:20, 268.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 50%|█████     | 5321/10570 [00:18<00:18, 289.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 50%|████▉     | 5234/10570 [00:18<00:18, 284.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 50%|█████     | 5300/10570 [00:18<00:18, 290.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 49%|████▉     | 5184/10570 [00:18<00:19, 282.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 51%|█████     | 5398/10570 [00:18<00:17, 293.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 46%|████▌     | 4888/10570 [00:17<00:22, 253.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 50%|█████     | 5329/10570 [00:18<00:17, 291.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 51%|█████     | 5349/10570 [00:18<00:18, 287.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 50%|█████     | 5318/10570 [00:18<00:18, 288.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 5035/10570 [00:17<00:20, 273.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 50%|████▉     | 5234/10570 [00:18<00:18, 282.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 48%|████▊     | 5086/10570 [00:18<00:20, 272.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 51%|█████     | 5364/10570 [00:18<00:18, 289.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 50%|█████     | 5288/10570 [00:18<00:18, 287.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 47%|████▋     | 4932/10570 [00:17<00:21, 261.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 47%|████▋     | 5000/10570 [00:17<00:20, 269.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 51%|█████     | 5350/10570 [00:18<00:18, 288.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 50%|████▉     | 5263/10570 [00:18<00:18, 284.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 50%|█████     | 5330/10570 [00:18<00:18, 291.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 49%|████▉     | 5213/10570 [00:18<00:18, 283.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 51%|█████▏    | 5428/10570 [00:18<00:17, 292.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 47%|████▋     | 4916/10570 [00:17<00:21, 260.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 51%|█████     | 5359/10570 [00:18<00:17, 291.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 51%|█████     | 5378/10570 [00:18<00:18, 285.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 51%|█████     | 5347/10570 [00:18<00:18, 287.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 5064/10570 [00:18<00:19, 276.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 50%|████▉     | 5263/10570 [00:18<00:18, 282.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 48%|████▊     | 5115/10570 [00:18<00:19, 274.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 50%|█████     | 5317/10570 [00:18<00:18, 288.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 51%|█████     | 5394/10570 [00:18<00:17, 289.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 47%|████▋     | 4960/10570 [00:17<00:21, 265.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 48%|████▊     | 5028/10570 [00:17<00:20, 271.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 51%|█████     | 5379/10570 [00:18<00:17, 288.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 50%|█████     | 5293/10570 [00:18<00:18, 286.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 51%|█████     | 5360/10570 [00:18<00:18, 289.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 50%|████▉     | 5242/10570 [00:18<00:18, 283.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 47%|████▋     | 4944/10570 [00:17<00:21, 265.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 51%|█████     | 5389/10570 [00:18<00:17, 291.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 51%|█████     | 5407/10570 [00:18<00:17, 286.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 52%|█████▏    | 5458/10570 [00:18<00:18, 275.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 51%|█████     | 5377/10570 [00:18<00:18, 288.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 5092/10570 [00:18<00:19, 274.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 50%|█████     | 5293/10570 [00:18<00:18, 285.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 49%|████▊     | 5144/10570 [00:18<00:19, 276.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 51%|█████     | 5346/10570 [00:18<00:18, 287.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 51%|█████▏    | 5423/10570 [00:18<00:17, 288.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 47%|████▋     | 4988/10570 [00:18<00:20, 267.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 48%|████▊     | 5056/10570 [00:18<00:20, 273.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 51%|█████     | 5409/10570 [00:18<00:17, 289.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 50%|█████     | 5322/10570 [00:18<00:18, 287.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 51%|█████     | 5390/10570 [00:18<00:17, 290.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 50%|████▉     | 5271/10570 [00:18<00:18, 283.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 47%|████▋     | 4971/10570 [00:17<00:21, 265.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 51%|█████▏    | 5419/10570 [00:18<00:17, 291.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 51%|█████▏    | 5436/10570 [00:18<00:18, 280.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 51%|█████     | 5407/10570 [00:18<00:17, 289.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 50%|█████     | 5322/10570 [00:18<00:18, 285.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|████▊     | 5121/10570 [00:18<00:19, 277.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 51%|█████     | 5375/10570 [00:18<00:18, 288.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 49%|████▉     | 5173/10570 [00:18<00:19, 277.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 52%|█████▏    | 5486/10570 [00:18<00:19, 263.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 47%|████▋     | 5015/10570 [00:18<00:21, 260.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 48%|████▊     | 5084/10570 [00:18<00:20, 274.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 52%|█████▏    | 5452/10570 [00:18<00:18, 273.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 51%|█████▏    | 5438/10570 [00:18<00:17, 288.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 51%|█████     | 5351/10570 [00:18<00:18, 285.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 50%|█████     | 5300/10570 [00:18<00:18, 285.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 51%|█████▏    | 5420/10570 [00:18<00:17, 286.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 47%|████▋     | 4998/10570 [00:18<00:20, 266.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▊     | 5149/10570 [00:18<00:19, 278.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 51%|█████▏    | 5436/10570 [00:18<00:18, 284.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 52%|█████▏    | 5449/10570 [00:19<00:18, 275.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 49%|████▉     | 5201/10570 [00:18<00:19, 277.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 51%|█████     | 5405/10570 [00:18<00:17, 289.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 51%|█████     | 5351/10570 [00:18<00:18, 279.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 52%|█████▏    | 5515/10570 [00:18<00:18, 269.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 48%|████▊     | 5112/10570 [00:18<00:19, 275.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 48%|████▊     | 5043/10570 [00:18<00:20, 264.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 52%|█████▏    | 5465/10570 [00:18<00:19, 263.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 52%|█████▏    | 5480/10570 [00:18<00:18, 270.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 51%|█████     | 5380/10570 [00:18<00:18, 286.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 52%|█████▏    | 5467/10570 [00:18<00:18, 269.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 50%|█████     | 5329/10570 [00:18<00:18, 285.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 48%|████▊     | 5026/10570 [00:18<00:20, 267.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 52%|█████▏    | 5449/10570 [00:18<00:18, 272.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 5177/10570 [00:18<00:19, 278.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 52%|█████▏    | 5477/10570 [00:19<00:18, 275.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 51%|█████▏    | 5434/10570 [00:19<00:17, 288.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 49%|████▉     | 5230/10570 [00:18<00:19, 278.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 51%|█████     | 5380/10570 [00:18<00:18, 281.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 52%|█████▏    | 5544/10570 [00:18<00:18, 274.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 48%|████▊     | 5071/10570 [00:18<00:20, 268.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 49%|████▊     | 5141/10570 [00:18<00:19, 276.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 52%|█████▏    | 5465/10570 [00:18<00:19, 266.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 52%|█████▏    | 5492/10570 [00:19<00:19, 255.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 52%|█████▏    | 5508/10570 [00:19<00:19, 263.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 51%|█████     | 5409/10570 [00:18<00:17, 286.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 51%|█████     | 5358/10570 [00:18<00:18, 284.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 48%|████▊     | 5054/10570 [00:18<00:20, 269.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 52%|█████▏    | 5495/10570 [00:19<00:19, 260.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 52%|█████▏    | 5477/10570 [00:18<00:18, 272.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|████▉     | 5206/10570 [00:18<00:19, 279.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 50%|████▉     | 5258/10570 [00:18<00:19, 277.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 51%|█████     | 5410/10570 [00:18<00:18, 284.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 53%|█████▎    | 5573/10570 [00:19<00:18, 277.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 52%|█████▏    | 5505/10570 [00:19<00:19, 265.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 49%|████▉     | 5169/10570 [00:18<00:19, 277.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 48%|████▊     | 5099/10570 [00:18<00:20, 269.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 52%|█████▏    | 5463/10570 [00:19<00:18, 269.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 52%|█████▏    | 5520/10570 [00:19<00:19, 262.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 52%|█████▏    | 5492/10570 [00:19<00:19, 258.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 52%|█████▏    | 5537/10570 [00:19<00:18, 268.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 51%|█████▏    | 5438/10570 [00:19<00:17, 286.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 51%|█████     | 5387/10570 [00:18<00:18, 285.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 52%|█████▏    | 5523/10570 [00:19<00:19, 265.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 48%|████▊     | 5082/10570 [00:18<00:20, 270.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 52%|█████▏    | 5505/10570 [00:19<00:19, 262.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|████▉     | 5235/10570 [00:18<00:19, 279.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 50%|█████     | 5287/10570 [00:18<00:18, 279.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 51%|█████▏    | 5439/10570 [00:18<00:17, 285.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 53%|█████▎    | 5603/10570 [00:19<00:17, 281.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 52%|█████▏    | 5534/10570 [00:19<00:18, 270.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 49%|████▉     | 5197/10570 [00:18<00:19, 277.45it/s][1,mpirank:8,algo-2]<stderr>:#015 49%|████▊     | 5127/10570 [00:18<00:19, 272.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 52%|█████▏    | 5549/10570 [00:19<00:18, 267.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 52%|█████▏    | 5520/10570 [00:19<00:19, 263.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 53%|█████▎    | 5566/10570 [00:19<00:18, 272.71it/s][1,mpirank:6,algo-1]<stderr>:#015 52%|█████▏    | 5491/10570 [00:19<00:19, 260.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 51%|█████     | 5416/10570 [00:19<00:18, 285.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 52%|█████▏    | 5467/10570 [00:19<00:19, 266.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 53%|█████▎    | 5552/10570 [00:19<00:18, 270.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 48%|████▊     | 5110/10570 [00:18<00:20, 272.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 52%|█████▏    | 5533/10570 [00:19<00:18, 267.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|████▉     | 5263/10570 [00:18<00:19, 278.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 50%|█████     | 5316/10570 [00:18<00:18, 280.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 53%|█████▎    | 5632/10570 [00:19<00:17, 282.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 49%|████▉     | 5155/10570 [00:18<00:19, 273.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 49%|████▉     | 5226/10570 [00:18<00:19, 278.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 53%|█████▎    | 5562/10570 [00:19<00:18, 266.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 53%|█████▎    | 5578/10570 [00:19<00:18, 271.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 52%|█████▏    | 5468/10570 [00:19<00:19, 265.38it/s][1,mpirank:14,algo-2]<stderr>:#015 52%|█████▏    | 5549/10570 [00:19<00:18, 269.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 52%|█████▏    | 5519/10570 [00:19<00:19, 264.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 53%|█████▎    | 5595/10570 [00:19<00:17, 276.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 52%|█████▏    | 5445/10570 [00:19<00:18, 280.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 49%|████▊     | 5138/10570 [00:18<00:19, 273.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 53%|█████▎    | 5581/10570 [00:19<00:18, 273.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 52%|█████▏    | 5494/10570 [00:19<00:19, 255.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 53%|█████▎    | 5562/10570 [00:19<00:18, 271.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 5291/10570 [00:18<00:18, 278.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 51%|█████     | 5345/10570 [00:19<00:18, 280.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 49%|████▉     | 5183/10570 [00:18<00:19, 274.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 50%|████▉     | 5254/10570 [00:18<00:19, 277.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 54%|█████▎    | 5661/10570 [00:19<00:18, 269.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 53%|█████▎    | 5589/10570 [00:19<00:18, 262.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 53%|█████▎    | 5607/10570 [00:19<00:17, 276.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 53%|█████▎    | 5578/10570 [00:19<00:18, 273.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 53%|█████▎    | 5623/10570 [00:19<00:17, 277.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 52%|█████▏    | 5548/10570 [00:19<00:18, 270.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 52%|█████▏    | 5495/10570 [00:19<00:19, 255.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 53%|█████▎    | 5610/10570 [00:19<00:17, 277.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 49%|████▉     | 5166/10570 [00:18<00:19, 272.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 52%|█████▏    | 5522/10570 [00:19<00:19, 260.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 52%|█████▏    | 5474/10570 [00:19<00:19, 266.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 53%|█████▎    | 5591/10570 [00:19<00:18, 276.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|█████     | 5320/10570 [00:18<00:18, 280.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 51%|█████     | 5374/10570 [00:19<00:18, 279.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 49%|████▉     | 5211/10570 [00:18<00:19, 275.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 50%|████▉     | 5283/10570 [00:18<00:18, 279.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 54%|█████▍    | 5691/10570 [00:19<00:17, 277.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 53%|█████▎    | 5635/10570 [00:19<00:17, 276.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 53%|█████▎    | 5616/10570 [00:19<00:19, 259.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 53%|█████▎    | 5607/10570 [00:19<00:17, 277.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 53%|█████▎    | 5652/10570 [00:19<00:17, 279.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 53%|█████▎    | 5577/10570 [00:19<00:18, 273.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 52%|█████▏    | 5523/10570 [00:19<00:19, 261.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 53%|█████▎    | 5638/10570 [00:19<00:17, 277.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 49%|████▉     | 5194/10570 [00:18<00:19, 273.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 53%|█████▎    | 5551/10570 [00:19<00:18, 265.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 53%|█████▎    | 5620/10570 [00:19<00:17, 277.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 52%|█████▏    | 5501/10570 [00:19<00:19, 256.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 5349/10570 [00:19<00:18, 279.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 51%|█████     | 5402/10570 [00:19<00:18, 279.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 50%|████▉     | 5239/10570 [00:18<00:19, 276.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 50%|█████     | 5312/10570 [00:18<00:18, 280.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 54%|█████▍    | 5721/10570 [00:19<00:17, 282.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 53%|█████▎    | 5635/10570 [00:19<00:17, 277.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 53%|█████▎    | 5642/10570 [00:19<00:19, 256.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 53%|█████▎    | 5606/10570 [00:19<00:17, 277.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 54%|█████▎    | 5663/10570 [00:19<00:18, 264.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 53%|█████▎    | 5552/10570 [00:19<00:18, 267.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 54%|█████▎    | 5680/10570 [00:19<00:18, 268.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 49%|████▉     | 5222/10570 [00:18<00:19, 273.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 54%|█████▎    | 5666/10570 [00:19<00:18, 266.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 53%|█████▎    | 5579/10570 [00:19<00:18, 266.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 53%|█████▎    | 5649/10570 [00:19<00:17, 279.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 52%|█████▏    | 5529/10570 [00:19<00:19, 261.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 5378/10570 [00:19<00:18, 279.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 51%|█████▏    | 5430/10570 [00:19<00:18, 278.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 50%|████▉     | 5267/10570 [00:19<00:19, 275.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 51%|█████     | 5341/10570 [00:19<00:18, 280.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 54%|█████▍    | 5750/10570 [00:19<00:17, 282.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 53%|█████▎    | 5635/10570 [00:19<00:17, 277.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 54%|█████▍    | 5693/10570 [00:19<00:17, 272.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 54%|█████▎    | 5663/10570 [00:19<00:18, 265.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 53%|█████▎    | 5581/10570 [00:19<00:18, 271.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 54%|█████▍    | 5710/10570 [00:19<00:17, 275.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 54%|█████▎    | 5668/10570 [00:19<00:20, 242.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 50%|████▉     | 5250/10570 [00:19<00:19, 272.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 54%|█████▍    | 5696/10570 [00:19<00:17, 273.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 53%|█████▎    | 5608/10570 [00:19<00:18, 272.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 53%|█████▎    | 5557/10570 [00:19<00:18, 266.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████     | 5407/10570 [00:19<00:18, 280.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 54%|█████▎    | 5678/10570 [00:19<00:18, 269.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 50%|█████     | 5296/10570 [00:19<00:18, 277.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 51%|█████     | 5370/10570 [00:19<00:18, 279.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 55%|█████▍    | 5779/10570 [00:19<00:16, 283.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 52%|█████▏    | 5458/10570 [00:19<00:19, 262.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 54%|█████▍    | 5722/10570 [00:19<00:17, 277.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 54%|█████▍    | 5693/10570 [00:19<00:17, 273.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 53%|█████▎    | 5610/10570 [00:19<00:17, 275.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 54%|█████▍    | 5739/10570 [00:19<00:17, 278.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 54%|█████▍    | 5694/10570 [00:20<00:19, 247.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 54%|█████▎    | 5663/10570 [00:19<00:18, 265.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 50%|████▉     | 5278/10570 [00:19<00:19, 274.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 54%|█████▍    | 5725/10570 [00:19<00:17, 277.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 53%|█████▎    | 5636/10570 [00:19<00:18, 273.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 53%|█████▎    | 5586/10570 [00:19<00:18, 270.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|█████▏    | 5436/10570 [00:19<00:18, 280.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 54%|█████▍    | 5708/10570 [00:19<00:17, 275.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 50%|█████     | 5325/10570 [00:19<00:18, 278.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 51%|█████     | 5399/10570 [00:19<00:18, 280.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 55%|█████▍    | 5808/10570 [00:19<00:16, 284.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 54%|█████▍    | 5751/10570 [00:20<00:17, 278.32it/s][1,mpirank:14,algo-2]<stderr>:#015 54%|█████▍    | 5722/10570 [00:19<00:17, 277.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 53%|█████▎    | 5638/10570 [00:19<00:17, 276.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 52%|█████▏    | 5485/10570 [00:19<00:20, 249.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 55%|█████▍    | 5768/10570 [00:20<00:17, 279.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 54%|█████▍    | 5720/10570 [00:20<00:19, 250.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 54%|█████▍    | 5693/10570 [00:19<00:17, 273.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 50%|█████     | 5307/10570 [00:19<00:19, 276.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 54%|█████▍    | 5754/10570 [00:20<00:17, 279.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 53%|█████▎    | 5615/10570 [00:19<00:18, 273.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 54%|█████▎    | 5664/10570 [00:19<00:18, 262.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 54%|█████▍    | 5737/10570 [00:19<00:17, 278.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 51%|█████     | 5353/10570 [00:19<00:18, 277.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 55%|█████▌    | 5838/10570 [00:19<00:16, 286.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 51%|█████▏    | 5428/10570 [00:19<00:18, 273.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 5465/10570 [00:19<00:19, 255.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 54%|█████▍    | 5751/10570 [00:19<00:17, 279.10it/s][1,mpirank:9,algo-2]<stderr>:#015 55%|█████▍    | 5780/10570 [00:20<00:17, 279.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 52%|█████▏    | 5513/10570 [00:19<00:19, 256.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 55%|█████▍    | 5797/10570 [00:20<00:17, 280.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 54%|█████▍    | 5746/10570 [00:20<00:19, 250.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 54%|█████▍    | 5722/10570 [00:20<00:17, 277.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 54%|█████▎    | 5666/10570 [00:19<00:18, 265.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 50%|█████     | 5335/10570 [00:19<00:18, 276.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 55%|█████▍    | 5783/10570 [00:20<00:17, 279.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 53%|█████▎    | 5643/10570 [00:19<00:17, 273.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 54%|█████▍    | 5694/10570 [00:20<00:18, 270.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 55%|█████▍    | 5766/10570 [00:19<00:17, 280.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 51%|█████     | 5381/10570 [00:19<00:18, 278.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 56%|█████▌    | 5867/10570 [00:20<00:16, 286.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 52%|█████▏    | 5541/10570 [00:19<00:19, 261.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 55%|█████▍    | 5780/10570 [00:20<00:17, 279.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 55%|█████▍    | 5809/10570 [00:20<00:17, 279.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 52%|█████▏    | 5456/10570 [00:19<00:19, 259.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 55%|█████▌    | 5826/10570 [00:20<00:16, 281.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 5492/10570 [00:19<00:20, 248.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 55%|█████▍    | 5772/10570 [00:20<00:19, 249.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 54%|█████▍    | 5751/10570 [00:20<00:17, 279.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 54%|█████▍    | 5696/10570 [00:19<00:17, 273.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 51%|█████     | 5363/10570 [00:19<00:18, 275.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 55%|█████▍    | 5812/10570 [00:20<00:16, 279.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 54%|█████▍    | 5723/10570 [00:20<00:17, 275.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 55%|█████▍    | 5795/10570 [00:20<00:17, 280.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 51%|█████     | 5410/10570 [00:19<00:18, 278.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 54%|█████▎    | 5671/10570 [00:19<00:18, 262.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 56%|█████▌    | 5897/10570 [00:20<00:16, 288.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 55%|█████▌    | 5838/10570 [00:20<00:16, 281.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 53%|█████▎    | 5569/10570 [00:19<00:18, 264.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 55%|█████▍    | 5809/10570 [00:20<00:17, 279.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 55%|█████▌    | 5855/10570 [00:20<00:16, 281.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 5520/10570 [00:19<00:19, 255.12it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 55%|█████▍    | 5798/10570 [00:20<00:19, 250.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 55%|█████▍    | 5780/10570 [00:20<00:17, 279.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 54%|█████▍    | 5725/10570 [00:20<00:17, 276.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 52%|█████▏    | 5483/10570 [00:19<00:20, 251.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 51%|█████     | 5391/10570 [00:19<00:18, 275.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 55%|█████▌    | 5841/10570 [00:20<00:16, 282.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 54%|█████▍    | 5752/10570 [00:20<00:17, 276.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 55%|█████▌    | 5824/10570 [00:20<00:16, 283.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 51%|█████▏    | 5438/10570 [00:19<00:18, 278.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 54%|█████▍    | 5700/10570 [00:20<00:18, 270.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 56%|█████▌    | 5926/10570 [00:20<00:16, 287.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 53%|█████▎    | 5597/10570 [00:19<00:18, 268.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 56%|█████▌    | 5867/10570 [00:20<00:16, 281.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 55%|█████▌    | 5838/10570 [00:20<00:16, 282.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 56%|█████▌    | 5884/10570 [00:20<00:16, 283.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 55%|█████▌    | 5824/10570 [00:20<00:18, 252.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|█████▏    | 5546/10570 [00:19<00:19, 251.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 55%|█████▍    | 5809/10570 [00:20<00:16, 280.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 54%|█████▍    | 5754/10570 [00:20<00:17, 278.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 52%|█████▏    | 5510/10570 [00:19<00:19, 255.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 51%|█████▏    | 5419/10570 [00:19<00:18, 271.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 56%|█████▌    | 5870/10570 [00:20<00:16, 281.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 55%|█████▍    | 5780/10570 [00:20<00:17, 277.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 55%|█████▌    | 5853/10570 [00:20<00:16, 283.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 54%|█████▍    | 5729/10570 [00:20<00:17, 273.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 56%|█████▋    | 5956/10570 [00:20<00:15, 288.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 52%|█████▏    | 5466/10570 [00:19<00:19, 259.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 56%|█████▌    | 5896/10570 [00:20<00:16, 284.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 53%|█████▎    | 5624/10570 [00:20<00:18, 267.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 56%|█████▌    | 5867/10570 [00:20<00:16, 282.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 56%|█████▌    | 5913/10570 [00:20<00:16, 283.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 55%|█████▌    | 5838/10570 [00:20<00:16, 282.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 55%|█████▌    | 5850/10570 [00:20<00:18, 251.37it/s][1,mpirank:4,algo-1]<stderr>:#015 55%|█████▍    | 5782/10570 [00:20<00:17, 278.19it/s][1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 5574/10570 [00:19<00:19, 257.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 52%|█████▏    | 5538/10570 [00:19<00:19, 260.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 56%|█████▌    | 5899/10570 [00:20<00:16, 283.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 52%|█████▏    | 5447/10570 [00:19<00:19, 262.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 55%|█████▍    | 5808/10570 [00:20<00:17, 277.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 56%|█████▌    | 5882/10570 [00:20<00:16, 284.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 54%|█████▍    | 5758/10570 [00:20<00:17, 275.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 57%|█████▋    | 5985/10570 [00:20<00:16, 275.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 53%|█████▎    | 5652/10570 [00:20<00:18, 269.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 56%|█████▌    | 5896/10570 [00:20<00:16, 283.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 56%|█████▌    | 5942/10570 [00:20<00:16, 283.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 52%|█████▏    | 5493/10570 [00:19<00:20, 248.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 56%|█████▌    | 5925/10570 [00:20<00:16, 275.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 55%|█████▍    | 5810/10570 [00:20<00:17, 277.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 5602/10570 [00:20<00:18, 263.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 56%|█████▌    | 5876/10570 [00:20<00:18, 252.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 56%|█████▌    | 5867/10570 [00:20<00:16, 281.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 53%|█████▎    | 5566/10570 [00:19<00:18, 263.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 56%|█████▌    | 5928/10570 [00:20<00:16, 282.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 52%|█████▏    | 5474/10570 [00:19<00:19, 256.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 55%|█████▌    | 5837/10570 [00:20<00:16, 279.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 56%|█████▌    | 5911/10570 [00:20<00:16, 284.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 55%|█████▍    | 5786/10570 [00:20<00:17, 276.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 57%|█████▋    | 6013/10570 [00:20<00:16, 274.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 56%|█████▌    | 5925/10570 [00:20<00:16, 282.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 56%|█████▋    | 5971/10570 [00:20<00:16, 284.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 56%|█████▋    | 5954/10570 [00:20<00:16, 279.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 52%|█████▏    | 5520/10570 [00:20<00:19, 253.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 55%|█████▌    | 5838/10570 [00:20<00:17, 278.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 56%|█████▌    | 5896/10570 [00:20<00:16, 283.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 56%|█████▌    | 5902/10570 [00:20<00:18, 253.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 54%|█████▎    | 5679/10570 [00:20<00:18, 257.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|█████▎    | 5629/10570 [00:20<00:19, 258.97it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 53%|█████▎    | 5594/10570 [00:19<00:18, 267.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 56%|█████▋    | 5957/10570 [00:20<00:16, 284.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 55%|█████▌    | 5866/10570 [00:20<00:16, 279.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 56%|█████▌    | 5940/10570 [00:20<00:16, 284.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 52%|█████▏    | 5500/10570 [00:19<00:20, 247.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 55%|█████▌    | 5814/10570 [00:20<00:17, 276.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 57%|█████▋    | 6042/10570 [00:20<00:16, 277.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 56%|█████▋    | 5954/10570 [00:20<00:16, 284.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 57%|█████▋    | 6000/10570 [00:20<00:16, 284.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 57%|█████▋    | 5983/10570 [00:20<00:16, 281.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 52%|█████▏    | 5548/10570 [00:20<00:19, 258.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 55%|█████▌    | 5866/10570 [00:20<00:16, 278.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 54%|█████▍    | 5707/10570 [00:20<00:18, 263.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 56%|█████▌    | 5925/10570 [00:20<00:16, 282.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 56%|█████▌    | 5928/10570 [00:20<00:18, 252.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 53%|█████▎    | 5622/10570 [00:20<00:18, 268.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▎    | 5655/10570 [00:20<00:19, 255.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 57%|█████▋    | 5986/10570 [00:20<00:16, 279.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 56%|█████▌    | 5895/10570 [00:20<00:16, 282.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 56%|█████▋    | 5969/10570 [00:20<00:16, 285.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 52%|█████▏    | 5527/10570 [00:20<00:19, 252.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 55%|█████▌    | 5843/10570 [00:20<00:16, 278.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 57%|█████▋    | 6070/10570 [00:20<00:16, 278.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 57%|█████▋    | 5983/10570 [00:20<00:16, 285.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 57%|█████▋    | 6029/10570 [00:20<00:15, 284.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 57%|█████▋    | 6012/10570 [00:20<00:16, 281.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 56%|█████▌    | 5895/10570 [00:20<00:16, 280.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 53%|█████▎    | 5576/10570 [00:20<00:19, 262.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 54%|█████▍    | 5735/10570 [00:20<00:18, 267.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 56%|█████▋    | 5954/10570 [00:21<00:18, 253.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 56%|█████▋    | 5954/10570 [00:20<00:16, 278.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 53%|█████▎    | 5650/10570 [00:20<00:18, 270.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 5682/10570 [00:20<00:18, 257.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 57%|█████▋    | 6015/10570 [00:20<00:16, 279.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 56%|█████▌    | 5924/10570 [00:20<00:16, 280.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 57%|█████▋    | 5998/10570 [00:20<00:16, 284.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 53%|█████▎    | 5554/10570 [00:20<00:19, 257.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 56%|█████▌    | 5871/10570 [00:20<00:16, 278.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 58%|█████▊    | 6099/10570 [00:20<00:15, 280.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 57%|█████▋    | 6012/10570 [00:20<00:16, 283.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 57%|█████▋    | 6058/10570 [00:21<00:16, 281.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 57%|█████▋    | 6041/10570 [00:21<00:16, 281.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 53%|█████▎    | 5604/10570 [00:20<00:18, 266.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 56%|█████▌    | 5924/10570 [00:20<00:16, 279.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 55%|█████▍    | 5763/10570 [00:20<00:17, 268.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 57%|█████▋    | 5980/10570 [00:21<00:18, 254.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 57%|█████▋    | 5983/10570 [00:21<00:16, 281.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 5711/10570 [00:20<00:18, 264.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 54%|█████▎    | 5678/10570 [00:20<00:19, 253.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 57%|█████▋    | 6044/10570 [00:21<00:16, 280.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 56%|█████▋    | 5953/10570 [00:20<00:16, 282.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 57%|█████▋    | 6027/10570 [00:20<00:15, 284.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 53%|█████▎    | 5581/10570 [00:20<00:19, 260.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 56%|█████▌    | 5900/10570 [00:20<00:16, 279.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 58%|█████▊    | 6128/10570 [00:21<00:15, 281.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 57%|█████▋    | 6041/10570 [00:21<00:15, 283.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 53%|█████▎    | 5631/10570 [00:20<00:18, 266.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 58%|█████▊    | 6087/10570 [00:21<00:15, 281.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 56%|█████▋    | 5953/10570 [00:20<00:16, 281.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 55%|█████▍    | 5790/10570 [00:20<00:17, 268.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 57%|█████▋    | 6070/10570 [00:21<00:16, 279.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 57%|█████▋    | 6006/10570 [00:21<00:18, 251.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 57%|█████▋    | 6012/10570 [00:21<00:16, 280.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|█████▍    | 5739/10570 [00:20<00:18, 267.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 54%|█████▍    | 5707/10570 [00:20<00:18, 261.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 57%|█████▋    | 6073/10570 [00:21<00:16, 278.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 57%|█████▋    | 5982/10570 [00:21<00:16, 283.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 53%|█████▎    | 5609/10570 [00:20<00:18, 264.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 57%|█████▋    | 6056/10570 [00:20<00:15, 282.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 56%|█████▌    | 5928/10570 [00:20<00:16, 279.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 58%|█████▊    | 6158/10570 [00:21<00:15, 284.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 57%|█████▋    | 6070/10570 [00:21<00:16, 280.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 57%|█████▋    | 5982/10570 [00:20<00:16, 281.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 55%|█████▌    | 5818/10570 [00:20<00:17, 270.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 58%|█████▊    | 6116/10570 [00:21<00:15, 280.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 58%|█████▊    | 6099/10570 [00:21<00:15, 280.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 57%|█████▋    | 6032/10570 [00:21<00:17, 252.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 57%|█████▋    | 6041/10570 [00:21<00:16, 281.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▍    | 5767/10570 [00:20<00:17, 270.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 54%|█████▎    | 5658/10570 [00:20<00:19, 254.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 54%|█████▍    | 5735/10570 [00:20<00:18, 265.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 58%|█████▊    | 6102/10570 [00:21<00:15, 279.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 57%|█████▋    | 6011/10570 [00:21<00:16, 282.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 53%|█████▎    | 5636/10570 [00:20<00:18, 264.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 58%|█████▊    | 6085/10570 [00:21<00:15, 281.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 56%|█████▋    | 5957/10570 [00:21<00:16, 280.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 59%|█████▊    | 6188/10570 [00:21<00:15, 286.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 58%|█████▊    | 6099/10570 [00:21<00:15, 280.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 55%|█████▌    | 5846/10570 [00:20<00:17, 271.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 58%|█████▊    | 6128/10570 [00:21<00:15, 282.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 57%|█████▋    | 6011/10570 [00:21<00:16, 280.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 58%|█████▊    | 6146/10570 [00:21<00:15, 283.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 57%|█████▋    | 6070/10570 [00:21<00:16, 279.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▍    | 5795/10570 [00:20<00:17, 270.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 57%|█████▋    | 6058/10570 [00:21<00:18, 248.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 54%|█████▍    | 5687/10570 [00:20<00:18, 262.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 55%|█████▍    | 5763/10570 [00:20<00:17, 268.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 58%|█████▊    | 6131/10570 [00:21<00:15, 280.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 57%|█████▋    | 6040/10570 [00:21<00:16, 280.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 58%|█████▊    | 6114/10570 [00:21<00:15, 283.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 57%|█████▋    | 5986/10570 [00:21<00:16, 282.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 54%|█████▎    | 5663/10570 [00:20<00:19, 252.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 59%|█████▉    | 6218/10570 [00:21<00:15, 287.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 58%|█████▊    | 6128/10570 [00:21<00:15, 283.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 58%|█████▊    | 6175/10570 [00:21<00:15, 283.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 56%|█████▌    | 5874/10570 [00:21<00:17, 270.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 57%|█████▋    | 6040/10570 [00:21<00:16, 279.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 58%|█████▊    | 6157/10570 [00:21<00:15, 276.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 58%|█████▊    | 6083/10570 [00:21<00:18, 248.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 54%|█████▍    | 5715/10570 [00:20<00:18, 266.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 58%|█████▊    | 6099/10570 [00:21<00:15, 280.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 5824/10570 [00:20<00:17, 273.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 55%|█████▍    | 5791/10570 [00:20<00:17, 268.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 58%|█████▊    | 6160/10570 [00:21<00:15, 282.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 57%|█████▋    | 6069/10570 [00:21<00:16, 277.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 57%|█████▋    | 6015/10570 [00:21<00:16, 280.25it/s][1,mpirank:13,algo-2]<stderr>:#015 58%|█████▊    | 6143/10570 [00:21<00:15, 280.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 54%|█████▍    | 5691/10570 [00:20<00:18, 259.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 58%|█████▊    | 6157/10570 [00:21<00:15, 281.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 59%|█████▉    | 6247/10570 [00:21<00:15, 274.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 56%|█████▌    | 5902/10570 [00:21<00:17, 272.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 59%|█████▊    | 6204/10570 [00:21<00:15, 283.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 57%|█████▋    | 6068/10570 [00:21<00:16, 278.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 59%|█████▊    | 6186/10570 [00:21<00:15, 279.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 58%|█████▊    | 6108/10570 [00:21<00:18, 246.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 54%|█████▍    | 5743/10570 [00:20<00:17, 268.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|█████▌    | 5852/10570 [00:20<00:17, 273.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 58%|█████▊    | 6128/10570 [00:21<00:15, 278.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 55%|█████▌    | 5819/10570 [00:20<00:17, 270.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 59%|█████▊    | 6189/10570 [00:21<00:15, 283.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 58%|█████▊    | 6097/10570 [00:21<00:16, 277.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 58%|█████▊    | 6172/10570 [00:21<00:15, 282.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 57%|█████▋    | 6044/10570 [00:21<00:16, 279.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 54%|█████▍    | 5719/10570 [00:20<00:18, 264.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 59%|█████▊    | 6186/10570 [00:21<00:15, 282.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 59%|█████▉    | 6276/10570 [00:21<00:15, 278.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 56%|█████▌    | 5930/10570 [00:21<00:17, 272.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 58%|█████▊    | 6096/10570 [00:21<00:16, 277.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 59%|█████▉    | 6215/10570 [00:21<00:15, 280.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 55%|█████▍    | 5770/10570 [00:20<00:17, 268.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 5880/10570 [00:21<00:17, 275.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 58%|█████▊    | 6138/10570 [00:21<00:17, 259.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 58%|█████▊    | 6157/10570 [00:21<00:15, 280.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 59%|█████▉    | 6233/10570 [00:21<00:16, 270.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 55%|█████▌    | 5847/10570 [00:20<00:17, 271.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 59%|█████▉    | 6218/10570 [00:21<00:15, 283.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 58%|█████▊    | 6125/10570 [00:21<00:16, 276.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 59%|█████▊    | 6201/10570 [00:21<00:15, 282.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 57%|█████▋    | 6072/10570 [00:21<00:16, 276.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 54%|█████▍    | 5746/10570 [00:20<00:18, 265.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 59%|█████▉    | 6215/10570 [00:21<00:15, 283.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 60%|█████▉    | 6306/10570 [00:21<00:15, 282.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 56%|█████▋    | 5958/10570 [00:21<00:16, 271.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 58%|█████▊    | 6125/10570 [00:21<00:15, 278.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 58%|█████▊    | 6167/10570 [00:21<00:16, 268.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 55%|█████▍    | 5797/10570 [00:21<00:17, 266.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 59%|█████▊    | 6186/10570 [00:21<00:15, 281.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 59%|█████▉    | 6262/10570 [00:21<00:15, 274.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 5908/10570 [00:21<00:17, 268.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 59%|█████▉    | 6244/10570 [00:21<00:16, 268.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 56%|█████▌    | 5875/10570 [00:21<00:17, 272.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 58%|█████▊    | 6154/10570 [00:21<00:15, 278.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 59%|█████▉    | 6247/10570 [00:21<00:15, 270.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 58%|█████▊    | 6100/10570 [00:21<00:16, 276.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 55%|█████▍    | 5773/10570 [00:20<00:18, 265.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 59%|█████▉    | 6230/10570 [00:21<00:16, 270.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 60%|█████▉    | 6335/10570 [00:21<00:14, 283.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 57%|█████▋    | 5986/10570 [00:21<00:16, 273.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 58%|█████▊    | 6154/10570 [00:21<00:15, 280.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 59%|█████▊    | 6196/10570 [00:21<00:15, 274.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 55%|█████▌    | 5825/10570 [00:21<00:17, 269.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 59%|█████▉    | 6244/10570 [00:21<00:16, 269.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 59%|█████▉    | 6215/10570 [00:21<00:15, 282.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 60%|█████▉    | 6291/10570 [00:21<00:15, 278.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▌    | 5936/10570 [00:21<00:17, 270.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 59%|█████▉    | 6273/10570 [00:21<00:15, 272.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 56%|█████▌    | 5903/10570 [00:21<00:17, 273.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 58%|█████▊    | 6183/10570 [00:21<00:15, 280.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 59%|█████▉    | 6276/10570 [00:21<00:15, 274.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 58%|█████▊    | 6128/10570 [00:21<00:16, 277.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 55%|█████▍    | 5800/10570 [00:21<00:17, 266.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 59%|█████▉    | 6259/10570 [00:21<00:15, 274.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 60%|██████    | 6365/10570 [00:21<00:14, 286.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 57%|█████▋    | 6014/10570 [00:21<00:16, 272.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 58%|█████▊    | 6183/10570 [00:21<00:15, 280.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 59%|█████▉    | 6224/10570 [00:22<00:15, 275.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 59%|█████▉    | 6273/10570 [00:21<00:15, 273.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 55%|█████▌    | 5853/10570 [00:21<00:17, 270.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 60%|█████▉    | 6321/10570 [00:21<00:15, 282.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|█████▋    | 5965/10570 [00:21<00:16, 273.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 60%|█████▉    | 6303/10570 [00:22<00:15, 278.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 59%|█████▉    | 6244/10570 [00:21<00:16, 270.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 56%|█████▌    | 5931/10570 [00:21<00:17, 267.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 59%|█████▉    | 6212/10570 [00:21<00:15, 281.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 60%|█████▉    | 6306/10570 [00:21<00:15, 279.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 58%|█████▊    | 6157/10570 [00:21<00:15, 278.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 55%|█████▌    | 5828/10570 [00:21<00:17, 267.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 59%|█████▉    | 6289/10570 [00:21<00:15, 279.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 57%|█████▋    | 6042/10570 [00:21<00:16, 272.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 59%|█████▉    | 6212/10570 [00:21<00:15, 278.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 56%|█████▌    | 5881/10570 [00:21<00:17, 271.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 60%|██████    | 6394/10570 [00:21<00:15, 273.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 60%|█████▉    | 6303/10570 [00:21<00:15, 279.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 59%|█████▉    | 6252/10570 [00:22<00:16, 267.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 60%|██████    | 6350/10570 [00:22<00:14, 282.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 5993/10570 [00:21<00:16, 275.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 60%|█████▉    | 6332/10570 [00:22<00:15, 279.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 59%|█████▉    | 6273/10570 [00:22<00:15, 274.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 56%|█████▋    | 5959/10570 [00:21<00:17, 271.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 60%|█████▉    | 6335/10570 [00:22<00:15, 280.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 59%|█████▊    | 6186/10570 [00:21<00:15, 279.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 55%|█████▌    | 5855/10570 [00:21<00:17, 268.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 59%|█████▉    | 6241/10570 [00:22<00:16, 268.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 60%|█████▉    | 6319/10570 [00:21<00:15, 282.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 57%|█████▋    | 6070/10570 [00:21<00:16, 270.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 61%|██████    | 6424/10570 [00:22<00:14, 280.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 56%|█████▌    | 5909/10570 [00:21<00:17, 272.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 60%|█████▉    | 6332/10570 [00:22<00:15, 280.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 59%|█████▉    | 6281/10570 [00:22<00:15, 274.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 6021/10570 [00:21<00:16, 275.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 59%|█████▉    | 6240/10570 [00:21<00:16, 266.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 60%|██████    | 6361/10570 [00:22<00:15, 277.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 60%|█████▉    | 6303/10570 [00:22<00:15, 279.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 60%|██████    | 6379/10570 [00:22<00:15, 270.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 57%|█████▋    | 5987/10570 [00:21<00:16, 273.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 60%|██████    | 6364/10570 [00:22<00:14, 281.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 59%|█████▉    | 6214/10570 [00:21<00:15, 277.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 56%|█████▌    | 5883/10570 [00:21<00:17, 269.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 59%|█████▉    | 6270/10570 [00:22<00:15, 272.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 60%|██████    | 6348/10570 [00:22<00:14, 283.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 58%|█████▊    | 6098/10570 [00:21<00:16, 270.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 61%|██████    | 6454/10570 [00:22<00:14, 285.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 56%|█████▌    | 5937/10570 [00:21<00:17, 272.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 60%|██████    | 6361/10570 [00:22<00:14, 280.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 60%|█████▉    | 6311/10570 [00:22<00:15, 279.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 6049/10570 [00:21<00:16, 274.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 59%|█████▉    | 6268/10570 [00:22<00:15, 270.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 60%|█████▉    | 6332/10570 [00:22<00:15, 281.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 61%|██████    | 6409/10570 [00:22<00:15, 276.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 57%|█████▋    | 6015/10570 [00:21<00:16, 272.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 60%|██████    | 6389/10570 [00:22<00:15, 266.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 56%|█████▌    | 5910/10570 [00:21<00:17, 269.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 60%|█████▉    | 6299/10570 [00:22<00:15, 276.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 60%|██████    | 6393/10570 [00:22<00:15, 269.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 59%|█████▉    | 6242/10570 [00:22<00:16, 264.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 60%|██████    | 6377/10570 [00:22<00:15, 272.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 58%|█████▊    | 6126/10570 [00:21<00:16, 273.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 61%|██████▏   | 6483/10570 [00:22<00:14, 285.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 56%|█████▋    | 5965/10570 [00:21<00:16, 273.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 60%|█████▉    | 6340/10570 [00:22<00:15, 281.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|█████▋    | 6077/10570 [00:21<00:16, 272.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 60%|█████▉    | 6298/10570 [00:22<00:15, 276.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 60%|██████    | 6361/10570 [00:22<00:14, 282.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 60%|██████    | 6390/10570 [00:22<00:15, 268.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 61%|██████    | 6437/10570 [00:22<00:15, 274.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 61%|██████    | 6418/10570 [00:22<00:15, 273.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 57%|█████▋    | 6043/10570 [00:21<00:16, 272.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 56%|█████▌    | 5937/10570 [00:21<00:17, 269.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 60%|█████▉    | 6328/10570 [00:22<00:15, 278.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 61%|██████    | 6423/10570 [00:22<00:15, 275.49it/s][1,mpirank:7,algo-1]<stderr>:#015 59%|█████▉    | 6270/10570 [00:22<00:16, 268.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 61%|██████    | 6406/10570 [00:22<00:15, 274.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 62%|██████▏   | 6512/10570 [00:22<00:14, 286.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 58%|█████▊    | 6154/10570 [00:22<00:16, 269.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 57%|█████▋    | 5993/10570 [00:21<00:16, 273.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 60%|██████    | 6369/10570 [00:22<00:14, 283.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 6105/10570 [00:21<00:16, 273.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 60%|█████▉    | 6327/10570 [00:22<00:15, 278.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 61%|██████    | 6419/10570 [00:22<00:15, 274.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 61%|██████    | 6467/10570 [00:22<00:14, 280.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 61%|██████    | 6448/10570 [00:22<00:14, 278.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 57%|█████▋    | 6071/10570 [00:21<00:16, 270.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 60%|██████    | 6390/10570 [00:22<00:15, 269.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 56%|█████▋    | 5965/10570 [00:21<00:17, 270.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 60%|█████▉    | 6299/10570 [00:22<00:15, 273.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 61%|██████    | 6453/10570 [00:22<00:14, 279.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 60%|██████    | 6356/10570 [00:22<00:15, 272.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 61%|██████    | 6436/10570 [00:22<00:14, 279.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 62%|██████▏   | 6542/10570 [00:22<00:13, 288.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 58%|█████▊    | 6182/10570 [00:22<00:16, 272.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 57%|█████▋    | 6021/10570 [00:21<00:16, 271.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 6133/10570 [00:21<00:16, 274.63it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 60%|██████    | 6355/10570 [00:22<00:15, 278.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 61%|██████    | 6449/10570 [00:22<00:14, 279.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 61%|██████    | 6398/10570 [00:22<00:15, 270.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 61%|██████▏   | 6496/10570 [00:22<00:14, 281.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 61%|██████▏   | 6477/10570 [00:22<00:14, 280.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 58%|█████▊    | 6099/10570 [00:21<00:16, 271.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 61%|██████    | 6419/10570 [00:22<00:15, 275.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 57%|█████▋    | 5993/10570 [00:21<00:17, 268.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 60%|█████▉    | 6328/10570 [00:22<00:15, 275.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 61%|██████▏   | 6482/10570 [00:22<00:14, 281.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 61%|██████    | 6466/10570 [00:22<00:14, 283.88it/s][1,mpirank:2,algo-1]<stderr>:#015 60%|██████    | 6384/10570 [00:22<00:15, 262.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 62%|██████▏   | 6571/10570 [00:22<00:13, 288.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 59%|█████▉    | 6210/10570 [00:22<00:15, 272.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|█████▊    | 6161/10570 [00:22<00:15, 275.93it/s]#033[A[1,mpirank:8,algo-2]<stderr>:#015 57%|█████▋    | 6049/10570 [00:22<00:16, 269.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 61%|██████▏   | 6478/10570 [00:22<00:14, 281.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 61%|██████    | 6426/10570 [00:22<00:15, 272.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 60%|██████    | 6383/10570 [00:22<00:15, 266.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 62%|██████▏   | 6525/10570 [00:22<00:14, 282.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 62%|██████▏   | 6506/10570 [00:22<00:14, 282.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 58%|█████▊    | 6127/10570 [00:21<00:16, 273.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 61%|██████    | 6449/10570 [00:22<00:14, 280.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 57%|█████▋    | 6020/10570 [00:21<00:16, 268.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 62%|██████▏   | 6511/10570 [00:22<00:14, 282.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 60%|██████    | 6357/10570 [00:22<00:15, 277.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 61%|██████    | 6413/10570 [00:22<00:15, 268.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 61%|██████▏   | 6495/10570 [00:22<00:14, 284.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 62%|██████▏   | 6600/10570 [00:22<00:13, 285.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▊    | 6189/10570 [00:22<00:15, 277.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 57%|█████▋    | 6077/10570 [00:22<00:16, 267.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 59%|█████▉    | 6238/10570 [00:22<00:16, 259.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 62%|██████▏   | 6507/10570 [00:22<00:14, 282.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 61%|██████    | 6456/10570 [00:22<00:14, 278.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 61%|██████    | 6412/10570 [00:22<00:15, 273.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 62%|██████▏   | 6554/10570 [00:22<00:14, 284.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 62%|██████▏   | 6535/10570 [00:22<00:14, 283.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 58%|█████▊    | 6155/10570 [00:22<00:16, 274.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 61%|██████▏   | 6478/10570 [00:22<00:14, 282.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 57%|█████▋    | 6047/10570 [00:22<00:16, 267.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 62%|██████▏   | 6540/10570 [00:22<00:14, 284.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 61%|██████    | 6442/10570 [00:22<00:15, 274.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 62%|██████▏   | 6524/10570 [00:22<00:14, 284.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 60%|██████    | 6385/10570 [00:22<00:15, 264.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 63%|██████▎   | 6630/10570 [00:22<00:13, 289.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 6217/10570 [00:22<00:15, 276.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 58%|█████▊    | 6104/10570 [00:22<00:16, 266.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 59%|█████▉    | 6266/10570 [00:22<00:16, 263.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 62%|██████▏   | 6536/10570 [00:22<00:14, 283.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 61%|██████▏   | 6485/10570 [00:23<00:14, 280.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 61%|██████    | 6442/10570 [00:22<00:14, 278.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 62%|██████▏   | 6583/10570 [00:22<00:13, 284.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 62%|██████▏   | 6564/10570 [00:22<00:14, 283.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 58%|█████▊    | 6183/10570 [00:22<00:15, 275.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 62%|██████▏   | 6507/10570 [00:22<00:14, 282.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 57%|█████▋    | 6074/10570 [00:22<00:16, 266.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 62%|██████▏   | 6569/10570 [00:22<00:14, 284.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 62%|██████▏   | 6553/10570 [00:22<00:14, 285.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 61%|██████    | 6471/10570 [00:22<00:14, 276.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 61%|██████    | 6414/10570 [00:22<00:15, 271.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 63%|██████▎   | 6659/10570 [00:22<00:14, 275.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 58%|█████▊    | 6132/10570 [00:22<00:16, 269.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 62%|██████▏   | 6514/10570 [00:23<00:14, 282.59it/s][1,mpirank:14,algo-2]<stderr>:#015 62%|██████▏   | 6565/10570 [00:22<00:14, 283.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 60%|█████▉    | 6295/10570 [00:22<00:15, 269.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 6245/10570 [00:22<00:16, 263.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 61%|██████    | 6471/10570 [00:22<00:14, 279.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 63%|██████▎   | 6612/10570 [00:23<00:13, 284.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 59%|█████▉    | 6211/10570 [00:22<00:15, 275.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 62%|██████▏   | 6593/10570 [00:23<00:14, 282.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 62%|██████▏   | 6536/10570 [00:22<00:14, 284.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 58%|█████▊    | 6101/10570 [00:22<00:16, 266.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 62%|██████▏   | 6598/10570 [00:23<00:14, 281.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 62%|██████▏   | 6582/10570 [00:22<00:13, 285.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 61%|██████    | 6443/10570 [00:22<00:14, 275.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 61%|██████▏   | 6500/10570 [00:22<00:14, 278.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 63%|██████▎   | 6688/10570 [00:22<00:13, 278.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 58%|█████▊    | 6160/10570 [00:22<00:16, 271.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 60%|█████▉    | 6323/10570 [00:22<00:15, 271.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 62%|██████▏   | 6594/10570 [00:22<00:14, 281.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|█████▉    | 6273/10570 [00:22<00:16, 266.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 61%|██████▏   | 6500/10570 [00:22<00:14, 280.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 63%|██████▎   | 6642/10570 [00:23<00:13, 286.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 63%|██████▎   | 6622/10570 [00:23<00:13, 284.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 62%|██████▏   | 6543/10570 [00:23<00:14, 271.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 62%|██████▏   | 6565/10570 [00:23<00:14, 284.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 59%|█████▉    | 6239/10570 [00:22<00:16, 261.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 58%|█████▊    | 6129/10570 [00:22<00:16, 269.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 63%|██████▎   | 6627/10570 [00:23<00:13, 284.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 61%|██████    | 6472/10570 [00:22<00:14, 277.97it/s][1,mpirank:2,algo-1]<stderr>:#015 62%|██████▏   | 6529/10570 [00:23<00:14, 280.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 63%|██████▎   | 6611/10570 [00:22<00:13, 283.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 64%|██████▎   | 6716/10570 [00:23<00:13, 278.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 59%|█████▊    | 6188/10570 [00:22<00:16, 272.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 63%|██████▎   | 6623/10570 [00:23<00:13, 284.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 60%|██████    | 6351/10570 [00:22<00:15, 269.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|█████▉    | 6302/10570 [00:22<00:15, 271.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 63%|██████▎   | 6671/10570 [00:23<00:13, 286.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 62%|██████▏   | 6529/10570 [00:22<00:14, 281.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 63%|██████▎   | 6651/10570 [00:23<00:13, 285.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 62%|██████▏   | 6572/10570 [00:23<00:14, 275.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 62%|██████▏   | 6594/10570 [00:23<00:14, 282.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 59%|█████▉    | 6267/10570 [00:22<00:16, 265.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 58%|█████▊    | 6156/10570 [00:22<00:16, 265.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 63%|██████▎   | 6656/10570 [00:23<00:13, 284.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 62%|██████▏   | 6558/10570 [00:23<00:14, 281.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 62%|██████▏   | 6501/10570 [00:22<00:14, 279.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 63%|██████▎   | 6641/10570 [00:23<00:13, 285.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 64%|██████▍   | 6746/10570 [00:23<00:13, 282.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 59%|█████▉    | 6216/10570 [00:22<00:15, 272.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 63%|██████▎   | 6652/10570 [00:23<00:13, 284.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|█████▉    | 6330/10570 [00:22<00:15, 273.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 62%|██████▏   | 6558/10570 [00:23<00:14, 282.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 63%|██████▎   | 6700/10570 [00:23<00:13, 284.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 63%|██████▎   | 6680/10570 [00:23<00:13, 285.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 62%|██████▏   | 6600/10570 [00:23<00:14, 276.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 60%|██████    | 6379/10570 [00:22<00:16, 259.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 63%|██████▎   | 6624/10570 [00:23<00:13, 284.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 60%|█████▉    | 6295/10570 [00:22<00:15, 267.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 59%|█████▊    | 6184/10570 [00:22<00:16, 267.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 63%|██████▎   | 6685/10570 [00:23<00:13, 284.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 63%|██████▎   | 6670/10570 [00:23<00:13, 286.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 62%|██████▏   | 6530/10570 [00:23<00:14, 280.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 62%|██████▏   | 6587/10570 [00:23<00:14, 281.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 64%|██████▍   | 6776/10570 [00:23<00:13, 286.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 63%|██████▎   | 6681/10570 [00:23<00:13, 285.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 6358/10570 [00:22<00:15, 274.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 62%|██████▏   | 6587/10570 [00:23<00:14, 281.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 64%|██████▎   | 6729/10570 [00:23<00:13, 283.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 59%|█████▉    | 6244/10570 [00:22<00:16, 259.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 63%|██████▎   | 6709/10570 [00:23<00:13, 282.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 63%|██████▎   | 6628/10570 [00:23<00:14, 272.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 63%|██████▎   | 6653/10570 [00:23<00:13, 285.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 61%|██████    | 6408/10570 [00:23<00:15, 266.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 60%|█████▉    | 6322/10570 [00:22<00:16, 264.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 59%|█████▉    | 6212/10570 [00:22<00:16, 268.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 64%|██████▎   | 6714/10570 [00:23<00:13, 281.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 63%|██████▎   | 6699/10570 [00:23<00:13, 284.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 63%|██████▎   | 6616/10570 [00:23<00:14, 281.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 62%|██████▏   | 6559/10570 [00:23<00:14, 280.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 64%|██████▍   | 6805/10570 [00:23<00:13, 286.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 63%|██████▎   | 6710/10570 [00:23<00:13, 281.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 63%|██████▎   | 6616/10570 [00:23<00:14, 281.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 64%|██████▍   | 6759/10570 [00:23<00:13, 286.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 64%|██████▎   | 6738/10570 [00:23<00:13, 283.25it/s][1,mpirank:8,algo-2]<stderr>:#015 59%|█████▉    | 6272/10570 [00:22<00:16, 263.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 63%|██████▎   | 6657/10570 [00:23<00:14, 277.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 63%|██████▎   | 6682/10570 [00:23<00:13, 285.93it/s][1,mpirank:15,algo-2]<stderr>:#015 61%|██████    | 6436/10570 [00:23<00:15, 270.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|██████    | 6386/10570 [00:22<00:15, 261.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 60%|██████    | 6350/10570 [00:22<00:15, 268.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 64%|██████▍   | 6743/10570 [00:23<00:13, 283.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 59%|█████▉    | 6239/10570 [00:22<00:17, 254.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 64%|██████▎   | 6728/10570 [00:23<00:13, 283.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 62%|██████▏   | 6588/10570 [00:23<00:14, 278.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 63%|██████▎   | 6645/10570 [00:23<00:14, 272.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 65%|██████▍   | 6834/10570 [00:23<00:13, 285.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 64%|██████▍   | 6739/10570 [00:23<00:13, 282.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 63%|██████▎   | 6645/10570 [00:23<00:13, 284.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 64%|██████▍   | 6788/10570 [00:23<00:13, 286.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 64%|██████▍   | 6768/10570 [00:23<00:13, 286.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 63%|██████▎   | 6686/10570 [00:23<00:13, 280.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 60%|█████▉    | 6301/10570 [00:22<00:15, 268.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 61%|██████    | 6465/10570 [00:23<00:14, 274.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 6415/10570 [00:23<00:15, 267.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 63%|██████▎   | 6711/10570 [00:23<00:13, 282.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 60%|██████    | 6377/10570 [00:22<00:16, 259.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 64%|██████▍   | 6773/10570 [00:23<00:13, 286.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 59%|█████▉    | 6266/10570 [00:22<00:16, 258.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 64%|██████▍   | 6758/10570 [00:23<00:13, 285.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 63%|██████▎   | 6616/10570 [00:23<00:14, 279.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 63%|██████▎   | 6674/10570 [00:23<00:14, 275.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 65%|██████▍   | 6863/10570 [00:23<00:12, 285.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 64%|██████▍   | 6769/10570 [00:23<00:13, 285.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 63%|██████▎   | 6674/10570 [00:23<00:13, 284.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 64%|██████▍   | 6817/10570 [00:23<00:13, 284.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 60%|█████▉    | 6329/10570 [00:23<00:15, 270.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 64%|██████▍   | 6797/10570 [00:23<00:13, 284.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 64%|██████▎   | 6715/10570 [00:23<00:13, 279.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 61%|██████▏   | 6493/10570 [00:23<00:14, 274.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 64%|██████▍   | 6740/10570 [00:23<00:13, 283.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 6442/10570 [00:23<00:15, 259.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 61%|██████    | 6404/10570 [00:23<00:15, 262.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 64%|██████▍   | 6802/10570 [00:23<00:13, 283.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 60%|█████▉    | 6294/10570 [00:22<00:16, 263.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 64%|██████▍   | 6787/10570 [00:23<00:13, 286.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 63%|██████▎   | 6645/10570 [00:23<00:13, 280.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 63%|██████▎   | 6702/10570 [00:23<00:14, 274.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 65%|██████▌   | 6892/10570 [00:23<00:12, 285.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 64%|██████▍   | 6798/10570 [00:23<00:13, 285.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 63%|██████▎   | 6703/10570 [00:23<00:13, 282.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 65%|██████▍   | 6846/10570 [00:23<00:13, 282.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 60%|██████    | 6357/10570 [00:23<00:15, 271.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 64%|██████▍   | 6744/10570 [00:23<00:13, 282.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 65%|██████▍   | 6826/10570 [00:23<00:13, 282.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 62%|██████▏   | 6521/10570 [00:23<00:14, 274.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 64%|██████▍   | 6770/10570 [00:23<00:13, 286.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████    | 6470/10570 [00:23<00:15, 264.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 61%|██████    | 6432/10570 [00:23<00:15, 267.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 65%|██████▍   | 6831/10570 [00:23<00:13, 282.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 60%|█████▉    | 6322/10570 [00:23<00:15, 266.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 64%|██████▍   | 6816/10570 [00:23<00:13, 284.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 63%|██████▎   | 6674/10570 [00:23<00:13, 280.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 64%|██████▎   | 6730/10570 [00:23<00:13, 275.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 65%|██████▌   | 6921/10570 [00:23<00:12, 286.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 65%|██████▍   | 6827/10570 [00:23<00:13, 283.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 64%|██████▎   | 6732/10570 [00:23<00:13, 282.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 65%|██████▌   | 6875/10570 [00:23<00:13, 282.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 64%|██████▍   | 6774/10570 [00:24<00:13, 286.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 65%|██████▍   | 6855/10570 [00:23<00:13, 282.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 62%|██████▏   | 6550/10570 [00:23<00:14, 275.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 64%|██████▍   | 6799/10570 [00:23<00:13, 284.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|██████▏   | 6498/10570 [00:23<00:15, 268.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 60%|██████    | 6385/10570 [00:23<00:16, 259.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 61%|██████    | 6461/10570 [00:23<00:15, 272.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 65%|██████▍   | 6860/10570 [00:23<00:13, 282.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 60%|██████    | 6349/10570 [00:23<00:16, 263.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 65%|██████▍   | 6845/10570 [00:23<00:13, 282.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 63%|██████▎   | 6703/10570 [00:23<00:13, 278.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 64%|██████▍   | 6759/10570 [00:23<00:13, 278.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 66%|██████▌   | 6951/10570 [00:23<00:12, 289.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 65%|██████▍   | 6856/10570 [00:23<00:13, 283.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 64%|██████▍   | 6762/10570 [00:23<00:13, 284.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 65%|██████▌   | 6904/10570 [00:24<00:13, 281.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 64%|██████▍   | 6803/10570 [00:24<00:13, 284.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 62%|██████▏   | 6578/10570 [00:23<00:14, 275.75it/s][1,mpirank:9,algo-2]<stderr>:#015 65%|██████▌   | 6884/10570 [00:24<00:13, 281.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 65%|██████▍   | 6828/10570 [00:24<00:13, 283.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 6526/10570 [00:23<00:14, 271.31it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 61%|██████    | 6413/10570 [00:23<00:15, 264.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 61%|██████▏   | 6489/10570 [00:23<00:15, 267.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 65%|██████▌   | 6889/10570 [00:24<00:13, 281.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 60%|██████    | 6376/10570 [00:23<00:16, 257.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 65%|██████▌   | 6874/10570 [00:23<00:13, 282.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 64%|██████▎   | 6731/10570 [00:23<00:13, 278.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 64%|██████▍   | 6788/10570 [00:23<00:13, 279.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 66%|██████▌   | 6981/10570 [00:24<00:12, 289.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 65%|██████▌   | 6885/10570 [00:24<00:13, 282.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 64%|██████▍   | 6791/10570 [00:23<00:13, 285.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 66%|██████▌   | 6933/10570 [00:24<00:12, 283.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 65%|██████▍   | 6832/10570 [00:24<00:13, 283.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 65%|██████▌   | 6913/10570 [00:24<00:12, 281.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 62%|██████▏   | 6606/10570 [00:23<00:14, 272.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 6554/10570 [00:23<00:14, 273.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 65%|██████▍   | 6857/10570 [00:24<00:13, 283.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 61%|██████    | 6441/10570 [00:23<00:15, 268.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 62%|██████▏   | 6517/10570 [00:23<00:15, 269.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 65%|██████▌   | 6918/10570 [00:24<00:12, 281.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 61%|██████    | 6402/10570 [00:23<00:16, 256.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 65%|██████▌   | 6903/10570 [00:23<00:12, 283.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 64%|██████▍   | 6760/10570 [00:23<00:13, 280.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 64%|██████▍   | 6817/10570 [00:24<00:13, 279.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 66%|██████▋   | 7011/10570 [00:24<00:12, 291.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 65%|██████▌   | 6914/10570 [00:24<00:12, 281.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 65%|██████▍   | 6820/10570 [00:23<00:13, 283.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 66%|██████▌   | 6963/10570 [00:24<00:12, 285.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 65%|██████▍   | 6861/10570 [00:24<00:13, 284.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 66%|██████▌   | 6943/10570 [00:24<00:12, 285.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|██████▏   | 6582/10570 [00:23<00:14, 275.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 63%|██████▎   | 6635/10570 [00:23<00:14, 274.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 65%|██████▌   | 6886/10570 [00:24<00:13, 282.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 61%|██████    | 6469/10570 [00:23<00:15, 271.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 62%|██████▏   | 6545/10570 [00:23<00:14, 272.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 66%|██████▌   | 6948/10570 [00:24<00:12, 284.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 61%|██████    | 6430/10570 [00:23<00:15, 262.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 66%|██████▌   | 6933/10570 [00:24<00:12, 285.43it/s][1,mpirank:7,algo-1]<stderr>:#015 64%|██████▍   | 6789/10570 [00:24<00:13, 281.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 65%|██████▍   | 6845/10570 [00:24<00:13, 278.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 67%|██████▋   | 7041/10570 [00:24<00:12, 291.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 66%|██████▌   | 6944/10570 [00:24<00:12, 285.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 66%|██████▌   | 6992/10570 [00:24<00:12, 284.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 65%|██████▍   | 6849/10570 [00:24<00:13, 281.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 65%|██████▌   | 6890/10570 [00:24<00:12, 283.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 66%|██████▌   | 6972/10570 [00:24<00:12, 286.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 6610/10570 [00:23<00:14, 274.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 63%|██████▎   | 6663/10570 [00:23<00:14, 275.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 65%|██████▌   | 6915/10570 [00:24<00:12, 281.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 61%|██████▏   | 6497/10570 [00:23<00:14, 272.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 62%|██████▏   | 6573/10570 [00:23<00:14, 273.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 66%|██████▌   | 6977/10570 [00:24<00:12, 285.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 61%|██████    | 6458/10570 [00:23<00:15, 267.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 66%|██████▌   | 6963/10570 [00:24<00:12, 287.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 65%|██████▍   | 6818/10570 [00:24<00:13, 278.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 65%|██████▌   | 6873/10570 [00:24<00:13, 278.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 67%|██████▋   | 7071/10570 [00:24<00:12, 291.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 66%|██████▌   | 6973/10570 [00:24<00:12, 286.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 66%|██████▋   | 7022/10570 [00:24<00:12, 286.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 65%|██████▌   | 6919/10570 [00:24<00:12, 284.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 65%|██████▌   | 6878/10570 [00:24<00:13, 280.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 66%|██████▌   | 7002/10570 [00:24<00:12, 287.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 6639/10570 [00:23<00:14, 276.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 63%|██████▎   | 6691/10570 [00:24<00:14, 274.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 62%|██████▏   | 6525/10570 [00:23<00:14, 272.70it/s][1,mpirank:6,algo-1]<stderr>:#015 66%|██████▌   | 6945/10570 [00:24<00:12, 284.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 62%|██████▏   | 6601/10570 [00:23<00:14, 271.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 66%|██████▋   | 7006/10570 [00:24<00:12, 286.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 61%|██████▏   | 6486/10570 [00:23<00:15, 268.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 66%|██████▌   | 6993/10570 [00:24<00:12, 288.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 65%|██████▍   | 6846/10570 [00:24<00:13, 277.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 65%|██████▌   | 6902/10570 [00:24<00:13, 279.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 67%|██████▋   | 7101/10570 [00:24<00:11, 291.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 66%|██████▋   | 7003/10570 [00:24<00:12, 287.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 67%|██████▋   | 7051/10570 [00:24<00:12, 286.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 66%|██████▌   | 6949/10570 [00:24<00:12, 287.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 67%|██████▋   | 7031/10570 [00:24<00:12, 288.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 65%|██████▌   | 6907/10570 [00:24<00:13, 278.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 6667/10570 [00:23<00:14, 276.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 66%|██████▌   | 6974/10570 [00:24<00:12, 285.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 64%|██████▎   | 6719/10570 [00:24<00:14, 272.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 62%|██████▏   | 6553/10570 [00:23<00:14, 268.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 63%|██████▎   | 6630/10570 [00:23<00:14, 274.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 67%|██████▋   | 7035/10570 [00:24<00:12, 287.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 62%|██████▏   | 6514/10570 [00:23<00:15, 269.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 66%|██████▋   | 7023/10570 [00:24<00:12, 288.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 65%|██████▌   | 6874/10570 [00:24<00:13, 276.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 66%|██████▌   | 6931/10570 [00:24<00:12, 281.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 67%|██████▋   | 7131/10570 [00:24<00:11, 292.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 67%|██████▋   | 7033/10570 [00:24<00:12, 288.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 67%|██████▋   | 7080/10570 [00:24<00:12, 286.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 66%|██████▌   | 6979/10570 [00:24<00:12, 288.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 67%|██████▋   | 7060/10570 [00:24<00:12, 286.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 66%|██████▌   | 6936/10570 [00:24<00:12, 281.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|██████▎   | 6695/10570 [00:24<00:14, 274.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 64%|██████▍   | 6747/10570 [00:24<00:13, 274.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 66%|██████▋   | 7004/10570 [00:24<00:12, 286.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 62%|██████▏   | 6581/10570 [00:23<00:14, 270.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 63%|██████▎   | 6658/10570 [00:23<00:14, 275.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 67%|██████▋   | 7064/10570 [00:24<00:12, 286.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 62%|██████▏   | 6542/10570 [00:23<00:14, 271.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 67%|██████▋   | 7052/10570 [00:24<00:12, 287.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 65%|██████▌   | 6902/10570 [00:24<00:13, 276.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 66%|██████▌   | 6960/10570 [00:24<00:12, 283.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 68%|██████▊   | 7161/10570 [00:24<00:11, 293.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 67%|██████▋   | 7062/10570 [00:24<00:12, 287.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 67%|██████▋   | 7110/10570 [00:24<00:12, 287.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 67%|██████▋   | 7089/10570 [00:24<00:12, 287.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 66%|██████▌   | 6965/10570 [00:24<00:12, 283.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 66%|██████▋   | 7009/10570 [00:24<00:12, 289.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▎   | 6723/10570 [00:24<00:14, 273.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 64%|██████▍   | 6776/10570 [00:24<00:13, 277.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 67%|██████▋   | 7033/10570 [00:24<00:12, 287.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 63%|██████▎   | 6609/10570 [00:24<00:14, 270.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 63%|██████▎   | 6686/10570 [00:24<00:14, 275.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 67%|██████▋   | 7093/10570 [00:24<00:12, 286.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 62%|██████▏   | 6570/10570 [00:23<00:14, 271.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 67%|██████▋   | 7081/10570 [00:24<00:12, 287.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 66%|██████▌   | 6931/10570 [00:24<00:13, 278.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 66%|██████▌   | 6989/10570 [00:24<00:12, 281.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 68%|██████▊   | 7191/10570 [00:24<00:11, 290.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 67%|██████▋   | 7091/10570 [00:24<00:12, 288.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 66%|██████▌   | 6994/10570 [00:24<00:12, 285.47it/s][1,mpirank:1,algo-1]<stderr>:#015 68%|██████▊   | 7140/10570 [00:24<00:11, 289.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 67%|██████▋   | 7038/10570 [00:24<00:12, 289.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 67%|██████▋   | 7119/10570 [00:24<00:11, 288.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 6751/10570 [00:24<00:13, 275.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 67%|██████▋   | 7062/10570 [00:24<00:12, 286.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 64%|██████▍   | 6804/10570 [00:24<00:13, 275.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 63%|██████▎   | 6637/10570 [00:24<00:14, 272.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 64%|██████▎   | 6714/10570 [00:24<00:14, 272.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 67%|██████▋   | 7122/10570 [00:24<00:12, 286.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 62%|██████▏   | 6598/10570 [00:24<00:14, 268.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 67%|██████▋   | 7111/10570 [00:24<00:11, 288.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 66%|██████▌   | 6960/10570 [00:24<00:12, 280.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 66%|██████▋   | 7018/10570 [00:24<00:12, 283.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 68%|██████▊   | 7221/10570 [00:24<00:11, 289.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 67%|██████▋   | 7121/10570 [00:24<00:11, 289.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 68%|██████▊   | 7169/10570 [00:24<00:11, 289.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 67%|██████▋   | 7067/10570 [00:25<00:12, 289.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 66%|██████▋   | 7023/10570 [00:24<00:12, 285.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 68%|██████▊   | 7149/10570 [00:24<00:11, 289.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 6779/10570 [00:24<00:13, 276.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 67%|██████▋   | 7091/10570 [00:24<00:12, 287.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 65%|██████▍   | 6832/10570 [00:24<00:13, 273.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 63%|██████▎   | 6665/10570 [00:24<00:14, 273.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 64%|██████▍   | 6742/10570 [00:24<00:13, 274.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 68%|██████▊   | 7151/10570 [00:24<00:11, 287.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 63%|██████▎   | 6626/10570 [00:24<00:14, 271.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 66%|██████▌   | 6989/10570 [00:24<00:12, 282.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 68%|██████▊   | 7141/10570 [00:24<00:11, 289.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 67%|██████▋   | 7047/10570 [00:24<00:12, 284.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 69%|██████▊   | 7251/10570 [00:24<00:11, 289.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 68%|██████▊   | 7151/10570 [00:24<00:11, 289.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 67%|██████▋   | 7096/10570 [00:25<00:12, 289.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 68%|██████▊   | 7198/10570 [00:25<00:11, 288.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 67%|██████▋   | 7052/10570 [00:24<00:12, 284.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 68%|██████▊   | 7178/10570 [00:25<00:11, 288.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|██████▍   | 6807/10570 [00:24<00:13, 275.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 67%|██████▋   | 7120/10570 [00:25<00:12, 287.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 65%|██████▍   | 6860/10570 [00:24<00:13, 273.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 63%|██████▎   | 6693/10570 [00:24<00:14, 271.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 64%|██████▍   | 6771/10570 [00:24<00:13, 276.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 68%|██████▊   | 7180/10570 [00:25<00:11, 286.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 63%|██████▎   | 6654/10570 [00:24<00:14, 271.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 66%|██████▋   | 7018/10570 [00:24<00:12, 283.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 68%|██████▊   | 7170/10570 [00:24<00:11, 289.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 67%|██████▋   | 7076/10570 [00:25<00:12, 283.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 69%|██████▉   | 7280/10570 [00:25<00:11, 286.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 68%|██████▊   | 7180/10570 [00:25<00:11, 288.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 68%|██████▊   | 7227/10570 [00:25<00:11, 287.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 67%|██████▋   | 7126/10570 [00:25<00:11, 289.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 67%|██████▋   | 7081/10570 [00:24<00:12, 283.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 68%|██████▊   | 7207/10570 [00:25<00:11, 286.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▍   | 6835/10570 [00:24<00:13, 274.15it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 68%|██████▊   | 7150/10570 [00:25<00:11, 288.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 65%|██████▌   | 6888/10570 [00:24<00:13, 272.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 64%|██████▎   | 6721/10570 [00:24<00:14, 269.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 64%|██████▍   | 6799/10570 [00:24<00:13, 275.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 68%|██████▊   | 7209/10570 [00:25<00:11, 285.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 67%|██████▋   | 7047/10570 [00:24<00:12, 283.14it/s][1,mpirank:12,algo-2]<stderr>:#015 63%|██████▎   | 6682/10570 [00:24<00:14, 271.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 68%|██████▊   | 7199/10570 [00:25<00:11, 287.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 67%|██████▋   | 7105/10570 [00:25<00:12, 284.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 69%|██████▉   | 7310/10570 [00:25<00:11, 288.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 68%|██████▊   | 7209/10570 [00:25<00:11, 286.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 68%|██████▊   | 7156/10570 [00:25<00:11, 290.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 67%|██████▋   | 7110/10570 [00:24<00:12, 285.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 69%|██████▊   | 7256/10570 [00:25<00:11, 284.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 68%|██████▊   | 7236/10570 [00:25<00:11, 285.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▍   | 6863/10570 [00:24<00:13, 274.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 68%|██████▊   | 7179/10570 [00:25<00:11, 287.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 65%|██████▌   | 6916/10570 [00:24<00:13, 272.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 64%|██████▍   | 6749/10570 [00:24<00:14, 271.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 65%|██████▍   | 6827/10570 [00:24<00:13, 273.72it/s][1,mpirank:5,algo-1]<stderr>:#015 68%|██████▊   | 7238/10570 [00:25<00:11, 285.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 68%|██████▊   | 7228/10570 [00:25<00:11, 287.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 67%|██████▋   | 7076/10570 [00:25<00:12, 282.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 68%|██████▊   | 7135/10570 [00:25<00:11, 286.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 63%|██████▎   | 6710/10570 [00:24<00:14, 267.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 68%|██████▊   | 7238/10570 [00:25<00:11, 285.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 68%|██████▊   | 7139/10570 [00:25<00:11, 286.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 69%|██████▉   | 7285/10570 [00:25<00:11, 284.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 68%|██████▊   | 7186/10570 [00:25<00:11, 289.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 69%|██████▊   | 7265/10570 [00:25<00:11, 285.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 6891/10570 [00:24<00:13, 274.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 68%|██████▊   | 7208/10570 [00:25<00:11, 286.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 69%|██████▉   | 7339/10570 [00:25<00:12, 266.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 66%|██████▌   | 6945/10570 [00:24<00:13, 275.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 64%|██████▍   | 6778/10570 [00:24<00:13, 274.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 65%|██████▍   | 6855/10570 [00:24<00:13, 273.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 69%|██████▉   | 7267/10570 [00:25<00:11, 283.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 69%|██████▊   | 7257/10570 [00:25<00:11, 286.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 67%|██████▋   | 7105/10570 [00:25<00:12, 283.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 68%|██████▊   | 7164/10570 [00:25<00:11, 287.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 64%|██████▎   | 6738/10570 [00:24<00:14, 269.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 69%|██████▉   | 7267/10570 [00:25<00:11, 285.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 68%|██████▊   | 7168/10570 [00:25<00:11, 285.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 69%|██████▉   | 7315/10570 [00:25<00:11, 286.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 68%|██████▊   | 7215/10570 [00:25<00:11, 288.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|██████▌   | 6919/10570 [00:24<00:13, 274.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 70%|██████▉   | 7367/10570 [00:25<00:11, 269.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 68%|██████▊   | 7237/10570 [00:25<00:11, 284.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 69%|██████▉   | 7294/10570 [00:25<00:11, 274.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 66%|██████▌   | 6973/10570 [00:25<00:13, 273.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 64%|██████▍   | 6806/10570 [00:24<00:13, 273.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 69%|██████▉   | 7296/10570 [00:25<00:11, 284.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 65%|██████▌   | 6883/10570 [00:24<00:13, 272.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 69%|██████▉   | 7286/10570 [00:25<00:11, 286.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 67%|██████▋   | 7134/10570 [00:25<00:12, 283.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 68%|██████▊   | 7193/10570 [00:25<00:11, 285.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 64%|██████▍   | 6766/10570 [00:24<00:13, 272.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 69%|██████▉   | 7296/10570 [00:25<00:11, 286.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 68%|██████▊   | 7197/10570 [00:25<00:11, 284.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 69%|██████▊   | 7244/10570 [00:25<00:11, 287.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 6948/10570 [00:24<00:13, 277.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 69%|██████▊   | 7266/10570 [00:25<00:11, 284.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 70%|██████▉   | 7396/10570 [00:25<00:11, 273.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 69%|██████▉   | 7323/10570 [00:25<00:11, 277.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 66%|██████▌   | 7002/10570 [00:25<00:12, 275.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 65%|██████▍   | 6834/10570 [00:24<00:13, 271.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 69%|██████▉   | 7325/10570 [00:25<00:11, 285.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 69%|██████▉   | 7344/10570 [00:25<00:12, 258.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 65%|██████▌   | 6911/10570 [00:24<00:13, 270.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 69%|██████▉   | 7315/10570 [00:25<00:11, 287.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 68%|██████▊   | 7163/10570 [00:25<00:12, 282.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 68%|██████▊   | 7222/10570 [00:25<00:11, 283.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 64%|██████▍   | 6794/10570 [00:24<00:13, 270.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 69%|██████▉   | 7273/10570 [00:25<00:11, 287.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 68%|██████▊   | 7226/10570 [00:25<00:11, 283.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 69%|██████▉   | 7325/10570 [00:25<00:11, 274.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 69%|██████▉   | 7295/10570 [00:25<00:11, 285.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 70%|███████   | 7425/10570 [00:25<00:11, 275.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▌   | 6976/10570 [00:25<00:13, 270.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 67%|██████▋   | 7031/10570 [00:25<00:12, 276.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 65%|██████▍   | 6862/10570 [00:25<00:13, 271.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 70%|██████▉   | 7374/10570 [00:25<00:11, 267.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 66%|██████▌   | 6940/10570 [00:24<00:13, 274.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 70%|██████▉   | 7351/10570 [00:25<00:12, 253.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 68%|██████▊   | 7192/10570 [00:25<00:12, 281.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 69%|██████▊   | 7251/10570 [00:25<00:11, 281.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 65%|██████▍   | 6822/10570 [00:24<00:13, 269.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 70%|██████▉   | 7354/10570 [00:25<00:12, 258.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 69%|██████▉   | 7344/10570 [00:25<00:12, 258.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 69%|██████▉   | 7302/10570 [00:25<00:11, 288.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 69%|██████▊   | 7255/10570 [00:25<00:11, 283.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 69%|██████▉   | 7324/10570 [00:25<00:11, 285.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 71%|███████   | 7454/10570 [00:25<00:11, 279.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|██████▋   | 7005/10570 [00:25<00:13, 274.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 67%|██████▋   | 7059/10570 [00:25<00:12, 276.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 65%|██████▌   | 6890/10570 [00:25<00:13, 271.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 70%|███████   | 7403/10570 [00:25<00:11, 271.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 66%|██████▌   | 6968/10570 [00:25<00:13, 275.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 70%|██████▉   | 7380/10570 [00:25<00:12, 262.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 70%|██████▉   | 7353/10570 [00:25<00:13, 240.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 68%|██████▊   | 7221/10570 [00:25<00:11, 280.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 65%|██████▍   | 6849/10570 [00:25<00:13, 268.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 70%|██████▉   | 7383/10570 [00:25<00:12, 265.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 69%|██████▉   | 7280/10570 [00:25<00:11, 278.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 70%|██████▉   | 7374/10570 [00:25<00:11, 268.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 69%|██████▉   | 7331/10570 [00:26<00:11, 287.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 69%|██████▉   | 7284/10570 [00:25<00:11, 282.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 71%|███████   | 7483/10570 [00:25<00:10, 281.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 7034/10570 [00:25<00:12, 276.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 67%|██████▋   | 7087/10570 [00:25<00:12, 276.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 65%|██████▌   | 6918/10570 [00:25<00:13, 270.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 70%|███████   | 7432/10570 [00:25<00:11, 276.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 66%|██████▌   | 6997/10570 [00:25<00:12, 277.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 70%|██████▉   | 7353/10570 [00:25<00:12, 258.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 70%|███████   | 7409/10570 [00:25<00:11, 268.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 70%|██████▉   | 7379/10570 [00:25<00:13, 243.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 69%|██████▊   | 7250/10570 [00:25<00:11, 280.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 65%|██████▌   | 6876/10570 [00:25<00:13, 268.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 70%|███████   | 7412/10570 [00:25<00:11, 271.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 69%|██████▉   | 7309/10570 [00:25<00:11, 281.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 70%|███████   | 7402/10570 [00:25<00:11, 271.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 69%|██████▉   | 7313/10570 [00:25<00:11, 281.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 71%|███████   | 7513/10570 [00:25<00:10, 284.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 7062/10570 [00:25<00:12, 273.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 67%|██████▋   | 7116/10570 [00:25<00:12, 278.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 70%|██████▉   | 7360/10570 [00:26<00:12, 260.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 71%|███████   | 7461/10570 [00:26<00:11, 278.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 66%|██████▋   | 7025/10570 [00:25<00:12, 278.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 70%|██████▉   | 7381/10570 [00:25<00:12, 262.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 66%|██████▌   | 6946/10570 [00:25<00:13, 263.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 70%|███████   | 7438/10570 [00:26<00:11, 273.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 70%|███████   | 7404/10570 [00:25<00:13, 240.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 69%|██████▉   | 7279/10570 [00:25<00:11, 280.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 65%|██████▌   | 6904/10570 [00:25<00:13, 268.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 70%|███████   | 7441/10570 [00:26<00:11, 274.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 70%|███████   | 7431/10570 [00:25<00:11, 276.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 69%|██████▉   | 7338/10570 [00:25<00:12, 262.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 71%|███████▏  | 7542/10570 [00:25<00:10, 282.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 7090/10570 [00:25<00:12, 275.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 68%|██████▊   | 7145/10570 [00:25<00:12, 279.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 70%|██████▉   | 7389/10570 [00:26<00:11, 266.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 71%|███████   | 7490/10570 [00:26<00:10, 281.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 67%|██████▋   | 7053/10570 [00:25<00:12, 277.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 69%|██████▉   | 7342/10570 [00:25<00:12, 253.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 70%|███████   | 7410/10570 [00:26<00:11, 269.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 66%|██████▌   | 6974/10570 [00:25<00:13, 266.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 71%|███████   | 7467/10570 [00:26<00:11, 276.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 70%|███████   | 7430/10570 [00:26<00:12, 243.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 69%|██████▉   | 7308/10570 [00:25<00:11, 282.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 66%|██████▌   | 6932/10570 [00:25<00:13, 270.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 71%|███████   | 7470/10570 [00:26<00:11, 276.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 71%|███████   | 7460/10570 [00:25<00:11, 278.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 70%|██████▉   | 7365/10570 [00:26<00:12, 262.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 72%|███████▏  | 7572/10570 [00:26<00:10, 285.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|██████▋   | 7119/10570 [00:25<00:12, 277.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 68%|██████▊   | 7173/10570 [00:25<00:12, 276.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 70%|███████   | 7418/10570 [00:26<00:11, 272.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 71%|███████   | 7520/10570 [00:26<00:10, 284.21it/s][1,mpirank:11,algo-2]<stderr>:#015 67%|██████▋   | 7081/10570 [00:25<00:12, 277.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 70%|██████▉   | 7372/10570 [00:25<00:12, 264.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 70%|███████   | 7439/10570 [00:26<00:11, 274.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 66%|██████▌   | 7002/10570 [00:25<00:13, 269.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 71%|███████   | 7496/10570 [00:26<00:11, 279.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 71%|███████   | 7457/10570 [00:26<00:12, 250.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 66%|██████▌   | 6960/10570 [00:25<00:13, 272.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 71%|███████   | 7499/10570 [00:26<00:10, 280.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 71%|███████   | 7489/10570 [00:26<00:10, 281.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 69%|██████▉   | 7337/10570 [00:25<00:12, 264.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 70%|██████▉   | 7393/10570 [00:26<00:11, 266.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 72%|███████▏  | 7602/10570 [00:26<00:10, 288.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 7148/10570 [00:25<00:12, 279.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 68%|██████▊   | 7201/10570 [00:25<00:12, 276.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 67%|██████▋   | 7110/10570 [00:25<00:12, 278.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 70%|███████   | 7400/10570 [00:26<00:11, 268.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 71%|███████▏  | 7549/10570 [00:26<00:10, 281.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 67%|██████▋   | 7030/10570 [00:25<00:13, 271.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 71%|███████   | 7468/10570 [00:26<00:11, 276.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 70%|███████   | 7446/10570 [00:26<00:11, 262.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 71%|███████   | 7526/10570 [00:26<00:10, 282.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 71%|███████   | 7486/10570 [00:26<00:11, 259.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 66%|██████▌   | 6988/10570 [00:25<00:13, 273.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 71%|███████   | 7529/10570 [00:26<00:10, 283.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 71%|███████   | 7519/10570 [00:26<00:10, 284.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 70%|███████   | 7422/10570 [00:26<00:11, 272.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 70%|██████▉   | 7364/10570 [00:26<00:12, 259.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 72%|███████▏  | 7632/10570 [00:26<00:10, 289.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 7176/10570 [00:25<00:12, 279.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 68%|██████▊   | 7229/10570 [00:25<00:12, 275.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 68%|██████▊   | 7139/10570 [00:25<00:12, 279.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 70%|███████   | 7429/10570 [00:26<00:11, 273.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 71%|███████   | 7497/10570 [00:26<00:10, 280.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 72%|███████▏  | 7579/10570 [00:26<00:10, 284.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 67%|██████▋   | 7058/10570 [00:25<00:12, 272.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 71%|███████   | 7473/10570 [00:26<00:11, 263.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 71%|███████▏  | 7555/10570 [00:26<00:10, 283.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 71%|███████   | 7515/10570 [00:26<00:11, 268.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 66%|██████▋   | 7016/10570 [00:25<00:12, 274.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 72%|███████▏  | 7558/10570 [00:26<00:10, 281.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 71%|███████▏  | 7548/10570 [00:26<00:10, 285.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 70%|███████   | 7451/10570 [00:26<00:11, 275.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 70%|██████▉   | 7392/10570 [00:26<00:12, 264.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 7204/10570 [00:25<00:12, 277.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 69%|██████▊   | 7257/10570 [00:26<00:12, 275.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 72%|███████▏  | 7661/10570 [00:26<00:10, 275.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 68%|██████▊   | 7167/10570 [00:25<00:12, 279.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 71%|███████   | 7458/10570 [00:26<00:11, 276.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 72%|███████▏  | 7608/10570 [00:26<00:10, 285.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 71%|███████   | 7527/10570 [00:26<00:10, 283.60it/s][1,mpirank:8,algo-2]<stderr>:#015 67%|██████▋   | 7086/10570 [00:25<00:12, 273.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 71%|███████   | 7503/10570 [00:26<00:11, 271.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 72%|███████▏  | 7584/10570 [00:26<00:10, 284.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 71%|███████▏  | 7544/10570 [00:26<00:11, 273.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 67%|██████▋   | 7044/10570 [00:25<00:12, 273.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 72%|███████▏  | 7587/10570 [00:26<00:10, 283.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 72%|███████▏  | 7577/10570 [00:26<00:10, 285.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 71%|███████   | 7480/10570 [00:26<00:11, 277.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 70%|███████   | 7421/10570 [00:26<00:11, 270.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|██████▊   | 7232/10570 [00:26<00:12, 276.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 69%|██████▉   | 7285/10570 [00:26<00:11, 275.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 73%|███████▎  | 7691/10570 [00:26<00:10, 281.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 68%|██████▊   | 7195/10570 [00:25<00:12, 278.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 72%|███████▏  | 7637/10570 [00:26<00:10, 286.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 71%|███████   | 7487/10570 [00:26<00:11, 279.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 71%|███████▏  | 7556/10570 [00:26<00:10, 283.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 67%|██████▋   | 7115/10570 [00:25<00:12, 275.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 72%|███████▏  | 7613/10570 [00:26<00:10, 285.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 71%|███████▏  | 7533/10570 [00:26<00:10, 278.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 72%|███████▏  | 7573/10570 [00:26<00:10, 277.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 67%|██████▋   | 7072/10570 [00:25<00:12, 272.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 72%|███████▏  | 7616/10570 [00:26<00:10, 284.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 72%|███████▏  | 7606/10570 [00:26<00:10, 286.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 71%|███████   | 7509/10570 [00:26<00:10, 280.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 70%|███████   | 7449/10570 [00:26<00:11, 270.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▊   | 7260/10570 [00:26<00:11, 277.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 69%|██████▉   | 7314/10570 [00:26<00:11, 276.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 73%|███████▎  | 7721/10570 [00:26<00:10, 284.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 68%|██████▊   | 7223/10570 [00:25<00:12, 277.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 71%|███████   | 7516/10570 [00:26<00:10, 280.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 72%|███████▏  | 7585/10570 [00:26<00:10, 284.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 72%|███████▏  | 7642/10570 [00:26<00:10, 286.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 72%|███████▏  | 7562/10570 [00:26<00:10, 280.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 68%|██████▊   | 7144/10570 [00:26<00:12, 277.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 73%|███████▎  | 7666/10570 [00:26<00:10, 273.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 72%|███████▏  | 7602/10570 [00:26<00:10, 281.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 67%|██████▋   | 7100/10570 [00:25<00:12, 273.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 72%|███████▏  | 7645/10570 [00:26<00:10, 285.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 72%|███████▏  | 7636/10570 [00:26<00:10, 287.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 71%|███████   | 7477/10570 [00:26<00:11, 273.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 71%|███████▏  | 7538/10570 [00:26<00:10, 278.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 7288/10570 [00:26<00:11, 276.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 69%|██████▊   | 7251/10570 [00:26<00:11, 277.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 72%|███████▏  | 7614/10570 [00:26<00:10, 285.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 71%|███████▏  | 7545/10570 [00:26<00:10, 280.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 73%|███████▎  | 7750/10570 [00:26<00:10, 273.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 68%|██████▊   | 7172/10570 [00:26<00:12, 277.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 72%|███████▏  | 7592/10570 [00:26<00:10, 283.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 73%|███████▎  | 7696/10570 [00:26<00:10, 278.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 72%|███████▏  | 7631/10570 [00:26<00:10, 283.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 73%|███████▎  | 7671/10570 [00:26<00:10, 272.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 69%|██████▉   | 7342/10570 [00:26<00:12, 248.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 67%|██████▋   | 7128/10570 [00:26<00:12, 274.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 73%|███████▎  | 7674/10570 [00:26<00:10, 272.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 71%|███████   | 7506/10570 [00:26<00:11, 275.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 72%|███████▏  | 7567/10570 [00:26<00:10, 280.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 73%|███████▎  | 7665/10570 [00:26<00:10, 273.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 7317/10570 [00:26<00:11, 278.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 69%|██████▉   | 7279/10570 [00:26<00:11, 276.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 72%|███████▏  | 7643/10570 [00:26<00:10, 286.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 72%|███████▏  | 7574/10570 [00:26<00:10, 283.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 72%|███████▏  | 7621/10570 [00:27<00:10, 285.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 74%|███████▎  | 7779/10570 [00:26<00:10, 276.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 68%|██████▊   | 7200/10570 [00:26<00:12, 276.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 73%|███████▎  | 7725/10570 [00:26<00:10, 281.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 73%|███████▎  | 7701/10570 [00:26<00:10, 278.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 70%|██████▉   | 7371/10570 [00:26<00:12, 257.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 68%|██████▊   | 7156/10570 [00:26<00:12, 274.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 72%|███████▏  | 7660/10570 [00:26<00:10, 270.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 73%|███████▎  | 7704/10570 [00:26<00:10, 277.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 72%|███████▏  | 7596/10570 [00:26<00:10, 282.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 71%|███████▏  | 7535/10570 [00:26<00:10, 277.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 73%|███████▎  | 7695/10570 [00:26<00:10, 279.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 69%|██████▉   | 7308/10570 [00:26<00:11, 277.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 72%|███████▏  | 7603/10570 [00:26<00:10, 283.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 74%|███████▍  | 7808/10570 [00:26<00:09, 279.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 72%|███████▏  | 7651/10570 [00:27<00:10, 287.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 68%|██████▊   | 7228/10570 [00:26<00:12, 275.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 73%|███████▎  | 7672/10570 [00:27<00:10, 272.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 73%|███████▎  | 7730/10570 [00:27<00:10, 281.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 70%|██████▉   | 7398/10570 [00:26<00:12, 261.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|██████▉   | 7345/10570 [00:26<00:13, 242.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 68%|██████▊   | 7184/10570 [00:26<00:12, 273.86it/s][1,mpirank:1,algo-1]<stderr>:#015 73%|███████▎  | 7754/10570 [00:27<00:10, 270.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 73%|███████▎  | 7690/10570 [00:26<00:10, 276.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 73%|███████▎  | 7733/10570 [00:27<00:10, 280.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 72%|███████▏  | 7625/10570 [00:26<00:10, 283.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 72%|███████▏  | 7564/10570 [00:26<00:10, 278.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 73%|███████▎  | 7724/10570 [00:26<00:10, 281.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 72%|███████▏  | 7632/10570 [00:26<00:10, 281.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 74%|███████▍  | 7837/10570 [00:27<00:09, 280.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 73%|███████▎  | 7701/10570 [00:27<00:10, 277.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 69%|██████▊   | 7256/10570 [00:26<00:12, 266.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 69%|██████▉   | 7336/10570 [00:26<00:12, 262.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 73%|███████▎  | 7680/10570 [00:27<00:10, 274.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 70%|███████   | 7425/10570 [00:26<00:11, 262.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|██████▉   | 7373/10570 [00:26<00:12, 250.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 74%|███████▎  | 7782/10570 [00:27<00:10, 273.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 68%|██████▊   | 7212/10570 [00:26<00:12, 272.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 73%|███████▎  | 7719/10570 [00:27<00:10, 280.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 73%|███████▎  | 7759/10570 [00:27<00:10, 269.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 72%|███████▏  | 7593/10570 [00:26<00:10, 280.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 72%|███████▏  | 7654/10570 [00:27<00:10, 281.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 73%|███████▎  | 7762/10570 [00:27<00:10, 268.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 73%|███████▎  | 7753/10570 [00:27<00:10, 263.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 74%|███████▍  | 7867/10570 [00:27<00:09, 284.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 73%|███████▎  | 7730/10570 [00:27<00:10, 280.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 69%|██████▉   | 7284/10570 [00:26<00:12, 269.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 73%|███████▎  | 7710/10570 [00:27<00:10, 279.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 72%|███████▏  | 7661/10570 [00:26<00:10, 269.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 71%|███████   | 7453/10570 [00:26<00:11, 266.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 7401/10570 [00:26<00:12, 256.63it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 74%|███████▍  | 7811/10570 [00:27<00:09, 276.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 70%|██████▉   | 7363/10570 [00:26<00:12, 254.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 68%|██████▊   | 7240/10570 [00:26<00:12, 272.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 74%|███████▎  | 7788/10570 [00:27<00:10, 272.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 73%|███████▎  | 7748/10570 [00:27<00:10, 269.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 72%|███████▏  | 7622/10570 [00:27<00:10, 281.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 74%|███████▎  | 7791/10570 [00:27<00:10, 272.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 73%|███████▎  | 7683/10570 [00:27<00:10, 272.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 74%|███████▎  | 7780/10570 [00:27<00:10, 258.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 75%|███████▍  | 7896/10570 [00:27<00:09, 285.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 69%|██████▉   | 7312/10570 [00:26<00:12, 271.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 71%|███████   | 7481/10570 [00:26<00:11, 269.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 73%|███████▎  | 7691/10570 [00:27<00:10, 275.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 73%|███████▎  | 7740/10570 [00:27<00:10, 282.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 70%|██████▉   | 7390/10570 [00:26<00:12, 258.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 74%|███████▍  | 7840/10570 [00:27<00:09, 278.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|███████   | 7428/10570 [00:26<00:12, 256.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 73%|███████▎  | 7759/10570 [00:27<00:10, 268.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 69%|██████▉   | 7268/10570 [00:26<00:12, 271.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 74%|███████▍  | 7817/10570 [00:27<00:09, 275.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 74%|███████▎  | 7776/10570 [00:27<00:10, 271.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 72%|███████▏  | 7651/10570 [00:27<00:10, 281.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 74%|███████▍  | 7820/10570 [00:27<00:10, 274.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 73%|███████▎  | 7711/10570 [00:27<00:10, 274.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 74%|███████▍  | 7808/10570 [00:27<00:10, 263.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 75%|███████▍  | 7926/10570 [00:27<00:09, 289.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 71%|███████   | 7509/10570 [00:27<00:11, 271.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 73%|███████▎  | 7720/10570 [00:27<00:10, 279.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 70%|███████   | 7418/10570 [00:26<00:11, 264.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 74%|███████▍  | 7869/10570 [00:27<00:09, 281.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 74%|███████▎  | 7787/10570 [00:27<00:10, 271.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 69%|██████▉   | 7296/10570 [00:26<00:12, 272.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 74%|███████▍  | 7846/10570 [00:27<00:09, 277.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 74%|███████▎  | 7769/10570 [00:27<00:10, 269.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 74%|███████▍  | 7804/10570 [00:27<00:10, 274.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 7455/10570 [00:26<00:12, 247.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 69%|██████▉   | 7340/10570 [00:26<00:13, 246.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 74%|███████▍  | 7849/10570 [00:27<00:09, 277.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 73%|███████▎  | 7740/10570 [00:27<00:10, 278.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 73%|███████▎  | 7680/10570 [00:27<00:10, 269.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 74%|███████▍  | 7836/10570 [00:27<00:10, 267.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 75%|███████▌  | 7956/10570 [00:27<00:09, 289.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 71%|███████▏  | 7537/10570 [00:27<00:11, 273.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 70%|███████   | 7446/10570 [00:26<00:11, 267.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 75%|███████▍  | 7898/10570 [00:27<00:09, 282.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 74%|███████▍  | 7816/10570 [00:27<00:10, 274.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 73%|███████▎  | 7749/10570 [00:27<00:10, 268.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 69%|██████▉   | 7324/10570 [00:26<00:11, 271.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 75%|███████▍  | 7875/10570 [00:27<00:09, 280.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 74%|███████▍  | 7798/10570 [00:27<00:10, 273.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 74%|███████▍  | 7832/10570 [00:27<00:09, 274.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 7483/10570 [00:27<00:12, 255.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 70%|██████▉   | 7368/10570 [00:26<00:12, 253.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 75%|███████▍  | 7878/10570 [00:27<00:09, 279.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 73%|███████▎  | 7709/10570 [00:27<00:10, 273.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 73%|███████▎  | 7768/10570 [00:27<00:10, 265.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 74%|███████▍  | 7865/10570 [00:27<00:09, 273.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 76%|███████▌  | 7985/10570 [00:27<00:08, 288.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 72%|███████▏  | 7565/10570 [00:27<00:10, 274.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 71%|███████   | 7474/10570 [00:26<00:11, 269.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 75%|███████▌  | 7928/10570 [00:27<00:09, 286.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 74%|███████▍  | 7845/10570 [00:27<00:09, 277.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 74%|███████▎  | 7777/10570 [00:27<00:10, 271.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 75%|███████▍  | 7904/10570 [00:27<00:09, 282.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 74%|███████▍  | 7827/10570 [00:27<00:09, 276.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████   | 7511/10570 [00:27<00:11, 262.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 74%|███████▍  | 7861/10570 [00:27<00:09, 276.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 70%|██████▉   | 7395/10570 [00:27<00:12, 257.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 75%|███████▍  | 7907/10570 [00:27<00:09, 282.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 70%|██████▉   | 7352/10570 [00:26<00:13, 244.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 73%|███████▎  | 7738/10570 [00:27<00:10, 277.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 74%|███████▍  | 7796/10570 [00:27<00:10, 267.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 75%|███████▍  | 7894/10570 [00:27<00:09, 277.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 76%|███████▌  | 8014/10570 [00:27<00:08, 288.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 72%|███████▏  | 7593/10570 [00:27<00:10, 275.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 71%|███████   | 7502/10570 [00:27<00:11, 271.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 75%|███████▌  | 7957/10570 [00:27<00:09, 286.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 74%|███████▍  | 7874/10570 [00:27<00:09, 280.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 74%|███████▍  | 7806/10570 [00:27<00:10, 274.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 75%|███████▌  | 7934/10570 [00:27<00:09, 285.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 74%|███████▍  | 7856/10570 [00:27<00:09, 279.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|███████▏  | 7539/10570 [00:27<00:11, 267.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 75%|███████▍  | 7890/10570 [00:27<00:09, 279.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 75%|███████▌  | 7937/10570 [00:27<00:09, 284.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 70%|███████   | 7422/10570 [00:27<00:12, 248.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 70%|██████▉   | 7380/10570 [00:27<00:12, 251.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 74%|███████▍  | 7824/10570 [00:27<00:10, 269.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 75%|███████▍  | 7924/10570 [00:27<00:09, 282.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 73%|███████▎  | 7766/10570 [00:27<00:10, 263.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 76%|███████▌  | 8043/10570 [00:27<00:08, 288.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 72%|███████▏  | 7621/10570 [00:27<00:10, 275.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 71%|███████   | 7531/10570 [00:27<00:11, 274.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 76%|███████▌  | 7986/10570 [00:27<00:09, 285.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 75%|███████▍  | 7903/10570 [00:27<00:09, 282.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 74%|███████▍  | 7834/10570 [00:27<00:09, 275.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 75%|███████▌  | 7963/10570 [00:27<00:09, 286.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 75%|███████▍  | 7885/10570 [00:28<00:09, 282.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 7567/10570 [00:27<00:11, 270.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 75%|███████▍  | 7920/10570 [00:27<00:09, 283.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 75%|███████▌  | 7966/10570 [00:27<00:09, 284.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 70%|███████   | 7448/10570 [00:27<00:12, 246.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 70%|███████   | 7408/10570 [00:27<00:12, 256.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 74%|███████▍  | 7852/10570 [00:27<00:10, 271.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 75%|███████▌  | 7953/10570 [00:27<00:09, 283.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 74%|███████▎  | 7794/10570 [00:27<00:10, 267.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 76%|███████▋  | 8072/10570 [00:27<00:08, 281.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 72%|███████▏  | 7649/10570 [00:27<00:10, 276.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 76%|███████▌  | 8015/10570 [00:27<00:08, 285.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 72%|███████▏  | 7559/10570 [00:27<00:10, 274.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 75%|███████▌  | 7933/10570 [00:27<00:09, 284.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 74%|███████▍  | 7863/10570 [00:27<00:09, 278.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 76%|███████▌  | 7992/10570 [00:28<00:09, 284.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 75%|███████▍  | 7915/10570 [00:28<00:09, 285.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 7596/10570 [00:27<00:10, 273.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 75%|███████▌  | 7949/10570 [00:27<00:09, 284.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 76%|███████▌  | 7995/10570 [00:27<00:09, 283.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 71%|███████   | 7473/10570 [00:27<00:12, 244.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 70%|███████   | 7436/10570 [00:27<00:11, 261.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 75%|███████▍  | 7881/10570 [00:27<00:09, 275.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 76%|███████▌  | 7982/10570 [00:27<00:09, 283.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 74%|███████▍  | 7822/10570 [00:27<00:10, 269.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 77%|███████▋  | 8102/10570 [00:27<00:08, 285.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 72%|███████▏  | 7587/10570 [00:27<00:10, 275.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 76%|███████▌  | 8044/10570 [00:28<00:08, 284.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 73%|███████▎  | 7677/10570 [00:27<00:10, 263.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 75%|███████▍  | 7892/10570 [00:27<00:09, 280.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 75%|███████▌  | 7962/10570 [00:28<00:09, 279.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 76%|███████▌  | 8021/10570 [00:28<00:08, 285.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 75%|███████▌  | 7945/10570 [00:28<00:09, 287.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 7624/10570 [00:27<00:10, 275.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 75%|███████▌  | 7978/10570 [00:27<00:09, 283.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 76%|███████▌  | 8024/10570 [00:28<00:08, 284.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 71%|███████   | 7498/10570 [00:27<00:12, 244.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 71%|███████   | 7463/10570 [00:27<00:11, 263.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 75%|███████▍  | 7910/10570 [00:28<00:09, 278.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 76%|███████▌  | 8011/10570 [00:27<00:09, 283.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 74%|███████▍  | 7851/10570 [00:27<00:09, 273.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 77%|███████▋  | 8132/10570 [00:28<00:08, 287.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 72%|███████▏  | 7615/10570 [00:27<00:10, 275.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 73%|███████▎  | 7705/10570 [00:27<00:10, 268.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 76%|███████▋  | 8073/10570 [00:28<00:08, 278.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 75%|███████▍  | 7922/10570 [00:27<00:09, 283.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 76%|███████▌  | 7991/10570 [00:28<00:09, 279.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 76%|███████▌  | 8050/10570 [00:28<00:08, 283.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 75%|███████▌  | 7974/10570 [00:28<00:09, 286.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|███████▏  | 7653/10570 [00:27<00:10, 277.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 76%|███████▌  | 8007/10570 [00:28<00:09, 283.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 76%|███████▌  | 8053/10570 [00:28<00:08, 281.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 71%|███████   | 7523/10570 [00:27<00:12, 245.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 71%|███████   | 7491/10570 [00:27<00:11, 266.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 75%|███████▌  | 7939/10570 [00:28<00:09, 281.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 76%|███████▌  | 8040/10570 [00:28<00:08, 284.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 75%|███████▍  | 7880/10570 [00:27<00:09, 275.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 77%|███████▋  | 8161/10570 [00:28<00:08, 286.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 72%|███████▏  | 7643/10570 [00:27<00:10, 276.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 73%|███████▎  | 7733/10570 [00:27<00:10, 271.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 75%|███████▌  | 7951/10570 [00:28<00:09, 284.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 77%|███████▋  | 8103/10570 [00:28<00:08, 282.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 76%|███████▌  | 8020/10570 [00:28<00:09, 281.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 76%|███████▋  | 8079/10570 [00:28<00:08, 284.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 76%|███████▌  | 8003/10570 [00:28<00:09, 284.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 76%|███████▌  | 8036/10570 [00:28<00:08, 284.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 7681/10570 [00:27<00:10, 264.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 76%|███████▋  | 8082/10570 [00:28<00:08, 283.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 71%|███████▏  | 7548/10570 [00:27<00:12, 243.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 71%|███████   | 7519/10570 [00:27<00:11, 268.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 75%|███████▌  | 7968/10570 [00:28<00:09, 282.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 76%|███████▋  | 8069/10570 [00:28<00:08, 282.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 75%|███████▍  | 7909/10570 [00:28<00:09, 278.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 77%|███████▋  | 8190/10570 [00:28<00:08, 286.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 75%|███████▌  | 7980/10570 [00:28<00:09, 283.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 77%|███████▋  | 8133/10570 [00:28<00:08, 284.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 77%|███████▋  | 8109/10570 [00:28<00:08, 287.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 76%|███████▌  | 8032/10570 [00:28<00:08, 285.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 76%|███████▌  | 8049/10570 [00:28<00:09, 280.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 73%|███████▎  | 7671/10570 [00:27<00:11, 261.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 73%|███████▎  | 7761/10570 [00:27<00:10, 258.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 76%|███████▋  | 8065/10570 [00:28<00:08, 280.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 7710/10570 [00:27<00:10, 269.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 77%|███████▋  | 8112/10570 [00:28<00:08, 286.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 72%|███████▏  | 7573/10570 [00:27<00:12, 243.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 71%|███████▏  | 7547/10570 [00:27<00:11, 268.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 76%|███████▌  | 7997/10570 [00:28<00:09, 282.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 77%|███████▋  | 8099/10570 [00:28<00:08, 285.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 75%|███████▌  | 7938/10570 [00:28<00:09, 281.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 78%|███████▊  | 8219/10570 [00:28<00:08, 284.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 76%|███████▌  | 8009/10570 [00:28<00:09, 282.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 77%|███████▋  | 8138/10570 [00:28<00:08, 286.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 76%|███████▋  | 8078/10570 [00:28<00:08, 282.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 77%|███████▋  | 8162/10570 [00:28<00:08, 282.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 76%|███████▋  | 8061/10570 [00:28<00:08, 283.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 73%|███████▎  | 7700/10570 [00:27<00:10, 267.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 74%|███████▎  | 7788/10570 [00:28<00:10, 261.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 77%|███████▋  | 8095/10570 [00:28<00:08, 283.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 7739/10570 [00:27<00:10, 273.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 77%|███████▋  | 8141/10570 [00:28<00:08, 285.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 72%|███████▏  | 7598/10570 [00:27<00:12, 244.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 72%|███████▏  | 7575/10570 [00:27<00:11, 270.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 76%|███████▌  | 8026/10570 [00:28<00:08, 282.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 77%|███████▋  | 8128/10570 [00:28<00:08, 286.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 75%|███████▌  | 7967/10570 [00:28<00:09, 281.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 78%|███████▊  | 8248/10570 [00:28<00:08, 284.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 76%|███████▌  | 8038/10570 [00:28<00:08, 283.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 77%|███████▋  | 8108/10570 [00:28<00:08, 285.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 77%|███████▋  | 8167/10570 [00:28<00:08, 284.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 77%|███████▋  | 8191/10570 [00:28<00:08, 281.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 73%|███████▎  | 7728/10570 [00:27<00:10, 270.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 77%|███████▋  | 8091/10570 [00:28<00:08, 286.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 74%|███████▍  | 7816/10570 [00:28<00:10, 264.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 77%|███████▋  | 8124/10570 [00:28<00:08, 285.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 77%|███████▋  | 8170/10570 [00:28<00:08, 283.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|███████▎  | 7767/10570 [00:28<00:10, 259.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 72%|███████▏  | 7623/10570 [00:27<00:12, 244.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 72%|███████▏  | 7603/10570 [00:27<00:10, 271.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 76%|███████▌  | 8055/10570 [00:28<00:08, 280.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 77%|███████▋  | 8157/10570 [00:28<00:08, 283.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 76%|███████▌  | 7996/10570 [00:28<00:09, 280.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 78%|███████▊  | 8277/10570 [00:28<00:08, 271.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 77%|███████▋  | 8137/10570 [00:28<00:08, 285.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 76%|███████▋  | 8067/10570 [00:28<00:08, 281.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 78%|███████▊  | 8196/10570 [00:28<00:08, 282.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 77%|███████▋  | 8121/10570 [00:28<00:08, 288.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 78%|███████▊  | 8220/10570 [00:28<00:08, 280.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 74%|███████▍  | 7844/10570 [00:28<00:10, 267.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 77%|███████▋  | 8153/10570 [00:28<00:08, 282.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 73%|███████▎  | 7756/10570 [00:27<00:10, 259.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 78%|███████▊  | 8199/10570 [00:28<00:08, 282.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▎  | 7794/10570 [00:28<00:10, 262.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 72%|███████▏  | 7648/10570 [00:28<00:11, 244.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 72%|███████▏  | 7631/10570 [00:27<00:10, 272.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 76%|███████▋  | 8084/10570 [00:28<00:08, 282.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 77%|███████▋  | 8186/10570 [00:28<00:08, 283.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 76%|███████▌  | 8025/10570 [00:28<00:09, 280.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 79%|███████▊  | 8305/10570 [00:28<00:08, 273.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 77%|███████▋  | 8166/10570 [00:28<00:08, 283.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 77%|███████▋  | 8097/10570 [00:28<00:08, 283.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 77%|███████▋  | 8150/10570 [00:28<00:08, 286.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 78%|███████▊  | 8225/10570 [00:28<00:08, 281.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 78%|███████▊  | 8249/10570 [00:28<00:08, 281.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 74%|███████▍  | 7872/10570 [00:28<00:09, 270.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 74%|███████▎  | 7783/10570 [00:28<00:10, 262.08it/s][1,mpirank:14,algo-2]<stderr>:#015 77%|███████▋  | 8182/10570 [00:28<00:08, 281.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 78%|███████▊  | 8228/10570 [00:28<00:08, 281.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 7822/10570 [00:28<00:10, 265.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 77%|███████▋  | 8114/10570 [00:28<00:08, 285.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 78%|███████▊  | 8215/10570 [00:28<00:08, 281.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 73%|███████▎  | 7673/10570 [00:28<00:12, 230.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 72%|███████▏  | 7659/10570 [00:28<00:11, 256.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 76%|███████▌  | 8054/10570 [00:28<00:09, 276.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 79%|███████▉  | 8334/10570 [00:28<00:08, 276.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 77%|███████▋  | 8126/10570 [00:28<00:08, 285.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 78%|███████▊  | 8195/10570 [00:28<00:08, 281.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 75%|███████▍  | 7900/10570 [00:28<00:09, 272.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 77%|███████▋  | 8179/10570 [00:29<00:08, 284.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 78%|███████▊  | 8254/10570 [00:28<00:08, 280.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 74%|███████▍  | 7811/10570 [00:28<00:10, 265.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 78%|███████▊  | 8211/10570 [00:28<00:08, 280.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 78%|███████▊  | 8278/10570 [00:28<00:08, 265.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|███████▍  | 7850/10570 [00:28<00:10, 268.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 78%|███████▊  | 8257/10570 [00:28<00:08, 279.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 77%|███████▋  | 8143/10570 [00:28<00:08, 282.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 73%|███████▎  | 7698/10570 [00:28<00:12, 235.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 78%|███████▊  | 8244/10570 [00:28<00:08, 280.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 73%|███████▎  | 7687/10570 [00:28<00:10, 262.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 76%|███████▋  | 8082/10570 [00:28<00:09, 276.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 79%|███████▉  | 8364/10570 [00:28<00:07, 281.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 75%|███████▌  | 7928/10570 [00:28<00:09, 272.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 77%|███████▋  | 8155/10570 [00:28<00:08, 280.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 78%|███████▊  | 8224/10570 [00:28<00:08, 280.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 78%|███████▊  | 8208/10570 [00:29<00:08, 282.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 74%|███████▍  | 7839/10570 [00:28<00:10, 267.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 78%|███████▊  | 8240/10570 [00:28<00:08, 280.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 79%|███████▊  | 8307/10570 [00:29<00:08, 271.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 78%|███████▊  | 8283/10570 [00:29<00:08, 265.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▍  | 7878/10570 [00:28<00:09, 271.12it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 78%|███████▊  | 8285/10570 [00:29<00:08, 265.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 77%|███████▋  | 8172/10570 [00:28<00:08, 280.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 73%|███████▎  | 7723/10570 [00:28<00:11, 237.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 78%|███████▊  | 8273/10570 [00:28<00:08, 279.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 73%|███████▎  | 7715/10570 [00:28<00:10, 265.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 77%|███████▋  | 8111/10570 [00:28<00:08, 279.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 79%|███████▉  | 8394/10570 [00:28<00:07, 285.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 75%|███████▌  | 7956/10570 [00:28<00:09, 274.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 77%|███████▋  | 8184/10570 [00:28<00:08, 280.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 78%|███████▊  | 8253/10570 [00:29<00:08, 279.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 78%|███████▊  | 8237/10570 [00:29<00:08, 281.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 74%|███████▍  | 7868/10570 [00:28<00:09, 271.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 78%|███████▊  | 8269/10570 [00:29<00:08, 278.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 79%|███████▉  | 8336/10570 [00:29<00:08, 274.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 79%|███████▊  | 8310/10570 [00:29<00:08, 265.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▍  | 7907/10570 [00:28<00:09, 274.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 79%|███████▊  | 8313/10570 [00:29<00:08, 269.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 78%|███████▊  | 8201/10570 [00:29<00:08, 278.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 73%|███████▎  | 7743/10570 [00:28<00:10, 269.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 77%|███████▋  | 8140/10570 [00:28<00:08, 279.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 73%|███████▎  | 7747/10570 [00:28<00:12, 229.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 79%|███████▊  | 8301/10570 [00:28<00:08, 262.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 80%|███████▉  | 8424/10570 [00:29<00:07, 287.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 76%|███████▌  | 7984/10570 [00:28<00:09, 274.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 78%|███████▊  | 8213/10570 [00:28<00:08, 280.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 78%|███████▊  | 8266/10570 [00:29<00:08, 280.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 75%|███████▍  | 7896/10570 [00:28<00:09, 273.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 79%|███████▉  | 8365/10570 [00:29<00:07, 278.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 79%|███████▉  | 8338/10570 [00:29<00:08, 269.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 78%|███████▊  | 8281/10570 [00:29<00:08, 264.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 7936/10570 [00:28<00:09, 277.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 78%|███████▊  | 8297/10570 [00:29<00:08, 265.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 79%|███████▉  | 8341/10570 [00:29<00:08, 272.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 78%|███████▊  | 8229/10570 [00:29<00:08, 278.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 77%|███████▋  | 8168/10570 [00:28<00:08, 278.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 74%|███████▎  | 7771/10570 [00:28<00:12, 228.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 79%|███████▉  | 8330/10570 [00:29<00:08, 268.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 74%|███████▎  | 7771/10570 [00:28<00:10, 254.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 80%|███████▉  | 8454/10570 [00:29<00:07, 291.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 76%|███████▌  | 8012/10570 [00:28<00:09, 274.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 78%|███████▊  | 8242/10570 [00:29<00:08, 280.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 75%|███████▍  | 7925/10570 [00:28<00:09, 276.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 79%|███████▉  | 8367/10570 [00:29<00:08, 274.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 79%|███████▊  | 8310/10570 [00:29<00:08, 271.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 79%|███████▉  | 8395/10570 [00:29<00:07, 282.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 78%|███████▊  | 8295/10570 [00:29<00:08, 268.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|███████▌  | 7964/10570 [00:28<00:09, 277.68it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 79%|███████▉  | 8325/10570 [00:29<00:08, 269.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 79%|███████▉  | 8370/10570 [00:29<00:07, 276.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 78%|███████▊  | 8257/10570 [00:29<00:08, 275.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 78%|███████▊  | 8196/10570 [00:29<00:08, 276.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 74%|███████▍  | 7796/10570 [00:28<00:11, 232.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 79%|███████▉  | 8359/10570 [00:29<00:08, 273.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 74%|███████▍  | 7798/10570 [00:28<00:10, 258.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 80%|████████  | 8484/10570 [00:29<00:07, 291.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 76%|███████▌  | 8040/10570 [00:28<00:09, 274.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 78%|███████▊  | 8271/10570 [00:29<00:08, 278.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 75%|███████▌  | 7953/10570 [00:28<00:09, 277.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 79%|███████▉  | 8338/10570 [00:29<00:08, 273.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 80%|███████▉  | 8424/10570 [00:29<00:07, 284.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 79%|███████▉  | 8397/10570 [00:29<00:07, 280.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 79%|███████▉  | 8324/10570 [00:29<00:08, 273.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 79%|███████▉  | 8354/10570 [00:29<00:08, 275.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 7992/10570 [00:28<00:09, 276.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 79%|███████▉  | 8400/10570 [00:29<00:07, 281.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 78%|███████▊  | 8224/10570 [00:29<00:08, 276.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 74%|███████▍  | 7820/10570 [00:28<00:11, 234.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 79%|███████▉  | 8388/10570 [00:29<00:07, 278.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 78%|███████▊  | 8285/10570 [00:29<00:08, 258.91it/s][1,mpirank:12,algo-2]<stderr>:#015 74%|███████▍  | 7825/10570 [00:28<00:10, 261.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 81%|████████  | 8514/10570 [00:29<00:07, 291.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 76%|███████▋  | 8068/10570 [00:29<00:09, 274.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 76%|███████▌  | 7981/10570 [00:28<00:09, 276.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 79%|███████▉  | 8367/10570 [00:29<00:07, 277.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 80%|███████▉  | 8454/10570 [00:29<00:07, 287.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 80%|███████▉  | 8427/10570 [00:29<00:07, 283.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 79%|███████▊  | 8299/10570 [00:29<00:08, 266.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 79%|███████▉  | 8354/10570 [00:29<00:07, 278.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 79%|███████▉  | 8383/10570 [00:29<00:07, 278.47it/s][1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 8020/10570 [00:28<00:09, 277.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 80%|███████▉  | 8430/10570 [00:29<00:07, 283.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 78%|███████▊  | 8252/10570 [00:29<00:08, 276.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 74%|███████▍  | 7845/10570 [00:28<00:11, 236.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 79%|███████▊  | 8313/10570 [00:29<00:08, 264.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 80%|███████▉  | 8418/10570 [00:29<00:07, 283.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 74%|███████▍  | 7853/10570 [00:28<00:10, 264.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 81%|████████  | 8544/10570 [00:29<00:07, 281.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 77%|███████▋  | 8097/10570 [00:29<00:08, 276.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 76%|███████▌  | 8009/10570 [00:28<00:09, 275.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 80%|████████  | 8483/10570 [00:29<00:07, 288.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 79%|███████▉  | 8397/10570 [00:29<00:07, 281.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 80%|████████  | 8457/10570 [00:29<00:07, 286.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 79%|███████▉  | 8327/10570 [00:29<00:08, 269.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 79%|███████▉  | 8383/10570 [00:29<00:07, 281.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 80%|███████▉  | 8413/10570 [00:29<00:07, 282.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▌  | 8048/10570 [00:29<00:09, 274.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 80%|████████  | 8460/10570 [00:29<00:07, 286.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 74%|███████▍  | 7870/10570 [00:29<00:11, 239.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 79%|███████▉  | 8341/10570 [00:29<00:08, 267.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 80%|███████▉  | 8448/10570 [00:29<00:07, 286.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 75%|███████▍  | 7881/10570 [00:28<00:10, 267.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 78%|███████▊  | 8280/10570 [00:29<00:08, 260.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 81%|████████  | 8573/10570 [00:29<00:07, 283.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 77%|███████▋  | 8125/10570 [00:29<00:08, 277.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 76%|███████▌  | 8037/10570 [00:28<00:09, 276.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 81%|████████  | 8512/10570 [00:29<00:07, 287.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 80%|███████▉  | 8426/10570 [00:29<00:07, 283.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 80%|████████  | 8486/10570 [00:29<00:07, 286.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 79%|███████▉  | 8356/10570 [00:29<00:08, 274.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 80%|███████▉  | 8413/10570 [00:29<00:07, 285.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|███████▋  | 8076/10570 [00:29<00:09, 275.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 80%|███████▉  | 8443/10570 [00:29<00:07, 285.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 80%|████████  | 8489/10570 [00:29<00:07, 286.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 75%|███████▍  | 7895/10570 [00:29<00:11, 240.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 79%|███████▉  | 8370/10570 [00:29<00:08, 272.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 80%|████████  | 8478/10570 [00:29<00:07, 288.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 75%|███████▍  | 7909/10570 [00:28<00:09, 269.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 79%|███████▊  | 8309/10570 [00:29<00:08, 266.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 81%|████████▏ | 8603/10570 [00:29<00:06, 286.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 77%|███████▋  | 8153/10570 [00:29<00:08, 273.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 76%|███████▋  | 8065/10570 [00:29<00:09, 274.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 80%|████████  | 8456/10570 [00:29<00:07, 286.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 81%|████████  | 8515/10570 [00:29<00:07, 286.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 79%|███████▉  | 8385/10570 [00:29<00:07, 277.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 80%|███████▉  | 8443/10570 [00:29<00:07, 287.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 8105/10570 [00:29<00:08, 278.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 81%|████████  | 8542/10570 [00:29<00:07, 277.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 80%|████████  | 8473/10570 [00:29<00:07, 287.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 81%|████████  | 8518/10570 [00:29<00:07, 286.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 75%|███████▍  | 7920/10570 [00:29<00:10, 242.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 79%|███████▉  | 8399/10570 [00:29<00:07, 277.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 75%|███████▌  | 7937/10570 [00:29<00:09, 272.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 80%|████████  | 8507/10570 [00:29<00:07, 286.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 79%|███████▉  | 8337/10570 [00:29<00:08, 268.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 82%|████████▏ | 8633/10570 [00:29<00:06, 288.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 77%|███████▋  | 8181/10570 [00:29<00:08, 273.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 77%|███████▋  | 8094/10570 [00:29<00:08, 276.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 80%|████████  | 8485/10570 [00:29<00:07, 286.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 80%|███████▉  | 8414/10570 [00:29<00:07, 281.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 81%|████████  | 8545/10570 [00:29<00:07, 288.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 80%|████████  | 8473/10570 [00:30<00:07, 290.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 8133/10570 [00:29<00:08, 278.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 81%|████████  | 8571/10570 [00:29<00:07, 279.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 80%|████████  | 8502/10570 [00:29<00:07, 287.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 81%|████████  | 8547/10570 [00:29<00:07, 282.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 80%|███████▉  | 8428/10570 [00:29<00:07, 280.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 75%|███████▌  | 7945/10570 [00:29<00:10, 243.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 75%|███████▌  | 7965/10570 [00:29<00:09, 272.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 81%|████████  | 8537/10570 [00:29<00:07, 288.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 79%|███████▉  | 8366/10570 [00:29<00:08, 272.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 82%|████████▏ | 8662/10570 [00:29<00:06, 287.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 78%|███████▊  | 8209/10570 [00:29<00:08, 272.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 77%|███████▋  | 8123/10570 [00:29<00:08, 278.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 81%|████████  | 8514/10570 [00:30<00:07, 286.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 80%|███████▉  | 8443/10570 [00:29<00:07, 283.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 81%|████████  | 8574/10570 [00:30<00:06, 286.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 80%|████████  | 8503/10570 [00:30<00:07, 289.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 81%|████████▏ | 8600/10570 [00:30<00:06, 281.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 81%|████████  | 8531/10570 [00:29<00:07, 287.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 8161/10570 [00:29<00:08, 275.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 81%|████████  | 8576/10570 [00:30<00:07, 282.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 80%|████████  | 8458/10570 [00:29<00:07, 283.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 75%|███████▌  | 7970/10570 [00:29<00:10, 242.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 81%|████████  | 8566/10570 [00:29<00:06, 288.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 76%|███████▌  | 7993/10570 [00:29<00:09, 270.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 79%|███████▉  | 8395/10570 [00:29<00:07, 277.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 82%|████████▏ | 8692/10570 [00:30<00:06, 289.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 78%|███████▊  | 8237/10570 [00:29<00:08, 272.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 80%|████████  | 8472/10570 [00:29<00:07, 285.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 81%|████████  | 8544/10570 [00:30<00:07, 288.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 77%|███████▋  | 8151/10570 [00:29<00:08, 275.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 81%|████████  | 8533/10570 [00:30<00:07, 290.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 81%|████████▏ | 8603/10570 [00:30<00:07, 281.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 82%|████████▏ | 8629/10570 [00:30<00:06, 283.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 81%|████████  | 8560/10570 [00:30<00:06, 287.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|███████▋  | 8189/10570 [00:29<00:08, 274.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 81%|████████▏ | 8605/10570 [00:30<00:06, 284.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 80%|████████  | 8487/10570 [00:30<00:07, 284.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 81%|████████▏ | 8595/10570 [00:30<00:06, 287.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 76%|███████▌  | 8021/10570 [00:29<00:09, 271.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 80%|███████▉  | 8424/10570 [00:29<00:07, 279.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 76%|███████▌  | 7995/10570 [00:29<00:11, 233.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 83%|████████▎ | 8721/10570 [00:30<00:06, 288.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 78%|███████▊  | 8265/10570 [00:29<00:08, 270.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 80%|████████  | 8501/10570 [00:29<00:07, 286.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 81%|████████  | 8573/10570 [00:30<00:06, 286.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 77%|███████▋  | 8179/10570 [00:29<00:08, 274.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 82%|████████▏ | 8632/10570 [00:30<00:06, 283.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 81%|████████  | 8563/10570 [00:30<00:06, 290.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 82%|████████▏ | 8658/10570 [00:30<00:06, 283.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 81%|████████▏ | 8589/10570 [00:30<00:06, 286.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 82%|████████▏ | 8634/10570 [00:30<00:06, 285.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 8217/10570 [00:29<00:08, 261.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 81%|████████  | 8516/10570 [00:30<00:07, 284.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 82%|████████▏ | 8624/10570 [00:30<00:06, 287.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 80%|███████▉  | 8454/10570 [00:30<00:07, 282.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 76%|███████▌  | 8049/10570 [00:29<00:09, 269.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 76%|███████▌  | 8020/10570 [00:29<00:10, 236.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 83%|████████▎ | 8750/10570 [00:30<00:06, 287.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 81%|████████  | 8530/10570 [00:30<00:07, 287.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 81%|████████▏ | 8602/10570 [00:30<00:06, 287.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 78%|███████▊  | 8207/10570 [00:29<00:08, 273.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 82%|████████▏ | 8661/10570 [00:30<00:06, 282.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 78%|███████▊  | 8293/10570 [00:29<00:08, 256.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 82%|████████▏ | 8618/10570 [00:30<00:06, 286.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 81%|████████▏ | 8593/10570 [00:30<00:06, 288.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 82%|████████▏ | 8688/10570 [00:30<00:06, 285.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 82%|████████▏ | 8663/10570 [00:30<00:06, 284.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 8245/10570 [00:29<00:08, 264.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 82%|████████▏ | 8653/10570 [00:30<00:06, 286.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 80%|████████  | 8483/10570 [00:30<00:07, 283.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 81%|████████  | 8545/10570 [00:30<00:07, 274.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 76%|███████▋  | 8077/10570 [00:29<00:09, 270.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 76%|███████▌  | 8044/10570 [00:29<00:10, 236.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 83%|████████▎ | 8779/10570 [00:30<00:06, 286.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 81%|████████  | 8559/10570 [00:30<00:07, 286.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 82%|████████▏ | 8631/10570 [00:30<00:06, 287.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 78%|███████▊  | 8235/10570 [00:29<00:08, 271.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 82%|████████▏ | 8690/10570 [00:30<00:06, 284.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 79%|███████▊  | 8321/10570 [00:30<00:08, 262.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 82%|████████▏ | 8647/10570 [00:30<00:06, 286.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 82%|████████▏ | 8622/10570 [00:30<00:06, 288.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 82%|████████▏ | 8717/10570 [00:30<00:06, 283.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 82%|████████▏ | 8692/10570 [00:30<00:06, 286.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|███████▊  | 8272/10570 [00:29<00:08, 263.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 82%|████████▏ | 8682/10570 [00:30<00:06, 286.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 81%|████████  | 8512/10570 [00:30<00:07, 282.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 77%|███████▋  | 8105/10570 [00:29<00:09, 272.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 81%|████████  | 8573/10570 [00:30<00:07, 273.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 76%|███████▋  | 8068/10570 [00:29<00:10, 237.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 83%|████████▎ | 8808/10570 [00:30<00:06, 286.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 81%|████████  | 8588/10570 [00:30<00:06, 284.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 82%|████████▏ | 8660/10570 [00:30<00:06, 284.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 82%|████████▏ | 8719/10570 [00:30<00:06, 284.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 79%|███████▉  | 8348/10570 [00:30<00:08, 264.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 78%|███████▊  | 8263/10570 [00:29<00:08, 269.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 82%|████████▏ | 8651/10570 [00:30<00:06, 287.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 82%|████████▏ | 8676/10570 [00:30<00:06, 281.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 83%|████████▎ | 8746/10570 [00:30<00:06, 282.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 83%|████████▎ | 8721/10570 [00:30<00:06, 284.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▊  | 8299/10570 [00:30<00:08, 253.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 82%|████████▏ | 8711/10570 [00:30<00:06, 287.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 81%|████████  | 8541/10570 [00:30<00:07, 284.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 77%|███████▋  | 8133/10570 [00:29<00:08, 272.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 77%|███████▋  | 8093/10570 [00:29<00:10, 240.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 81%|████████▏ | 8601/10570 [00:30<00:07, 271.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 84%|████████▎ | 8837/10570 [00:30<00:06, 287.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 82%|████████▏ | 8617/10570 [00:30<00:06, 284.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 82%|████████▏ | 8689/10570 [00:30<00:06, 284.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 79%|███████▉  | 8376/10570 [00:30<00:08, 267.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 83%|████████▎ | 8748/10570 [00:30<00:06, 283.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 82%|████████▏ | 8681/10570 [00:30<00:06, 288.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 82%|████████▏ | 8705/10570 [00:30<00:06, 281.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 83%|████████▎ | 8775/10570 [00:30<00:06, 281.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 78%|███████▊  | 8290/10570 [00:29<00:08, 256.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 8327/10570 [00:30<00:08, 258.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 83%|████████▎ | 8750/10570 [00:30<00:06, 266.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 83%|████████▎ | 8740/10570 [00:30<00:06, 285.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 82%|████████▏ | 8629/10570 [00:30<00:07, 273.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 81%|████████  | 8570/10570 [00:30<00:07, 279.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 77%|███████▋  | 8161/10570 [00:29<00:08, 269.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 84%|████████▍ | 8866/10570 [00:30<00:05, 287.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 77%|███████▋  | 8118/10570 [00:30<00:10, 235.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 82%|████████▏ | 8718/10570 [00:30<00:06, 284.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 82%|████████▏ | 8646/10570 [00:30<00:06, 282.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 80%|███████▉  | 8405/10570 [00:30<00:07, 271.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 83%|████████▎ | 8777/10570 [00:30<00:06, 282.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 82%|████████▏ | 8711/10570 [00:30<00:06, 289.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 83%|████████▎ | 8734/10570 [00:30<00:06, 280.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 83%|████████▎ | 8804/10570 [00:30<00:06, 282.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 79%|███████▊  | 8319/10570 [00:30<00:08, 263.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 8355/10570 [00:30<00:08, 264.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 83%|████████▎ | 8778/10570 [00:30<00:06, 270.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 83%|████████▎ | 8769/10570 [00:30<00:06, 284.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 82%|████████▏ | 8657/10570 [00:30<00:06, 274.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 77%|███████▋  | 8188/10570 [00:30<00:08, 269.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 81%|████████▏ | 8599/10570 [00:30<00:07, 280.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 84%|████████▍ | 8895/10570 [00:30<00:05, 285.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 77%|███████▋  | 8142/10570 [00:30<00:10, 230.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 82%|████████▏ | 8675/10570 [00:30<00:06, 283.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 83%|████████▎ | 8747/10570 [00:30<00:06, 280.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 80%|███████▉  | 8434/10570 [00:30<00:07, 275.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 83%|████████▎ | 8806/10570 [00:30<00:06, 282.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 83%|████████▎ | 8740/10570 [00:31<00:06, 286.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 84%|████████▎ | 8833/10570 [00:30<00:06, 283.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 83%|████████▎ | 8763/10570 [00:30<00:06, 281.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 79%|███████▉  | 8346/10570 [00:30<00:08, 265.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|███████▉  | 8383/10570 [00:30<00:08, 268.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 83%|████████▎ | 8807/10570 [00:30<00:06, 274.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 83%|████████▎ | 8798/10570 [00:30<00:06, 283.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 82%|████████▏ | 8686/10570 [00:30<00:06, 278.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 78%|███████▊  | 8215/10570 [00:30<00:08, 268.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 84%|████████▍ | 8924/10570 [00:30<00:05, 286.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 82%|████████▏ | 8628/10570 [00:30<00:06, 281.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 77%|███████▋  | 8166/10570 [00:30<00:10, 232.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 82%|████████▏ | 8704/10570 [00:30<00:06, 284.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 83%|████████▎ | 8776/10570 [00:30<00:06, 278.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 80%|████████  | 8463/10570 [00:30<00:07, 277.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 84%|████████▎ | 8835/10570 [00:31<00:06, 283.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 83%|████████▎ | 8769/10570 [00:31<00:06, 285.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 84%|████████▍ | 8862/10570 [00:31<00:05, 284.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 83%|████████▎ | 8792/10570 [00:30<00:06, 281.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 79%|███████▉  | 8374/10570 [00:30<00:08, 268.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|███████▉  | 8412/10570 [00:30<00:07, 272.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 84%|████████▎ | 8836/10570 [00:30<00:06, 277.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 84%|████████▎ | 8827/10570 [00:30<00:06, 283.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 82%|████████▏ | 8714/10570 [00:30<00:06, 278.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 85%|████████▍ | 8954/10570 [00:30<00:05, 287.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 82%|████████▏ | 8657/10570 [00:30<00:06, 280.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 78%|███████▊  | 8242/10570 [00:30<00:08, 262.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 77%|███████▋  | 8190/10570 [00:30<00:10, 233.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 83%|████████▎ | 8733/10570 [00:30<00:06, 281.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 83%|████████▎ | 8805/10570 [00:31<00:06, 279.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 84%|████████▍ | 8864/10570 [00:31<00:06, 284.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 80%|████████  | 8491/10570 [00:30<00:07, 276.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 83%|████████▎ | 8798/10570 [00:31<00:06, 284.95it/s][1,mpirank:1,algo-1]<stderr>:#015 84%|████████▍ | 8891/10570 [00:31<00:05, 285.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 83%|████████▎ | 8821/10570 [00:30<00:06, 281.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 79%|███████▉  | 8403/10570 [00:30<00:07, 272.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|███████▉  | 8441/10570 [00:30<00:07, 275.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 84%|████████▍ | 8865/10570 [00:31<00:06, 280.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 84%|████████▍ | 8856/10570 [00:30<00:06, 283.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 83%|████████▎ | 8742/10570 [00:31<00:06, 277.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 82%|████████▏ | 8686/10570 [00:30<00:06, 282.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 85%|████████▍ | 8984/10570 [00:31<00:05, 288.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 78%|███████▊  | 8269/10570 [00:30<00:08, 262.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 78%|███████▊  | 8214/10570 [00:30<00:10, 234.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 83%|████████▎ | 8762/10570 [00:30<00:06, 282.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 84%|████████▎ | 8834/10570 [00:31<00:06, 281.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 81%|████████  | 8519/10570 [00:30<00:07, 276.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 84%|████████▍ | 8893/10570 [00:31<00:05, 283.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 84%|████████▍ | 8920/10570 [00:31<00:05, 285.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 84%|████████▎ | 8827/10570 [00:31<00:06, 285.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 84%|████████▎ | 8850/10570 [00:31<00:06, 283.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 80%|███████▉  | 8432/10570 [00:30<00:07, 275.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 8470/10570 [00:30<00:07, 278.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 84%|████████▍ | 8894/10570 [00:31<00:05, 281.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 84%|████████▍ | 8885/10570 [00:31<00:05, 284.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 83%|████████▎ | 8770/10570 [00:31<00:06, 277.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 82%|████████▏ | 8715/10570 [00:30<00:06, 282.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 85%|████████▌ | 9014/10570 [00:31<00:05, 290.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 78%|███████▊  | 8238/10570 [00:30<00:09, 235.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 78%|███████▊  | 8296/10570 [00:30<00:09, 251.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 84%|████████▍ | 8863/10570 [00:31<00:06, 283.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 83%|████████▎ | 8791/10570 [00:31<00:06, 278.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 84%|████████▍ | 8922/10570 [00:31<00:05, 284.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 81%|████████  | 8548/10570 [00:30<00:07, 278.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 85%|████████▍ | 8949/10570 [00:31<00:05, 285.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 84%|████████▍ | 8857/10570 [00:31<00:05, 286.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 84%|████████▍ | 8879/10570 [00:31<00:05, 285.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 80%|████████  | 8461/10570 [00:30<00:07, 277.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|████████  | 8498/10570 [00:30<00:07, 278.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 84%|████████▍ | 8923/10570 [00:31<00:05, 282.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 84%|████████▍ | 8914/10570 [00:31<00:05, 284.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 83%|████████▎ | 8798/10570 [00:31<00:06, 278.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 83%|████████▎ | 8744/10570 [00:31<00:06, 279.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 86%|████████▌ | 9044/10570 [00:31<00:05, 288.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 78%|███████▊  | 8262/10570 [00:30<00:09, 234.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 79%|███████▉  | 8324/10570 [00:30<00:08, 256.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 83%|████████▎ | 8819/10570 [00:31<00:06, 278.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 84%|████████▍ | 8892/10570 [00:31<00:05, 283.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 85%|████████▍ | 8951/10570 [00:31<00:05, 283.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 81%|████████  | 8576/10570 [00:30<00:07, 276.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 85%|████████▍ | 8978/10570 [00:31<00:05, 286.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 84%|████████▍ | 8887/10570 [00:31<00:05, 287.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 84%|████████▍ | 8908/10570 [00:31<00:05, 284.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 80%|████████  | 8489/10570 [00:30<00:07, 277.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 85%|████████▍ | 8952/10570 [00:31<00:05, 283.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 85%|████████▍ | 8943/10570 [00:31<00:05, 283.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 8526/10570 [00:30<00:07, 265.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 84%|████████▎ | 8827/10570 [00:31<00:06, 279.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 86%|████████▌ | 9073/10570 [00:31<00:05, 288.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 83%|████████▎ | 8773/10570 [00:31<00:06, 278.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 79%|███████▉  | 8352/10570 [00:30<00:08, 261.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 78%|███████▊  | 8286/10570 [00:30<00:10, 222.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 84%|████████▎ | 8848/10570 [00:31<00:06, 280.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 81%|████████▏ | 8604/10570 [00:31<00:07, 277.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 84%|████████▍ | 8921/10570 [00:31<00:05, 281.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 85%|████████▍ | 8980/10570 [00:31<00:05, 284.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 85%|████████▌ | 9008/10570 [00:31<00:05, 287.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 85%|████████▍ | 8937/10570 [00:31<00:05, 285.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 84%|████████▍ | 8916/10570 [00:31<00:05, 283.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 81%|████████  | 8517/10570 [00:30<00:07, 277.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 85%|████████▍ | 8981/10570 [00:31<00:05, 284.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 85%|████████▍ | 8972/10570 [00:31<00:05, 284.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 8555/10570 [00:30<00:07, 270.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 84%|████████▍ | 8856/10570 [00:31<00:06, 281.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 86%|████████▌ | 9102/10570 [00:31<00:05, 287.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 83%|████████▎ | 8801/10570 [00:31<00:06, 278.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 79%|███████▉  | 8380/10570 [00:30<00:08, 265.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 79%|███████▊  | 8311/10570 [00:30<00:09, 228.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 84%|████████▍ | 8877/10570 [00:31<00:05, 282.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 82%|████████▏ | 8632/10570 [00:31<00:06, 278.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 85%|████████▌ | 9009/10570 [00:31<00:05, 285.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 85%|████████▍ | 8950/10570 [00:31<00:05, 282.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 85%|████████▌ | 9037/10570 [00:31<00:05, 285.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 85%|████████▍ | 8966/10570 [00:31<00:05, 284.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 81%|████████  | 8546/10570 [00:30<00:07, 278.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 85%|████████▍ | 8945/10570 [00:31<00:05, 274.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 85%|████████▌ | 9010/10570 [00:31<00:05, 285.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 85%|████████▌ | 9001/10570 [00:31<00:05, 285.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████  | 8583/10570 [00:31<00:07, 271.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 84%|████████▍ | 8885/10570 [00:31<00:05, 281.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 86%|████████▋ | 9131/10570 [00:31<00:04, 288.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 84%|████████▎ | 8829/10570 [00:31<00:06, 278.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 80%|███████▉  | 8408/10570 [00:30<00:08, 268.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 79%|███████▉  | 8335/10570 [00:31<00:09, 230.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 84%|████████▍ | 8906/10570 [00:31<00:05, 282.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 82%|████████▏ | 8660/10570 [00:31<00:06, 276.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 85%|████████▍ | 8979/10570 [00:31<00:05, 283.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 86%|████████▌ | 9038/10570 [00:31<00:05, 284.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 86%|████████▌ | 9066/10570 [00:31<00:05, 286.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 85%|████████▌ | 8995/10570 [00:31<00:05, 286.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 81%|████████  | 8574/10570 [00:30<00:07, 277.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 85%|████████▍ | 8974/10570 [00:31<00:05, 278.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 86%|████████▌ | 9039/10570 [00:31<00:05, 283.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 85%|████████▌ | 9030/10570 [00:31<00:05, 285.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|████████▏ | 8611/10570 [00:31<00:07, 273.95it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 84%|████████▍ | 8914/10570 [00:31<00:05, 280.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 87%|████████▋ | 9160/10570 [00:31<00:04, 287.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 84%|████████▍ | 8858/10570 [00:31<00:06, 279.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 80%|███████▉  | 8436/10570 [00:30<00:07, 271.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 79%|███████▉  | 8360/10570 [00:31<00:09, 235.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 85%|████████▌ | 9008/10570 [00:31<00:05, 284.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 85%|████████▍ | 8935/10570 [00:31<00:05, 281.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 86%|████████▌ | 9067/10570 [00:31<00:05, 285.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 82%|████████▏ | 8689/10570 [00:31<00:06, 276.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 86%|████████▌ | 9095/10570 [00:31<00:05, 285.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 85%|████████▌ | 9024/10570 [00:31<00:05, 286.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 81%|████████▏ | 8602/10570 [00:31<00:07, 271.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 85%|████████▌ | 9004/10570 [00:31<00:05, 282.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 86%|████████▌ | 9068/10570 [00:31<00:05, 284.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 86%|████████▌ | 9059/10570 [00:31<00:05, 284.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 8640/10570 [00:31<00:06, 275.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 85%|████████▍ | 8943/10570 [00:31<00:05, 280.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 87%|████████▋ | 9190/10570 [00:31<00:04, 288.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 84%|████████▍ | 8887/10570 [00:31<00:06, 280.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 80%|████████  | 8465/10570 [00:31<00:07, 274.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 79%|███████▉  | 8385/10570 [00:31<00:09, 237.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 85%|████████▌ | 9037/10570 [00:31<00:05, 283.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 86%|████████▌ | 9096/10570 [00:31<00:05, 284.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 85%|████████▍ | 8964/10570 [00:31<00:05, 280.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 82%|████████▏ | 8717/10570 [00:31<00:06, 273.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 86%|████████▌ | 9053/10570 [00:31<00:05, 284.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 86%|████████▋ | 9124/10570 [00:31<00:05, 279.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 82%|████████▏ | 8630/10570 [00:31<00:07, 273.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 85%|████████▌ | 9033/10570 [00:32<00:05, 282.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 86%|████████▌ | 9097/10570 [00:31<00:05, 283.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 86%|████████▌ | 9088/10570 [00:31<00:05, 284.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 8668/10570 [00:31<00:06, 274.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 85%|████████▍ | 8972/10570 [00:31<00:05, 280.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 87%|████████▋ | 9219/10570 [00:31<00:04, 288.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 84%|████████▍ | 8916/10570 [00:31<00:05, 279.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 80%|████████  | 8493/10570 [00:31<00:07, 273.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 80%|███████▉  | 8410/10570 [00:31<00:08, 240.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 86%|████████▌ | 9066/10570 [00:31<00:05, 284.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 86%|████████▋ | 9125/10570 [00:32<00:05, 284.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 85%|████████▌ | 8993/10570 [00:31<00:05, 281.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 83%|████████▎ | 8745/10570 [00:31<00:06, 272.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 86%|████████▌ | 9082/10570 [00:31<00:05, 284.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 87%|████████▋ | 9153/10570 [00:32<00:05, 280.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 82%|████████▏ | 8658/10570 [00:31<00:07, 273.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 86%|████████▌ | 9062/10570 [00:32<00:05, 283.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 86%|████████▋ | 9126/10570 [00:32<00:05, 283.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 86%|████████▋ | 9117/10570 [00:31<00:05, 284.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|████████▏ | 8697/10570 [00:31<00:06, 277.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 85%|████████▌ | 9001/10570 [00:31<00:05, 283.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 88%|████████▊ | 9249/10570 [00:31<00:04, 290.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 85%|████████▍ | 8945/10570 [00:31<00:05, 280.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 81%|████████  | 8521/10570 [00:31<00:07, 274.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 80%|███████▉  | 8435/10570 [00:31<00:08, 242.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 86%|████████▌ | 9095/10570 [00:32<00:05, 284.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 85%|████████▌ | 9022/10570 [00:31<00:05, 283.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 87%|████████▋ | 9154/10570 [00:32<00:04, 283.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 83%|████████▎ | 8773/10570 [00:31<00:06, 271.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 86%|████████▌ | 9111/10570 [00:31<00:05, 284.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 87%|████████▋ | 9182/10570 [00:32<00:04, 283.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 82%|████████▏ | 8687/10570 [00:31<00:06, 275.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 86%|████████▌ | 9091/10570 [00:32<00:05, 284.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 87%|████████▋ | 9155/10570 [00:32<00:05, 282.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 87%|████████▋ | 9146/10570 [00:31<00:05, 284.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 8725/10570 [00:31<00:06, 275.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 85%|████████▌ | 9030/10570 [00:32<00:05, 282.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 88%|████████▊ | 9279/10570 [00:32<00:04, 291.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 85%|████████▍ | 8974/10570 [00:31<00:05, 280.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 81%|████████  | 8549/10570 [00:31<00:07, 275.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 80%|████████  | 8461/10570 [00:31<00:08, 245.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 86%|████████▋ | 9124/10570 [00:32<00:05, 284.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 87%|████████▋ | 9183/10570 [00:32<00:04, 284.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 86%|████████▌ | 9051/10570 [00:31<00:05, 282.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 86%|████████▋ | 9140/10570 [00:32<00:05, 285.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 83%|████████▎ | 8801/10570 [00:31<00:06, 272.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 87%|████████▋ | 9211/10570 [00:32<00:04, 283.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 82%|████████▏ | 8715/10570 [00:31<00:06, 276.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 86%|████████▋ | 9120/10570 [00:32<00:05, 285.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 87%|████████▋ | 9184/10570 [00:32<00:04, 283.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 87%|████████▋ | 9175/10570 [00:32<00:04, 283.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 8753/10570 [00:31<00:06, 270.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 86%|████████▌ | 9059/10570 [00:32<00:05, 280.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 88%|████████▊ | 9309/10570 [00:32<00:04, 290.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 85%|████████▌ | 9003/10570 [00:31<00:05, 281.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 81%|████████  | 8577/10570 [00:31<00:07, 273.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 80%|████████  | 8486/10570 [00:31<00:08, 244.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 87%|████████▋ | 9153/10570 [00:32<00:05, 283.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 86%|████████▌ | 9080/10570 [00:32<00:05, 280.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 87%|████████▋ | 9169/10570 [00:32<00:04, 284.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 84%|████████▎ | 8829/10570 [00:31<00:06, 273.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 87%|████████▋ | 9240/10570 [00:32<00:04, 285.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 87%|████████▋ | 9212/10570 [00:32<00:04, 274.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 87%|████████▋ | 9149/10570 [00:32<00:04, 285.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 83%|████████▎ | 8743/10570 [00:31<00:06, 274.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 87%|████████▋ | 9213/10570 [00:32<00:04, 283.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 87%|████████▋ | 9204/10570 [00:32<00:04, 280.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 86%|████████▌ | 9088/10570 [00:32<00:05, 280.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 88%|████████▊ | 9339/10570 [00:32<00:04, 289.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 8781/10570 [00:31<00:06, 263.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 85%|████████▌ | 9032/10570 [00:32<00:05, 280.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 81%|████████▏ | 8605/10570 [00:31<00:07, 274.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 81%|████████  | 8511/10570 [00:31<00:08, 244.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 87%|████████▋ | 9182/10570 [00:32<00:04, 283.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 86%|████████▌ | 9109/10570 [00:32<00:05, 280.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 84%|████████▍ | 8857/10570 [00:31<00:06, 273.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 88%|████████▊ | 9269/10570 [00:32<00:04, 285.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 87%|████████▋ | 9241/10570 [00:32<00:04, 279.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 87%|████████▋ | 9198/10570 [00:32<00:04, 280.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 87%|████████▋ | 9178/10570 [00:32<00:04, 286.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 83%|████████▎ | 8771/10570 [00:31<00:06, 273.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 87%|████████▋ | 9242/10570 [00:32<00:04, 285.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 87%|████████▋ | 9233/10570 [00:32<00:04, 282.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 86%|████████▋ | 9117/10570 [00:32<00:05, 280.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 89%|████████▊ | 9369/10570 [00:32<00:04, 290.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|████████▎ | 8809/10570 [00:31<00:06, 267.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 86%|████████▌ | 9061/10570 [00:32<00:05, 279.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 82%|████████▏ | 8633/10570 [00:31<00:07, 274.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 81%|████████  | 8536/10570 [00:31<00:08, 245.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 87%|████████▋ | 9211/10570 [00:32<00:04, 282.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 86%|████████▋ | 9138/10570 [00:32<00:05, 282.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 84%|████████▍ | 8885/10570 [00:32<00:06, 275.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 88%|████████▊ | 9298/10570 [00:32<00:04, 286.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 87%|████████▋ | 9227/10570 [00:32<00:04, 282.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 88%|████████▊ | 9269/10570 [00:32<00:04, 275.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 83%|████████▎ | 8799/10570 [00:31<00:06, 273.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 87%|████████▋ | 9207/10570 [00:32<00:04, 276.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 88%|████████▊ | 9271/10570 [00:32<00:04, 286.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 88%|████████▊ | 9262/10570 [00:32<00:04, 282.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 87%|████████▋ | 9146/10570 [00:32<00:05, 281.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▎ | 8837/10570 [00:31<00:06, 270.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 89%|████████▉ | 9399/10570 [00:32<00:04, 288.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 86%|████████▌ | 9090/10570 [00:32<00:05, 280.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 82%|████████▏ | 8661/10570 [00:31<00:07, 272.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 81%|████████  | 8561/10570 [00:31<00:08, 244.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 87%|████████▋ | 9240/10570 [00:32<00:04, 284.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 87%|████████▋ | 9167/10570 [00:32<00:04, 281.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 84%|████████▍ | 8913/10570 [00:32<00:06, 274.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 88%|████████▊ | 9327/10570 [00:32<00:04, 285.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 88%|████████▊ | 9256/10570 [00:32<00:04, 282.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 88%|████████▊ | 9298/10570 [00:32<00:04, 278.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 84%|████████▎ | 8827/10570 [00:31<00:06, 273.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 87%|████████▋ | 9237/10570 [00:32<00:04, 280.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 88%|████████▊ | 9300/10570 [00:32<00:04, 285.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 88%|████████▊ | 9291/10570 [00:32<00:04, 283.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 87%|████████▋ | 9175/10570 [00:32<00:04, 281.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 8865/10570 [00:32<00:06, 272.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 89%|████████▉ | 9429/10570 [00:32<00:03, 289.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 86%|████████▋ | 9119/10570 [00:32<00:05, 279.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 82%|████████▏ | 8689/10570 [00:31<00:06, 273.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 81%|████████  | 8586/10570 [00:32<00:08, 242.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 88%|████████▊ | 9269/10570 [00:32<00:04, 285.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 87%|████████▋ | 9196/10570 [00:32<00:04, 282.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 85%|████████▍ | 8941/10570 [00:32<00:05, 274.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 89%|████████▊ | 9356/10570 [00:32<00:04, 283.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 88%|████████▊ | 9285/10570 [00:32<00:04, 283.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 88%|████████▊ | 9327/10570 [00:32<00:04, 279.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 84%|████████▍ | 8855/10570 [00:31<00:06, 273.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 88%|████████▊ | 9267/10570 [00:32<00:04, 283.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 88%|████████▊ | 9329/10570 [00:32<00:04, 284.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 88%|████████▊ | 9320/10570 [00:32<00:04, 283.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 87%|████████▋ | 9204/10570 [00:32<00:04, 281.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 8893/10570 [00:32<00:06, 273.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 89%|████████▉ | 9459/10570 [00:32<00:03, 290.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 87%|████████▋ | 9147/10570 [00:32<00:05, 279.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 82%|████████▏ | 8717/10570 [00:31<00:06, 273.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 81%|████████▏ | 8611/10570 [00:32<00:08, 244.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 88%|████████▊ | 9298/10570 [00:32<00:04, 285.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 85%|████████▍ | 8969/10570 [00:32<00:05, 275.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 87%|████████▋ | 9225/10570 [00:32<00:04, 282.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 89%|████████▉ | 9385/10570 [00:32<00:04, 283.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 88%|████████▊ | 9314/10570 [00:32<00:04, 283.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 89%|████████▊ | 9356/10570 [00:32<00:04, 281.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 84%|████████▍ | 8883/10570 [00:32<00:06, 275.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 88%|████████▊ | 9296/10570 [00:32<00:04, 285.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 89%|████████▊ | 9358/10570 [00:32<00:04, 283.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 88%|████████▊ | 9349/10570 [00:32<00:04, 284.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 87%|████████▋ | 9233/10570 [00:32<00:04, 282.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|████████▍ | 8921/10570 [00:32<00:05, 275.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 90%|████████▉ | 9489/10570 [00:32<00:03, 290.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 87%|████████▋ | 9175/10570 [00:32<00:04, 279.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 83%|████████▎ | 8745/10570 [00:32<00:06, 271.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 82%|████████▏ | 8636/10570 [00:32<00:07, 244.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 88%|████████▊ | 9327/10570 [00:32<00:04, 284.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 85%|████████▌ | 8997/10570 [00:32<00:05, 276.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 88%|████████▊ | 9255/10570 [00:32<00:04, 285.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 89%|████████▉ | 9414/10570 [00:32<00:04, 285.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 88%|████████▊ | 9343/10570 [00:32<00:04, 283.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 89%|████████▉ | 9385/10570 [00:32<00:04, 282.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 84%|████████▍ | 8911/10570 [00:32<00:06, 274.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 88%|████████▊ | 9325/10570 [00:33<00:04, 285.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 89%|████████▉ | 9387/10570 [00:32<00:04, 283.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 89%|████████▊ | 9378/10570 [00:32<00:04, 285.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▍ | 8949/10570 [00:32<00:05, 274.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 88%|████████▊ | 9263/10570 [00:32<00:04, 284.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 90%|█████████ | 9519/10570 [00:32<00:03, 291.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 87%|████████▋ | 9204/10570 [00:32<00:04, 280.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 83%|████████▎ | 8773/10570 [00:32<00:06, 269.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 82%|████████▏ | 8661/10570 [00:32<00:07, 242.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 89%|████████▊ | 9356/10570 [00:32<00:04, 284.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 85%|████████▌ | 9025/10570 [00:32<00:05, 275.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 88%|████████▊ | 9284/10570 [00:32<00:04, 285.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 89%|████████▉ | 9443/10570 [00:33<00:03, 286.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 89%|████████▊ | 9372/10570 [00:32<00:04, 285.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 89%|████████▉ | 9414/10570 [00:33<00:04, 283.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 85%|████████▍ | 8939/10570 [00:32<00:05, 274.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 88%|████████▊ | 9354/10570 [00:33<00:04, 283.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 89%|████████▉ | 9416/10570 [00:33<00:04, 284.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 89%|████████▉ | 9407/10570 [00:32<00:04, 285.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▍ | 8977/10570 [00:32<00:05, 275.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 88%|████████▊ | 9292/10570 [00:32<00:04, 284.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 90%|█████████ | 9549/10570 [00:32<00:03, 294.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 87%|████████▋ | 9233/10570 [00:32<00:04, 281.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 83%|████████▎ | 8801/10570 [00:32<00:06, 269.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 82%|████████▏ | 8686/10570 [00:32<00:07, 243.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 89%|████████▉ | 9385/10570 [00:33<00:04, 284.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 88%|████████▊ | 9313/10570 [00:32<00:04, 284.86it/s][1,mpirank:1,algo-1]<stderr>:#015 90%|████████▉ | 9473/10570 [00:33<00:03, 288.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 86%|████████▌ | 9053/10570 [00:32<00:05, 272.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 89%|████████▉ | 9443/10570 [00:33<00:03, 284.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 89%|████████▉ | 9401/10570 [00:33<00:04, 284.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 85%|████████▍ | 8967/10570 [00:32<00:05, 274.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 89%|████████▉ | 9383/10570 [00:33<00:04, 278.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 89%|████████▉ | 9445/10570 [00:33<00:03, 285.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 89%|████████▉ | 9436/10570 [00:32<00:03, 286.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▌ | 9006/10570 [00:32<00:05, 276.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 88%|████████▊ | 9321/10570 [00:33<00:04, 280.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 91%|█████████ | 9579/10570 [00:33<00:03, 293.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 88%|████████▊ | 9262/10570 [00:32<00:04, 283.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 84%|████████▎ | 8829/10570 [00:32<00:06, 270.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 89%|████████▉ | 9414/10570 [00:33<00:04, 285.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 82%|████████▏ | 8711/10570 [00:32<00:07, 243.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 90%|████████▉ | 9502/10570 [00:33<00:03, 287.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 86%|████████▌ | 9081/10570 [00:32<00:05, 273.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 88%|████████▊ | 9342/10570 [00:32<00:04, 283.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 89%|████████▉ | 9430/10570 [00:33<00:04, 284.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 90%|████████▉ | 9473/10570 [00:33<00:03, 287.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 85%|████████▌ | 8996/10570 [00:32<00:05, 276.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 89%|████████▉ | 9412/10570 [00:33<00:04, 281.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 90%|████████▉ | 9475/10570 [00:33<00:03, 286.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 90%|████████▉ | 9466/10570 [00:33<00:03, 288.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|████████▌ | 9034/10570 [00:32<00:05, 275.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 88%|████████▊ | 9350/10570 [00:33<00:04, 281.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 91%|█████████ | 9609/10570 [00:33<00:03, 293.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 88%|████████▊ | 9291/10570 [00:33<00:04, 282.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 84%|████████▍ | 8857/10570 [00:32<00:06, 271.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 89%|████████▉ | 9443/10570 [00:33<00:03, 283.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 83%|████████▎ | 8736/10570 [00:32<00:07, 240.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 90%|█████████ | 9532/10570 [00:33<00:03, 289.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 86%|████████▌ | 9109/10570 [00:32<00:05, 272.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 89%|████████▊ | 9371/10570 [00:33<00:04, 283.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 89%|████████▉ | 9460/10570 [00:33<00:03, 286.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 90%|████████▉ | 9502/10570 [00:33<00:03, 279.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 85%|████████▌ | 9024/10570 [00:32<00:05, 276.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 89%|████████▉ | 9442/10570 [00:33<00:03, 284.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 90%|████████▉ | 9504/10570 [00:33<00:03, 286.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 90%|████████▉ | 9495/10570 [00:33<00:03, 288.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 9062/10570 [00:32<00:05, 276.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 89%|████████▊ | 9379/10570 [00:33<00:04, 282.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 91%|█████████ | 9639/10570 [00:33<00:03, 292.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 88%|████████▊ | 9320/10570 [00:33<00:04, 280.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 84%|████████▍ | 8885/10570 [00:32<00:06, 272.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 90%|████████▉ | 9472/10570 [00:33<00:03, 284.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 83%|████████▎ | 8761/10570 [00:32<00:07, 240.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 90%|█████████ | 9562/10570 [00:33<00:03, 290.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 86%|████████▋ | 9137/10570 [00:33<00:05, 273.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 90%|████████▉ | 9489/10570 [00:33<00:03, 286.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 89%|████████▉ | 9400/10570 [00:33<00:04, 282.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 90%|█████████ | 9532/10570 [00:33<00:03, 283.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 86%|████████▌ | 9052/10570 [00:32<00:05, 274.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 90%|████████▉ | 9472/10570 [00:33<00:03, 287.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 90%|█████████ | 9534/10570 [00:33<00:03, 288.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 90%|█████████ | 9524/10570 [00:33<00:03, 289.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▌ | 9090/10570 [00:32<00:05, 275.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 89%|████████▉ | 9408/10570 [00:33<00:04, 282.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 91%|█████████▏| 9669/10570 [00:33<00:03, 292.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 88%|████████▊ | 9349/10570 [00:33<00:04, 277.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 84%|████████▍ | 8913/10570 [00:32<00:06, 271.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 90%|████████▉ | 9501/10570 [00:33<00:03, 279.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 83%|████████▎ | 8786/10570 [00:32<00:07, 239.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 87%|████████▋ | 9165/10570 [00:33<00:05, 273.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 89%|████████▉ | 9429/10570 [00:33<00:04, 283.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 91%|█████████ | 9592/10570 [00:33<00:03, 289.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 90%|█████████ | 9519/10570 [00:33<00:03, 288.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 90%|█████████ | 9562/10570 [00:33<00:03, 285.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 86%|████████▌ | 9080/10570 [00:32<00:05, 274.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 90%|████████▉ | 9501/10570 [00:33<00:03, 287.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 90%|█████████ | 9564/10570 [00:33<00:03, 290.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 90%|█████████ | 9554/10570 [00:33<00:03, 290.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|████████▋ | 9118/10570 [00:33<00:05, 272.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 89%|████████▉ | 9437/10570 [00:33<00:03, 283.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 92%|█████████▏| 9699/10570 [00:33<00:03, 287.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 89%|████████▊ | 9377/10570 [00:33<00:04, 277.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 85%|████████▍ | 8941/10570 [00:32<00:06, 271.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 90%|█████████ | 9531/10570 [00:33<00:03, 284.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 89%|████████▉ | 9458/10570 [00:33<00:03, 285.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 83%|████████▎ | 8814/10570 [00:32<00:07, 249.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 91%|█████████ | 9622/10570 [00:33<00:03, 290.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 87%|████████▋ | 9193/10570 [00:33<00:05, 272.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 90%|█████████ | 9549/10570 [00:33<00:03, 289.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 91%|█████████ | 9591/10570 [00:33<00:03, 282.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 86%|████████▌ | 9108/10570 [00:32<00:05, 274.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 90%|█████████ | 9531/10570 [00:33<00:03, 290.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 91%|█████████ | 9594/10570 [00:33<00:03, 288.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 91%|█████████ | 9584/10570 [00:33<00:03, 287.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 9146/10570 [00:33<00:05, 274.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 90%|████████▉ | 9467/10570 [00:33<00:03, 286.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 92%|█████████▏| 9729/10570 [00:33<00:02, 290.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 89%|████████▉ | 9405/10570 [00:33<00:04, 277.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 85%|████████▍ | 8969/10570 [00:32<00:05, 270.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 90%|█████████ | 9561/10570 [00:33<00:03, 287.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 90%|████████▉ | 9487/10570 [00:33<00:03, 285.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 84%|████████▎ | 8842/10570 [00:33<00:06, 257.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 87%|████████▋ | 9221/10570 [00:33<00:04, 273.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 91%|█████████ | 9578/10570 [00:33<00:03, 288.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 91%|█████████▏| 9652/10570 [00:33<00:03, 290.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 86%|████████▋ | 9136/10570 [00:33<00:05, 275.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 91%|█████████ | 9621/10570 [00:33<00:03, 285.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 90%|█████████ | 9561/10570 [00:33<00:03, 291.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 91%|█████████ | 9624/10570 [00:33<00:03, 289.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 91%|█████████ | 9614/10570 [00:33<00:03, 289.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 9174/10570 [00:33<00:05, 274.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 90%|████████▉ | 9496/10570 [00:33<00:03, 285.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 92%|█████████▏| 9759/10570 [00:33<00:02, 290.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 89%|████████▉ | 9434/10570 [00:33<00:04, 278.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 85%|████████▌ | 8997/10570 [00:33<00:05, 272.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 91%|█████████ | 9590/10570 [00:33<00:03, 286.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 90%|█████████ | 9516/10570 [00:33<00:03, 286.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 88%|████████▊ | 9249/10570 [00:33<00:04, 274.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 91%|█████████ | 9607/10570 [00:33<00:03, 289.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 84%|████████▍ | 8868/10570 [00:33<00:06, 255.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 92%|█████████▏| 9682/10570 [00:33<00:03, 286.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 87%|████████▋ | 9164/10570 [00:33<00:05, 274.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 91%|█████████▏| 9650/10570 [00:33<00:03, 283.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 91%|█████████ | 9591/10570 [00:34<00:03, 290.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 91%|█████████▏| 9653/10570 [00:33<00:03, 288.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 91%|█████████ | 9643/10570 [00:33<00:03, 284.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 9202/10570 [00:33<00:04, 275.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 90%|█████████ | 9525/10570 [00:33<00:03, 286.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 93%|█████████▎| 9789/10570 [00:33<00:02, 289.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 90%|████████▉ | 9463/10570 [00:33<00:03, 281.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 85%|████████▌ | 9025/10570 [00:33<00:05, 272.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 91%|█████████ | 9620/10570 [00:33<00:03, 287.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 90%|█████████ | 9546/10570 [00:33<00:03, 288.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 88%|████████▊ | 9277/10570 [00:33<00:04, 276.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 91%|█████████ | 9636/10570 [00:33<00:03, 288.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 84%|████████▍ | 8896/10570 [00:33<00:06, 260.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 92%|█████████▏| 9679/10570 [00:34<00:03, 285.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 92%|█████████▏| 9711/10570 [00:33<00:03, 279.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 91%|█████████ | 9621/10570 [00:34<00:03, 292.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 87%|████████▋ | 9192/10570 [00:33<00:05, 264.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 92%|█████████▏| 9682/10570 [00:33<00:03, 289.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 92%|█████████▏| 9672/10570 [00:33<00:03, 284.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|████████▋ | 9230/10570 [00:33<00:04, 276.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 90%|█████████ | 9555/10570 [00:33<00:03, 287.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 93%|█████████▎| 9819/10570 [00:33<00:02, 289.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 90%|████████▉ | 9492/10570 [00:33<00:03, 282.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 86%|████████▌ | 9053/10570 [00:33<00:05, 270.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 91%|█████████▏| 9649/10570 [00:34<00:03, 287.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 91%|█████████ | 9575/10570 [00:33<00:03, 288.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 91%|█████████▏| 9665/10570 [00:33<00:03, 287.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 88%|████████▊ | 9305/10570 [00:33<00:04, 272.39it/s][1,mpirank:8,algo-2]<stderr>:#015 84%|████████▍ | 8924/10570 [00:33<00:06, 264.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 92%|█████████▏| 9741/10570 [00:34<00:02, 283.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 92%|█████████▏| 9709/10570 [00:34<00:02, 287.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 87%|████████▋ | 9220/10570 [00:33<00:05, 267.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 91%|█████████▏| 9651/10570 [00:34<00:03, 291.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 92%|█████████▏| 9712/10570 [00:34<00:02, 290.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 92%|█████████▏| 9702/10570 [00:33<00:03, 287.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 9259/10570 [00:33<00:04, 278.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 91%|█████████ | 9584/10570 [00:33<00:03, 287.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 90%|█████████ | 9521/10570 [00:33<00:03, 283.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 93%|█████████▎| 9848/10570 [00:34<00:02, 287.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 92%|█████████▏| 9678/10570 [00:34<00:03, 288.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 91%|█████████ | 9604/10570 [00:33<00:03, 287.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 86%|████████▌ | 9081/10570 [00:33<00:05, 270.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 92%|█████████▏| 9695/10570 [00:34<00:03, 288.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 85%|████████▍ | 8952/10570 [00:33<00:06, 267.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 88%|████████▊ | 9333/10570 [00:33<00:04, 272.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 92%|█████████▏| 9739/10570 [00:34<00:02, 289.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 92%|█████████▏| 9771/10570 [00:34<00:02, 285.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 92%|█████████▏| 9681/10570 [00:34<00:03, 291.88it/s][1,mpirank:11,algo-2]<stderr>:#015 87%|████████▋ | 9247/10570 [00:33<00:04, 265.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 92%|█████████▏| 9742/10570 [00:34<00:02, 290.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 92%|█████████▏| 9732/10570 [00:33<00:02, 289.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 9287/10570 [00:33<00:04, 277.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 91%|█████████ | 9613/10570 [00:34<00:03, 288.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 90%|█████████ | 9550/10570 [00:33<00:03, 285.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 93%|█████████▎| 9877/10570 [00:34<00:02, 281.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 91%|█████████ | 9633/10570 [00:33<00:03, 287.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 92%|█████████▏| 9708/10570 [00:34<00:02, 289.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 86%|████████▌ | 9109/10570 [00:33<00:05, 270.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 92%|█████████▏| 9725/10570 [00:34<00:02, 288.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 89%|████████▊ | 9361/10570 [00:33<00:04, 273.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 85%|████████▍ | 8980/10570 [00:33<00:05, 269.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 92%|█████████▏| 9768/10570 [00:34<00:02, 288.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 93%|█████████▎| 9800/10570 [00:34<00:02, 285.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 88%|████████▊ | 9275/10570 [00:33<00:04, 269.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 92%|█████████▏| 9711/10570 [00:34<00:02, 292.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 92%|█████████▏| 9772/10570 [00:34<00:02, 290.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 92%|█████████▏| 9762/10570 [00:34<00:02, 289.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 9315/10570 [00:33<00:04, 276.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 91%|█████████ | 9642/10570 [00:34<00:03, 286.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 91%|█████████ | 9579/10570 [00:34<00:03, 284.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 94%|█████████▎| 9906/10570 [00:34<00:02, 281.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 91%|█████████▏| 9662/10570 [00:34<00:03, 287.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 92%|█████████▏| 9738/10570 [00:34<00:02, 290.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 86%|████████▋ | 9137/10570 [00:33<00:05, 271.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 92%|█████████▏| 9754/10570 [00:34<00:02, 288.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 85%|████████▌ | 9008/10570 [00:33<00:05, 271.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 89%|████████▉ | 9389/10570 [00:33<00:04, 272.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 93%|█████████▎| 9829/10570 [00:34<00:02, 286.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 93%|█████████▎| 9797/10570 [00:34<00:02, 286.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 88%|████████▊ | 9303/10570 [00:33<00:04, 270.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 92%|█████████▏| 9741/10570 [00:34<00:02, 293.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 93%|█████████▎| 9802/10570 [00:34<00:02, 288.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 93%|█████████▎| 9791/10570 [00:34<00:02, 288.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|████████▊ | 9343/10570 [00:33<00:04, 276.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 91%|█████████▏| 9671/10570 [00:34<00:03, 284.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 91%|█████████ | 9608/10570 [00:34<00:03, 284.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 94%|█████████▍| 9936/10570 [00:34<00:02, 284.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 92%|█████████▏| 9691/10570 [00:34<00:03, 285.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 92%|█████████▏| 9768/10570 [00:34<00:02, 289.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 93%|█████████▎| 9783/10570 [00:34<00:02, 288.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 87%|████████▋ | 9165/10570 [00:33<00:05, 271.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 89%|████████▉ | 9417/10570 [00:34<00:04, 273.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 85%|████████▌ | 9036/10570 [00:33<00:05, 269.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 93%|█████████▎| 9826/10570 [00:34<00:02, 287.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 93%|█████████▎| 9858/10570 [00:34<00:02, 284.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 88%|████████▊ | 9331/10570 [00:33<00:04, 271.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 92%|█████████▏| 9771/10570 [00:34<00:02, 292.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 93%|█████████▎| 9831/10570 [00:34<00:02, 287.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 93%|█████████▎| 9821/10570 [00:34<00:02, 289.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▊ | 9371/10570 [00:33<00:04, 277.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 92%|█████████▏| 9700/10570 [00:34<00:03, 279.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 91%|█████████ | 9637/10570 [00:34<00:03, 284.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 94%|█████████▍| 9965/10570 [00:34<00:02, 285.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 92%|█████████▏| 9720/10570 [00:34<00:02, 285.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 93%|█████████▎| 9797/10570 [00:34<00:02, 288.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 93%|█████████▎| 9812/10570 [00:34<00:02, 287.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 87%|████████▋ | 9193/10570 [00:33<00:05, 271.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 89%|████████▉ | 9445/10570 [00:34<00:04, 274.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 86%|████████▌ | 9064/10570 [00:33<00:05, 271.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 94%|█████████▎| 9887/10570 [00:34<00:02, 284.39it/s][1,mpirank:9,algo-2]<stderr>:#015 93%|█████████▎| 9855/10570 [00:34<00:02, 284.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 89%|████████▊ | 9359/10570 [00:33<00:04, 272.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 93%|█████████▎| 9801/10570 [00:34<00:02, 291.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 93%|█████████▎| 9860/10570 [00:34<00:02, 283.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 93%|█████████▎| 9850/10570 [00:34<00:02, 285.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 9399/10570 [00:34<00:04, 276.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 92%|█████████▏| 9729/10570 [00:34<00:02, 282.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 91%|█████████▏| 9666/10570 [00:34<00:03, 283.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 95%|█████████▍| 9994/10570 [00:34<00:02, 279.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 92%|█████████▏| 9749/10570 [00:34<00:02, 285.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 93%|█████████▎| 9826/10570 [00:34<00:02, 288.60it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 93%|█████████▎| 9841/10570 [00:34<00:02, 284.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 87%|████████▋ | 9221/10570 [00:33<00:04, 271.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 90%|████████▉ | 9473/10570 [00:34<00:04, 273.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 86%|████████▌ | 9092/10570 [00:33<00:05, 271.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 94%|█████████▍| 9916/10570 [00:34<00:02, 284.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 94%|█████████▎| 9884/10570 [00:34<00:02, 283.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 89%|████████▉ | 9387/10570 [00:33<00:04, 273.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 93%|█████████▎| 9831/10570 [00:34<00:02, 289.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 94%|█████████▎| 9889/10570 [00:34<00:02, 284.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 93%|█████████▎| 9879/10570 [00:34<00:02, 285.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 9427/10570 [00:34<00:04, 277.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 92%|█████████▏| 9758/10570 [00:34<00:02, 284.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 92%|█████████▏| 9695/10570 [00:34<00:03, 281.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 95%|█████████▍| 10024/10570 [00:34<00:01, 282.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 93%|█████████▎| 9779/10570 [00:34<00:02, 287.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 93%|█████████▎| 9855/10570 [00:34<00:02, 285.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 93%|█████████▎| 9870/10570 [00:34<00:02, 285.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 88%|████████▊ | 9249/10570 [00:33<00:04, 270.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 86%|████████▋ | 9120/10570 [00:34<00:05, 271.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 90%|████████▉ | 9501/10570 [00:34<00:03, 270.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 94%|█████████▍| 9946/10570 [00:34<00:02, 287.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 94%|█████████▍| 9913/10570 [00:34<00:02, 284.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 89%|████████▉ | 9415/10570 [00:34<00:04, 274.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 93%|█████████▎| 9860/10570 [00:34<00:02, 288.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 94%|█████████▍| 9918/10570 [00:34<00:02, 284.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 94%|█████████▎| 9908/10570 [00:34<00:02, 284.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|████████▉ | 9455/10570 [00:34<00:04, 277.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 93%|█████████▎| 9787/10570 [00:34<00:02, 284.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 92%|█████████▏| 9724/10570 [00:34<00:02, 283.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 95%|█████████▌| 10054/10570 [00:34<00:01, 286.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 93%|█████████▎| 9808/10570 [00:34<00:02, 285.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 94%|█████████▎| 9899/10570 [00:34<00:02, 286.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 94%|█████████▎| 9884/10570 [00:34<00:02, 283.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 88%|████████▊ | 9277/10570 [00:34<00:04, 271.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 87%|████████▋ | 9148/10570 [00:34<00:05, 272.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 90%|█████████ | 9530/10570 [00:34<00:03, 273.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 94%|█████████▍| 9975/10570 [00:34<00:02, 286.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 94%|█████████▍| 9942/10570 [00:34<00:02, 280.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 89%|████████▉ | 9443/10570 [00:34<00:04, 275.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 94%|█████████▍| 9948/10570 [00:34<00:02, 286.59it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 94%|█████████▎| 9889/10570 [00:35<00:02, 280.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|████████▉ | 9483/10570 [00:34<00:03, 278.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 94%|█████████▍| 9937/10570 [00:34<00:02, 274.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 93%|█████████▎| 9816/10570 [00:34<00:02, 284.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 92%|█████████▏| 9753/10570 [00:34<00:02, 283.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 95%|█████████▌| 10084/10570 [00:34<00:01, 287.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 93%|█████████▎| 9837/10570 [00:34<00:02, 283.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 94%|█████████▍| 9928/10570 [00:34<00:02, 284.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 94%|█████████▍| 9913/10570 [00:34<00:02, 283.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 88%|████████▊ | 9305/10570 [00:34<00:04, 271.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 87%|████████▋ | 9176/10570 [00:34<00:05, 272.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 90%|█████████ | 9559/10570 [00:34<00:03, 276.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 95%|█████████▍| 10004/10570 [00:34<00:01, 286.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 94%|█████████▍| 9971/10570 [00:35<00:02, 280.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 90%|████████▉ | 9472/10570 [00:34<00:03, 277.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 94%|█████████▍| 9977/10570 [00:34<00:02, 285.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|████████▉ | 9512/10570 [00:34<00:03, 278.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 94%|█████████▍| 9918/10570 [00:35<00:02, 272.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 94%|█████████▍| 9965/10570 [00:34<00:02, 268.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 93%|█████████▎| 9845/10570 [00:34<00:02, 281.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 93%|█████████▎| 9782/10570 [00:34<00:02, 284.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 96%|█████████▌| 10114/10570 [00:34<00:01, 289.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 93%|█████████▎| 9866/10570 [00:34<00:02, 283.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 94%|█████████▍| 9958/10570 [00:34<00:02, 286.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 94%|█████████▍| 9943/10570 [00:35<00:02, 286.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 87%|████████▋ | 9204/10570 [00:34<00:04, 273.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 88%|████████▊ | 9333/10570 [00:34<00:04, 271.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 91%|█████████ | 9587/10570 [00:34<00:03, 277.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 95%|█████████▍| 10034/10570 [00:35<00:01, 288.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 95%|█████████▍| 10000/10570 [00:35<00:02, 283.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 90%|████████▉ | 9500/10570 [00:34<00:03, 277.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 95%|█████████▍| 10006/10570 [00:35<00:01, 285.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|█████████ | 9541/10570 [00:34<00:03, 280.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 94%|█████████▍| 9946/10570 [00:35<00:02, 268.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 95%|█████████▍| 9992/10570 [00:34<00:02, 262.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 93%|█████████▎| 9811/10570 [00:34<00:02, 283.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 93%|█████████▎| 9874/10570 [00:35<00:02, 268.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 96%|█████████▌| 10144/10570 [00:35<00:01, 291.55it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 94%|█████████▎| 9895/10570 [00:34<00:02, 283.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 94%|█████████▍| 9972/10570 [00:35<00:02, 284.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 94%|█████████▍| 9987/10570 [00:35<00:02, 284.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 87%|████████▋ | 9232/10570 [00:34<00:04, 273.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 89%|████████▊ | 9361/10570 [00:34<00:04, 271.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 91%|█████████ | 9616/10570 [00:34<00:03, 278.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 95%|█████████▌| 10063/10570 [00:35<00:01, 288.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 95%|█████████▍| 10030/10570 [00:35<00:01, 286.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 90%|█████████ | 9529/10570 [00:34<00:03, 279.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 95%|█████████▍| 10036/10570 [00:35<00:01, 287.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 9570/10570 [00:34<00:03, 281.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 94%|█████████▍| 9973/10570 [00:35<00:02, 263.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 95%|█████████▍| 10019/10570 [00:35<00:02, 261.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 93%|█████████▎| 9840/10570 [00:34<00:02, 280.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 94%|█████████▎| 9903/10570 [00:35<00:02, 273.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 94%|█████████▍| 9924/10570 [00:35<00:02, 284.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 95%|█████████▍| 10016/10570 [00:35<00:01, 286.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 95%|█████████▍| 10001/10570 [00:35<00:01, 285.53it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 96%|█████████▋| 10174/10570 [00:35<00:01, 281.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 88%|████████▊ | 9261/10570 [00:34<00:04, 275.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 91%|█████████ | 9644/10570 [00:34<00:03, 277.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 89%|████████▉ | 9389/10570 [00:34<00:04, 271.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 95%|█████████▌| 10092/10570 [00:35<00:01, 289.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 95%|█████████▌| 10060/10570 [00:35<00:01, 288.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 90%|█████████ | 9558/10570 [00:34<00:03, 279.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 95%|█████████▌| 10065/10570 [00:35<00:01, 287.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 9599/10570 [00:34<00:03, 280.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 95%|█████████▍| 10000/10570 [00:35<00:02, 260.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 95%|█████████▌| 10046/10570 [00:35<00:02, 261.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 94%|█████████▍| 9932/10570 [00:35<00:02, 277.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 93%|█████████▎| 9869/10570 [00:35<00:02, 275.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 94%|█████████▍| 9953/10570 [00:35<00:02, 285.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 95%|█████████▌| 10046/10570 [00:35<00:01, 288.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 95%|█████████▍| 10031/10570 [00:35<00:01, 287.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 97%|█████████▋| 10204/10570 [00:35<00:01, 285.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 92%|█████████▏| 9672/10570 [00:34<00:03, 277.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 88%|████████▊ | 9289/10570 [00:34<00:04, 275.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 89%|████████▉ | 9417/10570 [00:34<00:04, 272.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 96%|█████████▌| 10122/10570 [00:35<00:01, 290.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 95%|█████████▌| 10089/10570 [00:35<00:01, 287.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 91%|█████████ | 9586/10570 [00:34<00:03, 277.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 95%|█████████▌| 10094/10570 [00:35<00:01, 288.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████ | 9628/10570 [00:34<00:03, 281.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 95%|█████████▍| 10027/10570 [00:35<00:02, 261.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 95%|█████████▌| 10073/10570 [00:35<00:01, 259.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 94%|█████████▍| 9961/10570 [00:35<00:02, 280.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 94%|█████████▎| 9898/10570 [00:35<00:02, 278.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 94%|█████████▍| 9982/10570 [00:35<00:02, 282.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 95%|█████████▌| 10075/10570 [00:35<00:01, 288.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 95%|█████████▌| 10061/10570 [00:35<00:01, 288.63it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 97%|█████████▋| 10234/10570 [00:35<00:01, 288.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 92%|█████████▏| 9701/10570 [00:35<00:03, 279.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 89%|████████▉ | 9445/10570 [00:34<00:04, 273.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 88%|████████▊ | 9317/10570 [00:34<00:04, 270.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 96%|█████████▌| 10152/10570 [00:35<00:01, 289.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 96%|█████████▌| 10119/10570 [00:35<00:01, 288.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 91%|█████████ | 9615/10570 [00:34<00:03, 279.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 96%|█████████▌| 10124/10570 [00:35<00:01, 289.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|█████████▏| 9657/10570 [00:34<00:03, 280.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 95%|█████████▌| 10054/10570 [00:35<00:01, 260.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 96%|█████████▌| 10099/10570 [00:35<00:01, 258.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 95%|█████████▍| 9990/10570 [00:35<00:02, 279.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 94%|█████████▍| 9927/10570 [00:35<00:02, 279.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 96%|█████████▌| 10104/10570 [00:35<00:01, 288.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 95%|█████████▍| 10011/10570 [00:35<00:01, 282.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 95%|█████████▌| 10090/10570 [00:35<00:01, 287.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 97%|█████████▋| 10263/10570 [00:35<00:01, 288.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 92%|█████████▏| 9730/10570 [00:35<00:02, 280.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 90%|████████▉ | 9473/10570 [00:34<00:04, 270.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 88%|████████▊ | 9345/10570 [00:34<00:04, 271.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 96%|█████████▌| 10149/10570 [00:35<00:01, 289.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 96%|█████████▋| 10181/10570 [00:35<00:01, 279.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 91%|█████████ | 9643/10570 [00:34<00:03, 267.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 96%|█████████▌| 10154/10570 [00:35<00:01, 289.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 9686/10570 [00:35<00:03, 280.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 95%|█████████▌| 10081/10570 [00:35<00:01, 259.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 96%|█████████▌| 10125/10570 [00:35<00:01, 258.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 95%|█████████▍| 10019/10570 [00:35<00:01, 282.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 94%|█████████▍| 9956/10570 [00:35<00:02, 280.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 96%|█████████▌| 10134/10570 [00:35<00:01, 289.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 95%|█████████▍| 10040/10570 [00:35<00:01, 281.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 96%|█████████▌| 10119/10570 [00:35<00:01, 287.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 97%|█████████▋| 10292/10570 [00:35<00:00, 283.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 92%|█████████▏| 9759/10570 [00:35<00:02, 280.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 90%|████████▉ | 9501/10570 [00:34<00:03, 271.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 89%|████████▊ | 9373/10570 [00:35<00:04, 272.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 97%|█████████▋| 10210/10570 [00:35<00:01, 282.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 96%|█████████▋| 10178/10570 [00:35<00:01, 278.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 91%|█████████▏| 9671/10570 [00:34<00:03, 270.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 96%|█████████▋| 10183/10570 [00:35<00:01, 277.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 9715/10570 [00:35<00:03, 282.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 96%|█████████▌| 10107/10570 [00:35<00:01, 259.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 96%|█████████▌| 10151/10570 [00:35<00:01, 258.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 95%|█████████▌| 10049/10570 [00:35<00:01, 284.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 94%|█████████▍| 9985/10570 [00:35<00:02, 279.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 95%|█████████▌| 10069/10570 [00:35<00:01, 283.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 96%|█████████▌| 10149/10570 [00:35<00:01, 288.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 96%|█████████▌| 10163/10570 [00:35<00:01, 281.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 93%|█████████▎| 9788/10570 [00:35<00:02, 279.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 90%|█████████ | 9530/10570 [00:34<00:03, 274.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 89%|████████▉ | 9401/10570 [00:35<00:04, 272.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 98%|█████████▊| 10321/10570 [00:35<00:00, 274.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 97%|█████████▋| 10240/10570 [00:35<00:01, 285.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 97%|█████████▋| 10208/10570 [00:35<00:01, 282.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 92%|█████████▏| 9700/10570 [00:35<00:03, 273.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 97%|█████████▋| 10212/10570 [00:35<00:01, 280.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 9744/10570 [00:35<00:02, 282.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 96%|█████████▌| 10133/10570 [00:36<00:01, 256.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 95%|█████████▌| 10078/10570 [00:35<00:01, 285.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 95%|█████████▍| 10014/10570 [00:35<00:01, 281.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 96%|█████████▋| 10177/10570 [00:35<00:01, 247.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 96%|█████████▌| 10098/10570 [00:35<00:01, 282.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 96%|█████████▋| 10192/10570 [00:35<00:01, 282.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 93%|█████████▎| 9816/10570 [00:35<00:02, 279.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 96%|█████████▋| 10178/10570 [00:35<00:01, 278.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 90%|█████████ | 9558/10570 [00:35<00:03, 275.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 89%|████████▉ | 9429/10570 [00:35<00:04, 273.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 97%|█████████▋| 10269/10570 [00:35<00:01, 285.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 98%|█████████▊| 10349/10570 [00:35<00:00, 269.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 97%|█████████▋| 10238/10570 [00:35<00:01, 285.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 92%|█████████▏| 9729/10570 [00:35<00:03, 276.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 97%|█████████▋| 10242/10570 [00:35<00:01, 283.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|█████████▏| 9773/10570 [00:35<00:02, 282.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 96%|█████████▌| 10160/10570 [00:36<00:01, 257.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 96%|█████████▌| 10107/10570 [00:35<00:01, 286.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 97%|█████████▋| 10203/10570 [00:35<00:01, 249.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 95%|█████████▌| 10043/10570 [00:35<00:01, 278.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 96%|█████████▌| 10128/10570 [00:35<00:01, 285.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 97%|█████████▋| 10222/10570 [00:35<00:01, 284.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 97%|█████████▋| 10207/10570 [00:35<00:01, 281.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 89%|████████▉ | 9457/10570 [00:35<00:04, 274.85it/s][1,mpirank:15,algo-2]<stderr>:#015 93%|█████████▎| 9844/10570 [00:35<00:02, 275.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 91%|█████████ | 9586/10570 [00:35<00:03, 273.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 97%|█████████▋| 10298/10570 [00:36<00:00, 286.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 98%|█████████▊| 10376/10570 [00:35<00:00, 263.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 97%|█████████▋| 10267/10570 [00:36<00:01, 285.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 92%|█████████▏| 9758/10570 [00:35<00:02, 277.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 97%|█████████▋| 10271/10570 [00:36<00:01, 283.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 9802/10570 [00:35<00:02, 280.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 96%|█████████▌| 10136/10570 [00:35<00:01, 283.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 96%|█████████▋| 10186/10570 [00:36<00:01, 246.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 97%|█████████▋| 10230/10570 [00:35<00:01, 254.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 95%|█████████▌| 10072/10570 [00:35<00:01, 280.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 96%|█████████▌| 10157/10570 [00:35<00:01, 286.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 97%|█████████▋| 10251/10570 [00:35<00:01, 285.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 97%|█████████▋| 10237/10570 [00:36<00:01, 284.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 93%|█████████▎| 9872/10570 [00:35<00:02, 276.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 90%|████████▉ | 9485/10570 [00:35<00:03, 274.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 91%|█████████ | 9614/10570 [00:35<00:03, 274.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 98%|█████████▊| 10327/10570 [00:36<00:00, 284.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 98%|█████████▊| 10403/10570 [00:36<00:00, 260.40it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 93%|█████████▎| 9786/10570 [00:35<00:02, 277.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 97%|█████████▋| 10296/10570 [00:36<00:00, 278.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 97%|█████████▋| 10300/10570 [00:36<00:00, 284.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 9831/10570 [00:35<00:02, 279.85it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 97%|█████████▋| 10212/10570 [00:36<00:01, 249.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 97%|█████████▋| 10259/10570 [00:35<00:01, 263.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 96%|█████████▌| 10101/10570 [00:35<00:01, 281.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 96%|█████████▌| 10165/10570 [00:36<00:01, 274.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 96%|█████████▋| 10186/10570 [00:35<00:01, 277.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 97%|█████████▋| 10280/10570 [00:36<00:01, 281.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 94%|█████████▎| 9900/10570 [00:35<00:02, 277.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 97%|█████████▋| 10266/10570 [00:36<00:01, 284.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 91%|█████████ | 9642/10570 [00:35<00:03, 272.36it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 90%|█████████ | 9513/10570 [00:35<00:03, 271.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 98%|█████████▊| 10356/10570 [00:36<00:00, 285.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 99%|█████████▊| 10430/10570 [00:36<00:00, 258.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 93%|█████████▎| 9814/10570 [00:35<00:02, 277.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 98%|█████████▊| 10324/10570 [00:36<00:00, 277.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 98%|█████████▊| 10329/10570 [00:36<00:00, 285.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|█████████▎| 9859/10570 [00:35<00:02, 278.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 97%|█████████▋| 10238/10570 [00:36<00:01, 252.73it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 96%|█████████▌| 10130/10570 [00:35<00:01, 283.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 97%|█████████▋| 10287/10570 [00:36<00:01, 266.45it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 96%|█████████▋| 10194/10570 [00:36<00:01, 278.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 98%|█████████▊| 10309/10570 [00:36<00:00, 283.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 94%|█████████▍| 9928/10570 [00:35<00:02, 277.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 97%|█████████▋| 10214/10570 [00:36<00:01, 275.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 97%|█████████▋| 10296/10570 [00:36<00:00, 287.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 91%|█████████▏| 9670/10570 [00:35<00:03, 272.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 98%|█████████▊| 10385/10570 [00:36<00:00, 285.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 90%|█████████ | 9541/10570 [00:35<00:03, 264.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 99%|█████████▉| 10456/10570 [00:36<00:00, 256.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 93%|█████████▎| 9842/10570 [00:35<00:02, 274.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 98%|█████████▊| 10353/10570 [00:36<00:00, 278.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 98%|█████████▊| 10358/10570 [00:36<00:00, 286.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▎| 9888/10570 [00:35<00:02, 279.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 97%|█████████▋| 10264/10570 [00:36<00:01, 252.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 98%|█████████▊| 10316/10570 [00:36<00:00, 272.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 96%|█████████▌| 10159/10570 [00:36<00:01, 283.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 97%|█████████▋| 10223/10570 [00:36<00:01, 281.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 94%|█████████▍| 9956/10570 [00:35<00:02, 277.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 98%|█████████▊| 10338/10570 [00:36<00:00, 284.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 97%|█████████▋| 10242/10570 [00:36<00:01, 276.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 98%|█████████▊| 10325/10570 [00:36<00:00, 287.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 92%|█████████▏| 9698/10570 [00:35<00:03, 273.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 99%|█████████▊| 10414/10570 [00:36<00:00, 286.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 93%|█████████▎| 9870/10570 [00:35<00:02, 276.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 91%|█████████ | 9568/10570 [00:35<00:03, 253.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 99%|█████████▉| 10482/10570 [00:36<00:00, 253.57it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 98%|█████████▊| 10382/10570 [00:36<00:00, 280.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 98%|█████████▊| 10387/10570 [00:36<00:00, 285.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 9916/10570 [00:35<00:02, 279.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 97%|█████████▋| 10290/10570 [00:36<00:01, 254.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 98%|█████████▊| 10345/10570 [00:36<00:00, 277.71it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 97%|█████████▋| 10252/10570 [00:36<00:01, 283.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 96%|█████████▋| 10188/10570 [00:36<00:01, 274.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 98%|█████████▊| 10367/10570 [00:36<00:00, 284.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 97%|█████████▋| 10270/10570 [00:36<00:01, 275.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 94%|█████████▍| 9984/10570 [00:36<00:02, 274.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 98%|█████████▊| 10354/10570 [00:36<00:00, 285.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 92%|█████████▏| 9726/10570 [00:35<00:03, 274.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 99%|█████████▉| 10443/10570 [00:36<00:00, 285.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 94%|█████████▎| 9898/10570 [00:35<00:02, 276.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015 99%|█████████▉| 10508/10570 [00:36<00:00, 254.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 98%|█████████▊| 10411/10570 [00:36<00:00, 281.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 91%|█████████ | 9594/10570 [00:35<00:03, 249.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 99%|█████████▊| 10416/10570 [00:36<00:00, 285.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 9945/10570 [00:35<00:02, 281.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 98%|█████████▊| 10374/10570 [00:36<00:00, 280.59it/s][1,mpirank:10,algo-2]<stderr>:#015 98%|█████████▊| 10316/10570 [00:36<00:00, 254.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 97%|█████████▋| 10281/10570 [00:36<00:01, 284.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 97%|█████████▋| 10217/10570 [00:36<00:01, 277.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 98%|█████████▊| 10396/10570 [00:36<00:00, 284.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 97%|█████████▋| 10300/10570 [00:36<00:00, 280.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 98%|█████████▊| 10383/10570 [00:36<00:00, 285.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 95%|█████████▍| 10013/10570 [00:36<00:02, 276.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 92%|█████████▏| 9754/10570 [00:35<00:02, 275.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 99%|█████████▉| 10472/10570 [00:36<00:00, 283.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015100%|█████████▉| 10534/10570 [00:36<00:00, 255.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 99%|█████████▉| 10440/10570 [00:36<00:00, 282.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 94%|█████████▍| 9926/10570 [00:35<00:02, 271.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 91%|█████████ | 9620/10570 [00:35<00:03, 249.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 99%|█████████▉| 10445/10570 [00:36<00:00, 285.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|█████████▍| 9974/10570 [00:36<00:02, 279.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 98%|█████████▊| 10342/10570 [00:36<00:00, 254.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 98%|█████████▊| 10403/10570 [00:36<00:00, 280.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 98%|█████████▊| 10310/10570 [00:36<00:00, 285.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 97%|█████████▋| 10246/10570 [00:36<00:01, 280.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 99%|█████████▊| 10425/10570 [00:36<00:00, 285.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 99%|█████████▊| 10412/10570 [00:36<00:00, 285.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 95%|█████████▌| 10042/10570 [00:36<00:01, 278.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 98%|█████████▊| 10329/10570 [00:36<00:00, 280.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 93%|█████████▎| 9782/10570 [00:35<00:02, 275.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015 99%|█████████▉| 10501/10570 [00:36<00:00, 283.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015100%|█████████▉| 10560/10570 [00:36<00:00, 256.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 94%|█████████▍| 9954/10570 [00:35<00:02, 273.70it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 99%|█████████▉| 10469/10570 [00:36<00:00, 281.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 91%|█████████ | 9645/10570 [00:36<00:03, 247.92it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 99%|█████████▉| 10474/10570 [00:36<00:00, 283.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▍| 10002/10570 [00:36<00:02, 278.97it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015100%|██████████| 10570/10570 [00:36<00:00, 288.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 99%|█████████▊| 10432/10570 [00:36<00:00, 283.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 98%|█████████▊| 10368/10570 [00:36<00:00, 254.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 98%|█████████▊| 10339/10570 [00:36<00:00, 284.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 97%|█████████▋| 10275/10570 [00:36<00:01, 280.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 99%|█████████▉| 10454/10570 [00:36<00:00, 285.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 95%|█████████▌| 10070/10570 [00:36<00:01, 277.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 98%|█████████▊| 10358/10570 [00:36<00:00, 282.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 99%|█████████▉| 10441/10570 [00:36<00:00, 284.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 93%|█████████▎| 9810/10570 [00:35<00:02, 274.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015100%|█████████▉| 10530/10570 [00:36<00:00, 283.37it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015 99%|█████████▉| 10498/10570 [00:36<00:00, 283.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 94%|█████████▍| 9982/10570 [00:36<00:02, 272.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 91%|█████████▏| 9671/10570 [00:36<00:03, 250.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015 99%|█████████▉| 10503/10570 [00:36<00:00, 284.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▍| 10031/10570 [00:36<00:01, 280.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 99%|█████████▉| 10461/10570 [00:36<00:00, 282.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 98%|█████████▊| 10394/10570 [00:37<00:00, 252.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 98%|█████████▊| 10368/10570 [00:36<00:00, 284.28it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 97%|█████████▋| 10304/10570 [00:36<00:00, 280.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 99%|█████████▉| 10483/10570 [00:36<00:00, 283.62it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 96%|█████████▌| 10098/10570 [00:36<00:01, 277.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 98%|█████████▊| 10387/10570 [00:36<00:00, 282.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 99%|█████████▉| 10470/10570 [00:36<00:00, 282.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 93%|█████████▎| 9838/10570 [00:36<00:02, 272.54it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015100%|█████████▉| 10559/10570 [00:36<00:00, 284.67it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 95%|█████████▍| 10011/10570 [00:36<00:02, 275.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015100%|█████████▉| 10532/10570 [00:36<00:00, 286.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 92%|█████████▏| 9700/10570 [00:36<00:03, 259.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015100%|██████████| 10570/10570 [00:36<00:00, 285.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015100%|█████████▉| 10528/10570 [00:37<00:00, 272.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 10060/10570 [00:36<00:01, 279.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015 99%|█████████▉| 10490/10570 [00:36<00:00, 283.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 99%|█████████▊| 10420/10570 [00:37<00:00, 253.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 98%|█████████▊| 10397/10570 [00:36<00:00, 282.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 98%|█████████▊| 10333/10570 [00:36<00:00, 281.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015 99%|█████████▉| 10512/10570 [00:36<00:00, 284.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 96%|█████████▌| 10127/10570 [00:36<00:01, 278.61it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015 99%|█████████▉| 10499/10570 [00:36<00:00, 284.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 99%|█████████▊| 10416/10570 [00:36<00:00, 281.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 93%|█████████▎| 9866/10570 [00:36<00:02, 272.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 95%|█████████▍| 10040/10570 [00:36<00:01, 277.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015100%|█████████▉| 10562/10570 [00:37<00:00, 288.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 92%|█████████▏| 9729/10570 [00:36<00:03, 265.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015100%|█████████▉| 10558/10570 [00:37<00:00, 277.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|█████████▌| 10088/10570 [00:36<00:01, 279.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015100%|██████████| 10570/10570 [00:37<00:00, 285.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015100%|█████████▉| 10519/10570 [00:36<00:00, 285.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 99%|█████████▉| 10446/10570 [00:37<00:00, 253.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 99%|█████████▊| 10426/10570 [00:36<00:00, 283.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015100%|██████████| 10570/10570 [00:37<00:00, 284.50it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 98%|█████████▊| 10362/10570 [00:36<00:00, 281.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015100%|█████████▉| 10541/10570 [00:37<00:00, 280.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 96%|█████████▌| 10155/10570 [00:36<00:01, 276.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 99%|█████████▉| 10445/10570 [00:36<00:00, 282.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015100%|█████████▉| 10529/10570 [00:37<00:00, 286.38it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 94%|█████████▎| 9894/10570 [00:36<00:02, 272.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 95%|█████████▌| 10068/10570 [00:36<00:01, 277.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 92%|█████████▏| 9758/10570 [00:36<00:03, 270.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 10117/10570 [00:36<00:01, 280.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015100%|█████████▉| 10548/10570 [00:36<00:00, 281.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 99%|█████████▉| 10455/10570 [00:37<00:00, 283.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 99%|█████████▉| 10472/10570 [00:37<00:00, 250.87it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 98%|█████████▊| 10391/10570 [00:36<00:00, 280.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015100%|██████████| 10570/10570 [00:37<00:00, 283.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015100%|██████████| 10570/10570 [00:37<00:00, 284.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015100%|█████████▉| 10558/10570 [00:37<00:00, 287.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 99%|█████████▉| 10474/10570 [00:36<00:00, 280.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 94%|█████████▍| 9922/10570 [00:36<00:02, 270.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 96%|█████████▋| 10183/10570 [00:36<00:01, 266.68it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 96%|█████████▌| 10096/10570 [00:36<00:01, 278.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015100%|██████████| 10570/10570 [00:37<00:00, 283.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 93%|█████████▎| 9786/10570 [00:36<00:02, 271.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▌| 10146/10570 [00:36<00:01, 280.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015100%|██████████| 10570/10570 [00:37<00:00, 285.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015 99%|█████████▉| 10498/10570 [00:37<00:00, 252.75it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 99%|█████████▉| 10484/10570 [00:37<00:00, 281.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 99%|█████████▊| 10420/10570 [00:37<00:00, 280.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015 99%|█████████▉| 10504/10570 [00:37<00:00, 283.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 94%|█████████▍| 9950/10570 [00:36<00:02, 271.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 97%|█████████▋| 10211/10570 [00:36<00:01, 270.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 96%|█████████▌| 10125/10570 [00:36<00:01, 279.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 93%|█████████▎| 9814/10570 [00:36<00:02, 272.42it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|█████████▋| 10175/10570 [00:36<00:01, 270.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015100%|█████████▉| 10526/10570 [00:37<00:00, 259.78it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015 99%|█████████▉| 10513/10570 [00:37<00:00, 281.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 99%|█████████▉| 10449/10570 [00:37<00:00, 280.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015100%|█████████▉| 10533/10570 [00:37<00:00, 280.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 97%|█████████▋| 10240/10570 [00:37<00:01, 273.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 94%|█████████▍| 9978/10570 [00:36<00:02, 270.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 96%|█████████▌| 10154/10570 [00:36<00:01, 279.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 93%|█████████▎| 9842/10570 [00:36<00:02, 270.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 10204/10570 [00:36<00:01, 273.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015100%|█████████▉| 10554/10570 [00:37<00:00, 264.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015100%|█████████▉| 10542/10570 [00:37<00:00, 282.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 99%|█████████▉| 10478/10570 [00:37<00:00, 274.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015100%|█████████▉| 10562/10570 [00:37<00:00, 282.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 97%|█████████▋| 10268/10570 [00:37<00:01, 272.35it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 95%|█████████▍| 10006/10570 [00:36<00:02, 270.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015100%|██████████| 10570/10570 [00:37<00:00, 283.34it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015100%|██████████| 10570/10570 [00:37<00:00, 280.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 93%|█████████▎| 9870/10570 [00:36<00:02, 269.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 96%|█████████▋| 10182/10570 [00:36<00:01, 267.46it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 10233/10570 [00:37<00:01, 275.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015100%|██████████| 10570/10570 [00:37<00:00, 281.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015 99%|█████████▉| 10507/10570 [00:37<00:00, 277.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 97%|█████████▋| 10296/10570 [00:37<00:01, 272.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 95%|█████████▍| 10034/10570 [00:36<00:01, 271.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 97%|█████████▋| 10210/10570 [00:36<00:01, 270.65it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 94%|█████████▎| 9898/10570 [00:36<00:02, 270.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 10261/10570 [00:37<00:01, 275.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015100%|█████████▉| 10536/10570 [00:37<00:00, 279.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 98%|█████████▊| 10324/10570 [00:37<00:00, 273.66it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 95%|█████████▌| 10062/10570 [00:36<00:01, 272.51it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 94%|█████████▍| 9926/10570 [00:37<00:02, 272.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 97%|█████████▋| 10239/10570 [00:37<00:01, 273.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|█████████▋| 10290/10570 [00:37<00:01, 277.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015100%|█████████▉| 10565/10570 [00:37<00:00, 280.30it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 98%|█████████▊| 10352/10570 [00:37<00:00, 274.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 95%|█████████▌| 10090/10570 [00:37<00:01, 272.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015100%|██████████| 10570/10570 [00:37<00:00, 281.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 94%|█████████▍| 9954/10570 [00:37<00:02, 274.26it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 97%|█████████▋| 10267/10570 [00:37<00:01, 273.76it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 10318/10570 [00:37<00:00, 277.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 98%|█████████▊| 10380/10570 [00:37<00:00, 273.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 96%|█████████▌| 10118/10570 [00:37<00:01, 273.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 94%|█████████▍| 9982/10570 [00:37<00:02, 272.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 97%|█████████▋| 10295/10570 [00:37<00:01, 268.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 10346/10570 [00:37<00:00, 276.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 96%|█████████▌| 10146/10570 [00:37<00:01, 274.24it/s][1,mpirank:15,algo-2]<stderr>:#015 98%|█████████▊| 10408/10570 [00:37<00:00, 273.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 95%|█████████▍| 10010/10570 [00:37<00:02, 273.94it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 98%|█████████▊| 10323/10570 [00:37<00:00, 270.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 10374/10570 [00:37<00:00, 276.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 99%|█████████▊| 10436/10570 [00:37<00:00, 274.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 96%|█████████▋| 10174/10570 [00:37<00:01, 264.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 95%|█████████▍| 10039/10570 [00:37<00:01, 276.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 98%|█████████▊| 10351/10570 [00:37<00:00, 272.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|█████████▊| 10402/10570 [00:37<00:00, 275.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 99%|█████████▉| 10464/10570 [00:37<00:00, 271.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 97%|█████████▋| 10202/10570 [00:37<00:01, 267.41it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 95%|█████████▌| 10067/10570 [00:37<00:01, 275.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 98%|█████████▊| 10379/10570 [00:37<00:00, 273.29it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▊| 10430/10570 [00:37<00:00, 275.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015 99%|█████████▉| 10492/10570 [00:37<00:00, 271.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 97%|█████████▋| 10230/10570 [00:37<00:01, 270.47it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 98%|█████████▊| 10407/10570 [00:37<00:00, 273.49it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 96%|█████████▌| 10095/10570 [00:37<00:01, 268.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 10458/10570 [00:37<00:00, 271.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015100%|█████████▉| 10520/10570 [00:38<00:00, 273.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 97%|█████████▋| 10258/10570 [00:37<00:01, 270.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 99%|█████████▊| 10435/10570 [00:37<00:00, 274.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 96%|█████████▌| 10123/10570 [00:37<00:01, 269.52it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 10486/10570 [00:37<00:00, 271.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015100%|█████████▉| 10548/10570 [00:38<00:00, 274.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 97%|█████████▋| 10286/10570 [00:37<00:01, 271.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 99%|█████████▉| 10463/10570 [00:37<00:00, 273.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 96%|█████████▌| 10151/10570 [00:37<00:01, 271.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|█████████▉| 10514/10570 [00:38<00:00, 273.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015100%|██████████| 10570/10570 [00:38<00:00, 276.56it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 98%|█████████▊| 10314/10570 [00:37<00:00, 271.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015 99%|█████████▉| 10491/10570 [00:37<00:00, 273.77it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 96%|█████████▋| 10179/10570 [00:38<00:01, 263.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|█████████▉| 10543/10570 [00:38<00:00, 276.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 98%|█████████▊| 10342/10570 [00:37<00:00, 271.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015100%|█████████▉| 10519/10570 [00:38<00:00, 275.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 97%|█████████▋| 10207/10570 [00:38<00:01, 266.90it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 10570/10570 [00:38<00:00, 276.43it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:50:02 - INFO - utils_qa - Saving predictions to /opt/ml/model/eval_predictions.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:50:02 - INFO - utils_qa - Saving nbest_preds to /opt/ml/model/eval_nbest_predictions.json.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 98%|█████████▊| 10370/10570 [00:38<00:00, 271.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015100%|█████████▉| 10547/10570 [00:38<00:00, 268.69it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 97%|█████████▋| 10234/10570 [00:38<00:01, 262.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 98%|█████████▊| 10398/10570 [00:38<00:00, 269.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015100%|██████████| 10570/10570 [00:38<00:00, 276.33it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 97%|█████████▋| 10261/10570 [00:38<00:01, 256.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 99%|█████████▊| 10426/10570 [00:38<00:00, 269.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 97%|█████████▋| 10289/10570 [00:38<00:01, 261.32it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 99%|█████████▉| 10454/10570 [00:38<00:00, 270.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 98%|█████████▊| 10316/10570 [00:38<00:00, 263.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 99%|█████████▉| 10482/10570 [00:38<00:00, 267.58it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 98%|█████████▊| 10344/10570 [00:38<00:00, 266.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015 99%|█████████▉| 10510/10570 [00:38<00:00, 269.31it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 98%|█████████▊| 10372/10570 [00:38<00:00, 268.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015100%|█████████▉| 10538/10570 [00:38<00:00, 270.64it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 98%|█████████▊| 10399/10570 [00:38<00:00, 266.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015100%|█████████▉| 10566/10570 [00:38<00:00, 271.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015100%|██████████| 10570/10570 [00:38<00:00, 272.39it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 99%|█████████▊| 10427/10570 [00:38<00:00, 267.72it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 99%|█████████▉| 10455/10570 [00:39<00:00, 269.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 99%|█████████▉| 10482/10570 [00:39<00:00, 268.44it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 99%|█████████▉| 10510/10570 [00:39<00:00, 271.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|█████████▉| 10538/10570 [00:39<00:00, 258.88it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|█████████▉| 10565/10570 [00:39<00:00, 261.27it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|██████████| 10570/10570 [00:39<00:00, 267.48it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:06/03/2022 21:50:09 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad/default/default_experiment-a0eb7cf8-6a68-4f8c-8dce-1c7f0136a59a-1-0.arrow\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|██████████| 169/169 [01:18<00:00,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:***** eval metrics *****\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  epoch            =    0.07\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  eval_exact_match = 76.4428\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  eval_f1          = 85.7368\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:  eval_samples     =   10784\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:[INFO|modelcard.py:460] 2022-06-03 21:50:09,591 >> Dropping the following result as it does not have all the necessary fields:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad', 'type': 'squad', 'args': 'plain_text'}}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[INFO|modelcard.py:460] 2022-06-03 21:50:09,591 >> Dropping the following result as it does not have all the necessary fields:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad', 'type': 'squad', 'args': 'plain_text'}}\u001b[0m\n",
      "\u001b[35m2022-06-03 21:50:22,266 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[34m2022-06-03 21:50:22,255 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2022-06-03 21:50:52,296 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2022-06-03 21:50:52,297 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach to old training job to an estimator (Optional)\n",
    "\n",
    "In Sagemaker you can attach an old training job to an estimator to continue training, get results etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# job which is going to be attached to the estimator\n",
    "old_training_job_name='<my-old-ddp-job>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach old training job\n",
    "huggingface_estimator_loaded = Estimator.attach(old_training_job_name)\n",
    "\n",
    "# get model output s3 from training job\n",
    "huggingface_estimator_loaded.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
