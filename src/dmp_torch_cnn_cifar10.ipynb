{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8203b4f5",
   "metadata": {
    "papermill": {
     "duration": 0.010006,
     "end_time": "2021-06-03T00:09:43.604098",
     "exception": false,
     "start_time": "2021-06-03T00:09:43.594092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SageMaker Distrubuted Training: Hosting a Model Parallelism with PyTorch CNN on CPU Instances\n",
    "\n",
    "*(This notebook was tested with the \"Python 3 (PyTorch CPU Optimized)\" kernel.)*\n",
    "\n",
    "This notebook demonstrates how to use SageMaker distributed model parallelism to train a CNN based model for image classification based on Cifar10 dataset. It is inteneded to run a live workshop for SageMaker distributed training with limited time and constrained resource. The training job runs on a single node with multiple vCPUs such as ml.c5.2xlarge. The code was based on SageMaker 2.x Python3.8 and PyTorch 1.11. \n",
    "\n",
    "\n",
    "Amazon SageMaker is a fully managed service that provides developers and data scientists with the ability to build, train, and deploy machine learning (ML) models quickly. Amazon SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high-quality, large models. The SageMaker Python SDK makes it easy to train and deploy models in Amazon SageMaker with several different machine learning and deep learning frameworks, including PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc15918",
   "metadata": {
    "papermill": {
     "duration": 0.010028,
     "end_time": "2021-06-03T00:09:43.624145",
     "exception": false,
     "start_time": "2021-06-03T00:09:43.614117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- An Amazon S3 bucket and prefix for training and model data. This should be in the same region used for SageMaker Studio, training, and hosting.\n",
    "- An IAM role for SageMaker to access to your training and model data. If you wish to use a different role than the one set up for SageMaker Studio, replace `sagemaker.get_execution_role()` with the appropriate IAM role or ARN. For more about using IAM roles with SageMaker, see [the AWS documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c98e1",
   "metadata": {
    "papermill": {
     "duration": 1.233937,
     "end_time": "2021-06-03T00:09:44.868186",
     "exception": false,
     "start_time": "2021-06-03T00:09:43.634249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"pytorch-cnn-cifar10-example\"\n",
    "\n",
    "#role = sagemaker.get_execution_role()\n",
    "role = 'arn:aws:iam::976939723775:role/service-role/AmazonSageMaker-ExecutionRole-20210317T133000'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e2850d",
   "metadata": {
    "papermill": {
     "duration": 0.010276,
     "end_time": "2021-06-03T00:09:44.888982",
     "exception": false,
     "start_time": "2021-06-03T00:09:44.878706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare the training data\n",
    "\n",
    "The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) is a subset of the [80 million tiny images dataset](https://people.csail.mit.edu/torralba/tinyimages). It consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f4947",
   "metadata": {
    "papermill": {
     "duration": 0.010117,
     "end_time": "2021-06-03T00:09:44.909449",
     "exception": false,
     "start_time": "2021-06-03T00:09:44.899332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Download the data\n",
    "\n",
    "First we download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c540d7",
   "metadata": {
    "papermill": {
     "duration": 8.725248,
     "end_time": "2021-06-03T00:09:53.644896",
     "exception": false,
     "start_time": "2021-06-03T00:09:44.919648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "tar xfvz cifar-10-python.tar.gz\n",
    "\n",
    "mkdir data\n",
    "mv cifar-10-batches-py data/.\n",
    "\n",
    "rm cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c418748d",
   "metadata": {
    "papermill": {
     "duration": 0.017264,
     "end_time": "2021-06-03T00:09:53.690619",
     "exception": false,
     "start_time": "2021-06-03T00:09:53.673355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After downloading the dataset, we use the [`torchvision.datasets` module](https://pytorch.org/docs/stable/torchvision/datasets.html) to load the CIFAR-10 dataset, utilizing the [`torchvision.transforms` module](https://pytorch.org/docs/stable/torchvision/transforms.html) to convert the data into normalized tensor images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa1883",
   "metadata": {
    "papermill": {
     "duration": 2.001889,
     "end_time": "2021-06-03T00:09:55.708807",
     "exception": false,
     "start_time": "2021-06-03T00:09:53.706918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cifar_utils import classes, show_img, train_data_loader, test_data_loader\n",
    "\n",
    "train_loader = train_data_loader()\n",
    "test_loader = test_data_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd19b7",
   "metadata": {
    "papermill": {
     "duration": 0.016706,
     "end_time": "2021-06-03T00:09:55.742596",
     "exception": false,
     "start_time": "2021-06-03T00:09:55.725890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Preview the data\n",
    "\n",
    "Now we can view some of data we have prepared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567526e4",
   "metadata": {
    "papermill": {
     "duration": 0.358628,
     "end_time": "2021-06-03T00:09:56.117889",
     "exception": false,
     "start_time": "2021-06-03T00:09:55.759261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision, torch\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "show_img(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels\n",
    "print(\" \".join(\"%9s\" % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ef23d",
   "metadata": {
    "papermill": {
     "duration": 0.018795,
     "end_time": "2021-06-03T00:09:56.155912",
     "exception": false,
     "start_time": "2021-06-03T00:09:56.137117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Upload the dataset to s3 \n",
    "We use the `sagemaker.s3.S3Uploader` to upload our dataset to Amazon S3. The return value `inputs` identifies the location -- we use this later for the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4496a996",
   "metadata": {
    "papermill": {
     "duration": 5.109122,
     "end_time": "2021-06-03T00:10:01.283490",
     "exception": false,
     "start_time": "2021-06-03T00:09:56.174368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "inputs = S3Uploader.upload(\"data\", \"s3://{}/{}/data\".format(bucket, prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51287599",
   "metadata": {
    "papermill": {
     "duration": 0.018788,
     "end_time": "2021-06-03T00:10:01.321435",
     "exception": false,
     "start_time": "2021-06-03T00:10:01.302647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare the entry-point script\n",
    "\n",
    "When SageMaker trains and hosts our model, it runs a Python script that we provide. (This is run as the entry point of a Docker container.) For training, this script contains the PyTorch code needed for the model to learn from our dataset. For inference, the code is for loading the model and processing the prediction input. For convenience, we put both the training and inference code in the same file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff13743d",
   "metadata": {
    "papermill": {
     "duration": 0.018532,
     "end_time": "2021-06-03T00:10:01.358640",
     "exception": false,
     "start_time": "2021-06-03T00:10:01.340108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training\n",
    "\n",
    "The training code is very similar to a training script we might run outside of Amazon SageMaker, but we can access useful properties about the training environment through various environment variables. For this notebook, our script retrieves the following environment variable values:\n",
    "\n",
    "* `SM_HOSTS`: a list of hosts on the container network.\n",
    "* `SM_CURRENT_HOST`: the name of the current container on the container network.\n",
    "* `SM_MODEL_DIR`: the location for model artifacts. This directory is uploaded to Amazon S3 at the end of the training job.\n",
    "* `SM_CHANNEL_TRAINING`: the location of our training data.\n",
    "* `SM_NUM_GPUS`: the number of GPUs available to the current container.\n",
    "\n",
    "We also use a main guard (`if __name__=='__main__':`) to ensure that our training code is executed only for training, as SageMaker imports the entry-point script.\n",
    "\n",
    "For more about writing a PyTorch training script with SageMaker, please see the [SageMaker documentation](https://sagemaker.readthedocs.io/en/stable/using_pytorch.html#prepare-a-pytorch-training-script)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbfce94",
   "metadata": {
    "papermill": {
     "duration": 0.01844,
     "end_time": "2021-06-03T00:10:01.395461",
     "exception": false,
     "start_time": "2021-06-03T00:10:01.377021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Inference\n",
    "\n",
    "For inference, we need to implement a few specific functions to tell SageMaker how to load our model and handle prediction input.\n",
    "\n",
    "* `model_fn(model_dir)`: loads the model from disk. This function must be implemented.\n",
    "* `input_fn(serialized_input_data, content_type)`: deserializes the prediction input.\n",
    "* `predict_fn(input_data, model)`: calls the model on the deserialized data.\n",
    "* `output_fn(prediction_output, accept)`: serializes the prediction output.\n",
    "\n",
    "The last three functions - `input_fn`, `predict_fn`, and `output_fn` - are optional because SageMaker has default implementations to handle common content types. However, there is no default implementation of `model_fn` for PyTorch models on SageMaker, so our script has to implement `model_fn`.\n",
    "\n",
    "For more about PyTorch inference with SageMaker, please see the [SageMaker documentation](https://sagemaker.readthedocs.io/en/stable/using_pytorch.html#id3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb37d8",
   "metadata": {
    "papermill": {
     "duration": 0.018422,
     "end_time": "2021-06-03T00:10:01.432398",
     "exception": false,
     "start_time": "2021-06-03T00:10:01.413976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Put it all together\n",
    "\n",
    "Here is the full script for both training and hosting our convolutional neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f8296",
   "metadata": {
    "papermill": {
     "duration": 0.852241,
     "end_time": "2021-06-03T00:10:02.303309",
     "exception": false,
     "start_time": "2021-06-03T00:10:01.451068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize scripts/cifar10_torch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a0ee7",
   "metadata": {
    "papermill": {
     "duration": 0.019986,
     "end_time": "2021-06-03T00:10:02.343439",
     "exception": false,
     "start_time": "2021-06-03T00:10:02.323453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run a SageMaker training job\n",
    "\n",
    "The SageMaker Python SDK makes it easy for us to interact with SageMaker. Here, we use the `PyTorch` estimator class to start a training job. We configure it with the following parameters:\n",
    "\n",
    "* `entry_point`: our training script.\n",
    "* `role`: an IAM role that SageMaker uses to access training and model data.\n",
    "* `framework_version`: the PyTorch version we wish to use. For a list of supported versions, see [here](https://github.com/aws/sagemaker-python-sdk#pytorch-sagemaker-estimators).\n",
    "* `instance_count`: the number of training instances.\n",
    "* `instance_type`: the training instance type. For a list of supported instance types, see [the AWS Documentation](https://aws.amazon.com/sagemaker/pricing/instance-types/).\n",
    "\n",
    "Once we our `PyTorch` estimator, we start a training job by calling `fit()` and passing the training data we uploaded to S3 earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35210639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "hyperparameters = {\"epoch\": 10,\n",
    "                  \"lr\":0.0005,\n",
    "                  \"batch_size\":8}\n",
    "env={'SAGEMAKER_REQUIREMENTS': 'requirements.txt'}\n",
    "\n",
    "kwargs = dict(\n",
    "    source_dir=\"./scripts\",\n",
    "    entry_point=\"cifar10_torch.py\",\n",
    "    model_dir=False,\n",
    "    env=env,\n",
    "    instance_type=\"ml.c5.2xlarge\",\n",
    "    instance_count=1,\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version='py38',\n",
    "    debugger_hook_config=None,\n",
    "    disable_profiler=True,\n",
    "    max_run=60 * 60,  # 60 minutes\n",
    "    role=role,\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"training_loss\", \"Regex\": \"loss: ([0-9.]*?) \"},\n",
    "        {\"Name\": \"training_accuracy\", \"Regex\": \"accuracy: ([0-9.]*?) \"},\n",
    "        {\"Name\": \"training_latency_per_epoch\", \"Regex\": \"- ([0-9.]*?)s/epoch\"},\n",
    "        {\"Name\": \"training_avg_latency_per_step\", \"Regex\": \"- ([0-9.]*?)ms/step\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454afa6",
   "metadata": {},
   "source": [
    "## Configure smdistributed parameters for distributed model parallelism\n",
    "\n",
    "Model parallelism allows partitioning a large deep learning model across multiple devices, within or across instances during training. Increasing the size of deep learning models (layers and parameters) yields better accuracy for complex tasks such as computer vision and natural language processing. However, there is a limit to the maximum model size you can fit in the memory of a single GPU. When training DL models, GPU memory limitations can be bottlenecks in the following ways:\n",
    "\n",
    "* They limit the size of the model you can train, since the memory footprint of a model scales proportionally to the number of parameters.\n",
    "\n",
    "* They limit the per-GPU batch size during training, driving down GPU utilization and training efficiency.\n",
    "\n",
    "To overcome the limitations associated with training a model on a single GPU, SageMaker provides the model parallel library to help distribute and train DL models efficiently on multiple compute nodes. Furthermore, with the library, you can achieve most optimized distributed training using EFA-supported devices, which enhance the performance of inter-node communication with low latency, high throughput, and OS bypass.\n",
    "\n",
    "\n",
    "For a training job that uses AMP (FP16) and Adam optimizers, the required GPU memory per parameter is about 20 bytes, which we can break down as follows:\n",
    "\n",
    "* An FP16 parameter ~ 2 bytes\n",
    "* An FP16 gradient ~ 2 bytes\n",
    "* An FP32 optimizer state ~ 8 bytes based on the Adam optimizers\n",
    "* An FP32 copy of parameter ~ 4 bytes (needed for the optimizer apply (OA) operation)\n",
    "* An FP32 copy of gradient ~ 4 bytes (needed for the OA operation)\n",
    "\n",
    "Even for a relatively small DL model with 10 billion parameters, it can require at least 200GB of memory, which is much larger than the typical GPU memory (for example, NVIDIA A100 with 40GB/80GB memory and V100 with 16/32 GB) available on a single GPU. Note that on top of the memory requirements for model and optimizer states, there are other memory consumers such as activations generated in the forward pass. The memory required can be a lot greater than 200GB.\n",
    "\n",
    "SageMaker distributed training libraries support Pipeline parallelism, Tensor parallelism (available for PyTorch) and Optimizer state sharding (available for PyTorch).\n",
    "\n",
    "* Pipeline parallelism partitions the set of layers or operations across the set of devices, leaving each operation intact.\n",
    "* Tensor parallelism splits individual layers, or nn.Modules, across devices, to be run in parallel. \n",
    "* Optimizing state sharding is to avoid replicating optimizer state in all of your GPUs by using a single replica of the optimizer state which is sharded across data-parallel ranks, with no redundancy across devices.\n",
    "\n",
    "\n",
    "SageMaker's distributed model parallel library to train large deep learning (DL) models that are difficult to train due to GPU memory limitations. The library automatically and efficiently splits a model across multiple GPUs and instances. Using the library, you can achieve a target prediction accuracy faster by efficiently training larger DL models with billions or trillions of parameters.\n",
    "\n",
    "You can use the library to automatically partition your own TensorFlow and PyTorch models across multiple GPUs and multiple nodes with minimal code changes. You can access the library's API through the SageMaker Python SDK.To track the latest updates of the library, see the [SageMaker Distributed Model Parallel Release Notes](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_release_notes/smd_model_parallel_change_log.html) in the SageMaker Python SDK documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f87dc",
   "metadata": {},
   "source": [
    "Amazon SageMaker’s TensorFlow and PyTorch estimator objects contain a distribution parameter, which you can use to enable and specify parameters for SageMaker distributed training. The SageMaker model parallel library internally uses MPI. To use model parallelism, both smdistributed and MPI must be enabled through the distribution parameter.\n",
    "\n",
    "* microbatches: The number of microbatches to perform pipelining over. 1 means no pipelining. Batch size must be divisible by the number of microbatches.\n",
    "* pipeline: The pipeline schedule \"interleaved\" or \"simple\"\n",
    "* placement_strategy: Determines the mapping of model partitions onto physical devices. \"cluster\", \"spread\"\n",
    "* optimize: Determines the distribution mechanism of transformer layers. \"memory\" or \"speed\".\n",
    "* auto_partition: Enable auto-partitioning. If disabled, default_partition parameter must be provided.\n",
    "* default_partition: Required if auto_partition is false. The partition ID to place operations/modules that are not placed in any smp.partition contexts. 0 or 1\n",
    "\n",
    "In addition, there are a few PyTorch and Tensorflow specific parameters. For details please refer to this the [SageMaker document](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for running training on smdistributed Model Parallel\n",
    "mpi_options = {\n",
    "    \"enabled\" : True,\n",
    "    \"processes_per_host\" : 8, # (pipeline_parallel_degree) x (data_parallel_degree) = processes_per_host\n",
    "    \"custom_mpi_options\": \"-verbose\", #To avoid Docker warnings from contaminating your training logs,\n",
    "}\n",
    "\n",
    "smp_options = {\n",
    "    \"enabled\":True,\n",
    "    \"parameters\": {\n",
    "        \"microbatches\": 4,  # Mini-batchs are split in micro-batch to increase parallelism\n",
    "        \"placement_strategy\": \"spread\",\n",
    "        \"pipeline\": \"interleaved\",\n",
    "        \"optimize\": \"speed\",\n",
    "        \"partitions\": 4, # we'll partition the model among the 4 CPUs\n",
    "        \"ddp\": True,\n",
    "    },\n",
    "    \"parameter_server\": {\n",
    "        \"enabled\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "distribution={\n",
    "    \"smdistributed\": {\"modelparallel\": smp_options},\n",
    "    \"mpi\": mpi_options\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb085a",
   "metadata": {},
   "source": [
    "# Set the distribution\n",
    "data_distribution = { \"smdistributed\":\n",
    "                { \"dataparallel\":\n",
    "                 { \"enabled\": True\n",
    "                 }\n",
    "                }\n",
    "               } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0efe121",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs['instance_count']  =  1\n",
    "\n",
    "def_dmp_estimator = sagemaker.pytorch.estimator.PyTorch(\n",
    "    hyperparameters=hyperparameters,\n",
    "    **kwargs,\n",
    "    distribution=distribution,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3155ad2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def_dmp_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aee0ba",
   "metadata": {},
   "source": [
    "## Deploy the model for inference\n",
    "\n",
    "After we train our model, we can deploy it to a SageMaker Endpoint, which serves prediction requests in real-time. To do so, we simply call `deploy()` on our estimator, passing in the desired number of instances and instance type for the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c39183",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def_dmp_predictor = def_dmp_estimator.deploy(initial_instance_count=1, instance_type=\"ml.c5.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a9d8c7",
   "metadata": {},
   "source": [
    "## Send a few samples for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "\n",
    "# print images, labels, and predictions\n",
    "show_img(torchvision.utils.make_grid(images))\n",
    "print(\"GroundTruth: \", \" \".join(\"%4s\" % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "outputs = def_dmp_predictor.predict(images.numpy())\n",
    "\n",
    "_, predicted = torch.max(torch.from_numpy(np.array(outputs)), 1)\n",
    "\n",
    "print(\"Predicted:   \", \" \".join(\"%4s\" % classes[predicted[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89957cc3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Once finished, we delete our endpoint to release the instances (and avoid incurring extra costs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b196e7d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def_dmp_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9486136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "papermill": {
   "default_parameters": {},
   "duration": 21.049102,
   "end_time": "2021-06-03T00:10:03.807533",
   "environment_variables": {},
   "exception": true,
   "input_path": "pytorch_cnn_cifar10.ipynb",
   "output_path": "/opt/ml/processing/output/pytorch_cnn_cifar10-2021-06-03-00-05-20.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25"
   },
   "start_time": "2021-06-03T00:09:42.758431",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
